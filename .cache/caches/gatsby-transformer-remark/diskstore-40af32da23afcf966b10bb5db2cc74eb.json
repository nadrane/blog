{"expireTime":9007200794876223000,"key":"transformer-remark-markdown-html-00f8e0fe527c06e8aae3932fc2421380--","val":"<p>I've conducted about 100 technical interviews over the past 6 months for a software development recruiting company called <a href=\"https://triplebyte.com/iv/RrzqKKw/bp\">Triplebyte</a>. I've also been doing consulting work, which has required me to take numerous technical interviews. It's interesting contrasting the experiences to identify what works and what doesn't.</p>\n<p>Every Triplebyte interview begins with the candidate coding a short game. We have a series of steps, and each step precisely defines simple requirements for the program to handle. I can generally tell a couple minutes into the 2 hour interview whether the candidate will be successful. There are certainly outliers (as well as mechanisms to prevent bias), but in general, I can quickly ascertain how well a candidate stacks up technically. So then, it begs the question, why is it so hard to hire engineers? The answer is, most of us are doing it wrong.</p>\n<!-- more -->\n<h2>Interview Standardization</h2>\n<p>About 12 months ago, while running <a href=\"https://www.fullstackacademy.com/\">Fullstack Academy's</a> Chicago campus, I was looking to hire a qualified instructor. Our interview consisted of live lecture as well as a mock 1-1 teaching session, where the candidate was required to guide me (playing the role of a struggling student) through a simple recursion problem. The latter exercise was almost always a better indicator of instructor fit than the former. The reason was simple: the latter exercise was assigned to the candidate whereas the lecture topic was their choice. This meant that every lecture topic was different and made lectures difficult to compare. In contrast, after having seen dozens of candidates attempt to teach me the same recursion problem, it was obvious who stood out.</p>\n<p>The first mistake that companies make is that they fail to standardize the interview across candidates. As interviewers, it's tempting to say, \"let's have the candidate pair with us, that way we can see how they perform on real day-to-day challenges.\" Or another fault I've seen committed: give some candidates one problem and other candidates a different problem. This way a company can protect against its problems getting leaked online, right?</p>\n<p>The issue is, as an interviewer, unless you have witnessed a dozen candidates attempt a given problem, it's very challenging to assess their abilities. It's easy to fall victim to the \"I solved it, why can't they\" bias. You need to compare candidates to each other to assess who did a good job and who didn't. Which brings me to my next point.</p>\n<h2>Objective Scoring</h2>\n<p>You need to compare candidates objectively, scoring them on every exercise across a variety of factors. There must be well-defined criteria to differentiate scores, and scoring should be done immediately upon completion of the exercise.</p>\n<p>Suppose our interviewer tasks the candidate to construct a simple web application. You assign a score of 1 through 4 to the following factors:</p>\n<ol>\n<li>Productivity</li>\n<li>Programming Style</li>\n<li>Language/Framework Familiarity</li>\n<li>Debugging Ability</li>\n</ol>\n<p>Beforehand, you would need to write a short description of what qualifies each score for each of the above factors. Then, during the interview, you will need a rubric to help you determine what score a candidate earns for each factor. Productivity can be measured exceptionally objectively if you have the candidate follow a prescribed series of steps to solve the programming challenge. For example, at Triplebyte, we literally tell them (loosely) what functions to write and in what order.</p>\n<p>After a week or two of interviewing a half dozen candidates, this scoring becomes critical and allows you avoid biases such as the <a href=\"https://en.wikipedia.org/wiki/Serial-position_effect\">Serial-position effect</a>.</p>\n<h2>Culture Fit</h2>\n<p>Surprisingly, a lot of this advice can also be applied to assessing culture fit. You can have the same person, ask the same questions, in the same order, using the same rubric to compare every answer. The risk is that you will inadvertently dehumanize the candidate, but it is possible to do this objective assessment such that the candidate is unaware of it. At the very least, you need to document your impression of the candidate immediately after the interview, lest you rely on your memory/feelings a week later when making a decision.</p>\n<p>You could take this philosophy to the absurd, and it would certainly eliminate bias, though I personally believe there's a point of diminishing returns, at which you degrade the experience of the interviewee. Nevertheless, some standardization across interviews is critical to avoid implicit bias and identify good candidates, and I encourage every tech company to examine their interview process to assess their level of standardization.</p>"}