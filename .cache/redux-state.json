{
  "nodes": {
    "SitePage /dev-404-page/": {
      "jsonName": "dev-404-page-5f9",
      "internalComponentName": "ComponentDev404Page",
      "path": "/dev-404-page/",
      "component": "/Users/nickdrane/projects/blog/.cache/dev-404-page.js",
      "componentChunkName": "component---cache-dev-404-page-js",
      "context": {},
      "pluginCreator___NODE": "Plugin dev-404-page",
      "pluginCreatorId": "Plugin dev-404-page",
      "componentPath": "/Users/nickdrane/projects/blog/.cache/dev-404-page.js",
      "id": "SitePage /dev-404-page/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "032dbf9c8b67c2c8faba3e36a5c2cc4f",
        "description": "Plugin dev-404-page",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin dev-404-page": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/dev-404-page",
      "id": "Plugin dev-404-page",
      "name": "dev-404-page",
      "version": "1.0.0",
      "pluginOptions": {
        "plugins": []
      },
      "nodeAPIs": [
        "createPagesStatefully"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/dev-404-page",
      "packageJson": {
        "name": "dev-404-page",
        "description": "Internal plugin to add a 404 page in development with helpful information",
        "version": "1.0.0",
        "main": "index.js",
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [],
        "devDependencies": [],
        "peerDependencies": [],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "9f81e50c2fc3c1d464b07bdc9667d2d6",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin load-babel-config": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/load-babel-config",
      "id": "Plugin load-babel-config",
      "name": "load-babel-config",
      "version": "1.0.0",
      "pluginOptions": {
        "plugins": []
      },
      "nodeAPIs": [
        "onPreBootstrap"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/load-babel-config",
      "packageJson": {
        "name": "load-babel-config",
        "description": "Internal plugin that handles loading Babel configs",
        "version": "1.0.0",
        "main": "index.js",
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [],
        "devDependencies": [],
        "peerDependencies": [],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "412d78aa70c930ab362cbfc8ee132bb9",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin internal-data-bridge": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/internal-data-bridge",
      "id": "Plugin internal-data-bridge",
      "name": "internal-data-bridge",
      "version": "1.0.0",
      "pluginOptions": {
        "plugins": []
      },
      "nodeAPIs": [
        "sourceNodes",
        "onCreatePage"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/internal-data-bridge",
      "packageJson": {
        "name": "internal-data-bridge",
        "description": "An internal Gatsby plugin which creates data nodes from internal data",
        "version": "1.0.0",
        "main": "index.js",
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [],
        "devDependencies": [],
        "peerDependencies": [],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "35fbf529cfa3c7e73ce6ea2c2c6dd88f",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin prod-404": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/prod-404",
      "id": "Plugin prod-404",
      "name": "prod-404",
      "version": "1.0.0",
      "pluginOptions": {
        "plugins": []
      },
      "nodeAPIs": [
        "onCreatePage"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/prod-404",
      "packageJson": {
        "name": "prod-404",
        "description": "Internal plugin to detect various flavors of 404 pages and ensure there's a 404.html path created as well to ensure compatability with static hosts",
        "version": "1.0.0",
        "main": "index.js",
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [],
        "devDependencies": [],
        "peerDependencies": [],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "c5fdbfdf921e0546f17b6e5df603aa62",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin query-runner": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/query-runner",
      "id": "Plugin query-runner",
      "name": "query-runner",
      "version": "1.0.0",
      "pluginOptions": {
        "plugins": []
      },
      "nodeAPIs": [
        "onCreatePage"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby/dist/internal-plugins/query-runner",
      "packageJson": {
        "name": "query-runner",
        "description": "Internal plugin for running queries",
        "version": "1.0.0",
        "main": "index.js",
        "author": "",
        "license": "MIT",
        "dependencies": [],
        "devDependencies": [],
        "peerDependencies": [],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "e3de689e484dbf33bb77f389a8d50992",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin gatsby-plugin-react-helmet": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby-plugin-react-helmet",
      "id": "Plugin gatsby-plugin-react-helmet",
      "name": "gatsby-plugin-react-helmet",
      "version": "3.0.0",
      "pluginOptions": {
        "plugins": []
      },
      "nodeAPIs": [],
      "browserAPIs": [],
      "ssrAPIs": [
        "onRenderBody"
      ],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby-plugin-react-helmet",
      "packageJson": {
        "name": "gatsby-plugin-react-helmet",
        "description": "Manage document head data with react-helmet. Provides drop-in server rendering support for Gatsby.",
        "version": "3.0.0",
        "main": "index.js",
        "keywords": [
          "gatsby",
          "gatsby-plugin",
          "favicon",
          "react-helmet",
          "seo",
          "document",
          "head",
          "title",
          "meta",
          "link",
          "script",
          "base",
          "noscript",
          "style"
        ],
        "author": "Kyle Mathews <matthews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [
          {
            "name": "@babel/runtime",
            "version": "^7.0.0"
          }
        ],
        "devDependencies": [
          {
            "name": "@babel/cli",
            "version": "^7.0.0"
          },
          {
            "name": "@babel/core",
            "version": "^7.0.0"
          },
          {
            "name": "cross-env",
            "version": "^5.1.4"
          }
        ],
        "peerDependencies": [
          {
            "name": "gatsby",
            "version": ">2.0.0-alpha"
          },
          {
            "name": "react-helmet",
            "version": ">=5.1.3"
          }
        ],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "5be544fd86571b1a78c488446517e335",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin gatsby-plugin-manifest": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby-plugin-manifest",
      "id": "Plugin gatsby-plugin-manifest",
      "name": "gatsby-plugin-manifest",
      "version": "2.0.5",
      "pluginOptions": {
        "plugins": [],
        "name": "gatsby-starter-default",
        "short_name": "starter",
        "start_url": "/",
        "background_color": "#663399",
        "theme_color": "#663399",
        "display": "minimal-ui",
        "icon": "src/images/gatsby-icon.png"
      },
      "nodeAPIs": [
        "onPostBootstrap"
      ],
      "browserAPIs": [],
      "ssrAPIs": [
        "onRenderBody"
      ],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby-plugin-manifest",
      "packageJson": {
        "name": "gatsby-plugin-manifest",
        "description": "Gatsby plugin which adds a manifest.webmanifest to make sites progressive web apps",
        "version": "2.0.5",
        "main": "index.js",
        "keywords": [
          "gatsby",
          "gatsby-plugin",
          "favicon",
          "icons",
          "manifest.webmanifest",
          "progressive-web-app",
          "pwa"
        ],
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [
          {
            "name": "@babel/runtime",
            "version": "^7.0.0"
          },
          {
            "name": "bluebird",
            "version": "^3.5.0"
          },
          {
            "name": "sharp",
            "version": "^0.20.2"
          }
        ],
        "devDependencies": [
          {
            "name": "@babel/cli",
            "version": "^7.0.0"
          },
          {
            "name": "@babel/core",
            "version": "^7.0.0"
          },
          {
            "name": "cross-env",
            "version": "^5.1.4"
          }
        ],
        "peerDependencies": [
          {
            "name": "gatsby",
            "version": ">2.0.0-alpha"
          }
        ],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "4c4bcaf148018f0c4eab4609df1c8659",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin gatsby-plugin-offline": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby-plugin-offline",
      "id": "Plugin gatsby-plugin-offline",
      "name": "gatsby-plugin-offline",
      "version": "2.0.7",
      "pluginOptions": {
        "plugins": []
      },
      "nodeAPIs": [
        "createPages",
        "onPostBuild"
      ],
      "browserAPIs": [
        "registerServiceWorker",
        "onPostPrefetchPathname",
        "onServiceWorkerActive"
      ],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby-plugin-offline",
      "packageJson": {
        "name": "gatsby-plugin-offline",
        "description": "Gatsby plugin which sets up a site to be able to run offline",
        "version": "2.0.7",
        "main": "index.js",
        "keywords": [
          "gatsby",
          "gatsby-plugin",
          "offline",
          "precache",
          "service-worker"
        ],
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [
          {
            "name": "@babel/runtime",
            "version": "^7.0.0"
          },
          {
            "name": "cheerio",
            "version": "^1.0.0-rc.2"
          },
          {
            "name": "lodash",
            "version": "^4.17.10"
          },
          {
            "name": "workbox-build",
            "version": "^3.4.1"
          }
        ],
        "devDependencies": [
          {
            "name": "@babel/cli",
            "version": "^7.0.0"
          },
          {
            "name": "@babel/core",
            "version": "^7.0.0"
          },
          {
            "name": "cross-env",
            "version": "^5.1.4"
          }
        ],
        "peerDependencies": [
          {
            "name": "gatsby",
            "version": ">=2.0.20"
          }
        ],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "575f964f58cc5b8676976ac79de914f0",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin gatsby-source-filesystem": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby-source-filesystem",
      "id": "Plugin gatsby-source-filesystem",
      "name": "gatsby-source-filesystem",
      "version": "2.0.5",
      "pluginOptions": {
        "plugins": [],
        "name": "posts",
        "path": "/Users/nickdrane/projects/blog/content/_posts"
      },
      "nodeAPIs": [
        "sourceNodes",
        "setFieldsOnGraphQLNodeType"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby-source-filesystem",
      "packageJson": {
        "name": "gatsby-source-filesystem",
        "description": "Gatsby plugin which parses files within a directory for further parsing by other plugins",
        "version": "2.0.5",
        "keywords": [
          "gatsby",
          "gatsby-plugin"
        ],
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [
          {
            "name": "@babel/runtime",
            "version": "^7.0.0"
          },
          {
            "name": "better-queue",
            "version": "^3.8.7"
          },
          {
            "name": "bluebird",
            "version": "^3.5.0"
          },
          {
            "name": "chokidar",
            "version": "^1.7.0"
          },
          {
            "name": "fs-extra",
            "version": "^5.0.0"
          },
          {
            "name": "got",
            "version": "^7.1.0"
          },
          {
            "name": "md5-file",
            "version": "^3.1.1"
          },
          {
            "name": "mime",
            "version": "^2.2.0"
          },
          {
            "name": "pretty-bytes",
            "version": "^4.0.2"
          },
          {
            "name": "slash",
            "version": "^1.0.0"
          },
          {
            "name": "valid-url",
            "version": "^1.0.9"
          },
          {
            "name": "xstate",
            "version": "^3.1.0"
          }
        ],
        "devDependencies": [
          {
            "name": "@babel/cli",
            "version": "^7.0.0"
          },
          {
            "name": "@babel/core",
            "version": "^7.0.0"
          },
          {
            "name": "cross-env",
            "version": "^5.1.4"
          }
        ],
        "peerDependencies": [
          {
            "name": "gatsby",
            "version": ">2.0.0-alpha"
          }
        ],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "9f76dee68c474aeb04f1efaedd41d29f",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin gatsby-transformer-remark": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby-transformer-remark",
      "id": "Plugin gatsby-transformer-remark",
      "name": "gatsby-transformer-remark",
      "version": "2.1.8",
      "pluginOptions": {
        "plugins": [],
        "excerpt_separator": "<!-- more -->"
      },
      "nodeAPIs": [
        "onCreateNode",
        "setFieldsOnGraphQLNodeType"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby-transformer-remark",
      "packageJson": {
        "name": "gatsby-transformer-remark",
        "description": "Gatsby transformer plugin for Markdown using the Remark library and ecosystem",
        "version": "2.1.8",
        "keywords": [
          "gatsby",
          "gatsby-plugin",
          "markdown",
          "remark"
        ],
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [
          {
            "name": "@babel/runtime",
            "version": "^7.0.0"
          },
          {
            "name": "bluebird",
            "version": "^3.5.0"
          },
          {
            "name": "gray-matter",
            "version": "^4.0.0"
          },
          {
            "name": "hast-util-raw",
            "version": "^4.0.0"
          },
          {
            "name": "hast-util-to-html",
            "version": "^3.0.0"
          },
          {
            "name": "lodash",
            "version": "^4.17.10"
          },
          {
            "name": "mdast-util-to-hast",
            "version": "^3.0.0"
          },
          {
            "name": "mdast-util-toc",
            "version": "^2.0.1"
          },
          {
            "name": "remark",
            "version": "^9.0.0"
          },
          {
            "name": "remark-parse",
            "version": "^5.0.0"
          },
          {
            "name": "remark-retext",
            "version": "^3.1.0"
          },
          {
            "name": "remark-stringify",
            "version": "^5.0.0"
          },
          {
            "name": "retext-english",
            "version": "^3.0.0"
          },
          {
            "name": "sanitize-html",
            "version": "^1.18.2"
          },
          {
            "name": "underscore.string",
            "version": "^3.3.4"
          },
          {
            "name": "unified",
            "version": "^6.1.5"
          },
          {
            "name": "unist-util-remove-position",
            "version": "^1.1.2"
          },
          {
            "name": "unist-util-select",
            "version": "^1.5.0"
          },
          {
            "name": "unist-util-visit",
            "version": "^1.3.0"
          }
        ],
        "devDependencies": [
          {
            "name": "@babel/cli",
            "version": "^7.0.0"
          },
          {
            "name": "@babel/core",
            "version": "^7.0.0"
          },
          {
            "name": "cross-env",
            "version": "^5.1.4"
          }
        ],
        "peerDependencies": [
          {
            "name": "gatsby",
            "version": ">2.0.0-alpha"
          }
        ],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "4c6010878ba860cb44d3c1678d674939",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin default-site-plugin": {
      "resolve": "/Users/nickdrane/projects/blog",
      "id": "Plugin default-site-plugin",
      "name": "default-site-plugin",
      "version": "09931e01871e2ad69950b5b1a1ca4eb6",
      "pluginOptions": {
        "plugins": []
      },
      "nodeAPIs": [
        "setFieldsOnGraphQLNodeType",
        "createPages"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog",
      "packageJson": {
        "name": "gatsby-starter-default",
        "description": "Gatsby default starter",
        "version": "1.0.0",
        "keywords": [
          "gatsby"
        ],
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [
          {
            "name": "gatsby",
            "version": "^2.0.19"
          },
          {
            "name": "gatsby-plugin-manifest",
            "version": "^2.0.5"
          },
          {
            "name": "gatsby-plugin-offline",
            "version": "^2.0.5"
          },
          {
            "name": "gatsby-plugin-react-helmet",
            "version": "^3.0.0"
          },
          {
            "name": "gatsby-source-filesystem",
            "version": "^2.0.5"
          },
          {
            "name": "gatsby-transformer-remark",
            "version": "^2.1.8"
          },
          {
            "name": "ramda",
            "version": "^0.25.0"
          },
          {
            "name": "react",
            "version": "^16.5.1"
          },
          {
            "name": "react-dom",
            "version": "^16.5.1"
          },
          {
            "name": "react-helmet",
            "version": "^5.2.0"
          }
        ],
        "devDependencies": [
          {
            "name": "prettier",
            "version": "^1.14.2"
          }
        ],
        "peerDependencies": [],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "7dbd7a41ce26f8b27b4a79b9b7520872",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Plugin gatsby-plugin-page-creator": {
      "resolve": "/Users/nickdrane/projects/blog/node_modules/gatsby-plugin-page-creator",
      "id": "Plugin gatsby-plugin-page-creator",
      "name": "gatsby-plugin-page-creator",
      "version": "2.0.1",
      "pluginOptions": {
        "plugins": [],
        "path": "/Users/nickdrane/projects/blog/src/pages",
        "pathCheck": false
      },
      "nodeAPIs": [
        "createPagesStatefully"
      ],
      "browserAPIs": [],
      "ssrAPIs": [],
      "pluginFilepath": "/Users/nickdrane/projects/blog/node_modules/gatsby-plugin-page-creator",
      "packageJson": {
        "name": "gatsby-plugin-page-creator",
        "description": "Gatsby plugin that automatically creates pages from React components in specified directories",
        "version": "2.0.1",
        "main": "index.js",
        "keywords": [
          "gatsby",
          "gatsby-plugin"
        ],
        "author": "Kyle Mathews <mathews.kyle@gmail.com>",
        "license": "MIT",
        "dependencies": [
          {
            "name": "@babel/runtime",
            "version": "^7.0.0"
          },
          {
            "name": "bluebird",
            "version": "^3.5.0"
          },
          {
            "name": "chokidar",
            "version": "^1.7.0"
          },
          {
            "name": "fs-exists-cached",
            "version": "^1.0.0"
          },
          {
            "name": "glob",
            "version": "^7.1.1"
          },
          {
            "name": "lodash",
            "version": "^4.17.10"
          },
          {
            "name": "parse-filepath",
            "version": "^1.0.1"
          },
          {
            "name": "slash",
            "version": "^1.0.0"
          }
        ],
        "devDependencies": [
          {
            "name": "@babel/cli",
            "version": "^7.0.0"
          },
          {
            "name": "@babel/core",
            "version": "^7.0.0"
          },
          {
            "name": "cross-env",
            "version": "^5.0.5"
          }
        ],
        "peerDependencies": [
          {
            "name": "gatsby",
            "version": ">2.0.0-alpha"
          }
        ],
        "optionalDependecies": [],
        "bundledDependecies": []
      },
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "133aa5e6f5ef7030561339abdefbeab9",
        "type": "SitePlugin",
        "owner": "internal-data-bridge"
      }
    },
    "Site": {
      "siteMetadata": {
        "title": "Gatsby Default Starter"
      },
      "port": "8000",
      "host": "localhost",
      "pathPrefix": "",
      "polyfill": true,
      "buildTime": "2018-10-21T15:26:39.620Z",
      "id": "Site",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "contentDigest": "fa3f24eb63680410f9e127e87540eacc",
        "type": "Site",
        "owner": "internal-data-bridge"
      }
    },
    "e5bbd1cb-9b8c-52f3-b677-c3dcba0bd9e5": {
      "id": "e5bbd1cb-9b8c-52f3-b677-c3dcba0bd9e5",
      "children": [],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "b022dc8c14276650637258e0966ba73a",
        "type": "Directory",
        "description": "Directory \"content/_posts\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts",
      "relativePath": "",
      "extension": "",
      "size": 448,
      "prettySize": "448 B",
      "modifiedTime": "2018-10-19T02:37:11.309Z",
      "accessTime": "2018-10-21T15:26:41.436Z",
      "changeTime": "2018-10-20T16:55:59.540Z",
      "birthTime": "2018-09-15T20:59:09.600Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content",
      "base": "_posts",
      "ext": "",
      "name": "_posts",
      "relativeDirectory": "..",
      "dev": 16777220,
      "mode": 16877,
      "nlink": 14,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 2991427,
      "blocks": 0,
      "atimeMs": 1540135601436.453,
      "mtimeMs": 1539916631308.5767,
      "ctimeMs": 1540054559540.1401,
      "birthtimeMs": 1537045149599.8171,
      "atime": "2018-10-21T15:26:41.436Z",
      "mtime": "2018-10-19T02:37:11.309Z",
      "ctime": "2018-10-20T16:55:59.540Z",
      "birthtime": "2018-09-15T20:59:09.600Z"
    },
    "2546894c-d49f-53f5-b060-a64985442bba": {
      "id": "2546894c-d49f-53f5-b060-a64985442bba",
      "children": [
        "6e5a1b24-f308-5104-8f33-b43745551d5f"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "9976aa9623a5a9030a6d1638a2a5b703",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/build-your-own-nested-query-string-encoder.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/build-your-own-nested-query-string-encoder.md",
      "relativePath": "build-your-own-nested-query-string-encoder.md",
      "extension": "md",
      "size": 8208,
      "prettySize": "8.21 kB",
      "modifiedTime": "2018-09-30T21:05:29.859Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-09-30T21:05:29.859Z",
      "birthTime": "2018-09-15T20:59:09.600Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "build-your-own-nested-query-string-encoder.md",
      "ext": ".md",
      "name": "build-your-own-nested-query-string-encoder",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 2991428,
      "blocks": 24,
      "atimeMs": 1540135197513.822,
      "mtimeMs": 1538341529858.7385,
      "ctimeMs": 1538341529858.7385,
      "birthtimeMs": 1537045149599.869,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-09-30T21:05:29.859Z",
      "ctime": "2018-09-30T21:05:29.859Z",
      "birthtime": "2018-09-15T20:59:09.600Z"
    },
    "031a5100-2916-5e59-a79a-799fbfff2048": {
      "id": "031a5100-2916-5e59-a79a-799fbfff2048",
      "children": [
        "17642430-0738-5531-88a3-12996ff69b04"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "df5811184d1b8393908bc1c5a5143166",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/build-your-own-regex.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/build-your-own-regex.md",
      "relativePath": "build-your-own-regex.md",
      "extension": "md",
      "size": 10625,
      "prettySize": "10.6 kB",
      "modifiedTime": "2018-10-18T03:20:23.620Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-10-18T03:20:23.620Z",
      "birthTime": "2018-10-18T03:20:23.619Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "build-your-own-regex.md",
      "ext": ".md",
      "name": "build-your-own-regex",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 6040263,
      "blocks": 24,
      "atimeMs": 1540135197513.843,
      "mtimeMs": 1539832823619.637,
      "ctimeMs": 1539832823619.637,
      "birthtimeMs": 1539832823619.4211,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-10-18T03:20:23.620Z",
      "ctime": "2018-10-18T03:20:23.620Z",
      "birthtime": "2018-10-18T03:20:23.619Z"
    },
    "acd24474-09d5-5ec7-8a1b-0bf9a5e4dc60": {
      "id": "acd24474-09d5-5ec7-8a1b-0bf9a5e4dc60",
      "children": [
        "44b69257-3ece-526d-a8d9-25a8319c3e69"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "b5d86124a9d0d774654711dbcb0218b1",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/ethical-engineering-for-the-average-engineer.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/ethical-engineering-for-the-average-engineer.md",
      "relativePath": "ethical-engineering-for-the-average-engineer.md",
      "extension": "md",
      "size": 4857,
      "prettySize": "4.86 kB",
      "modifiedTime": "2018-09-30T20:52:27.477Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-09-30T20:52:27.477Z",
      "birthTime": "2018-09-15T21:00:58.351Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "ethical-engineering-for-the-average-engineer.md",
      "ext": ".md",
      "name": "ethical-engineering-for-the-average-engineer",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 2991566,
      "blocks": 16,
      "atimeMs": 1540135197513.8628,
      "mtimeMs": 1538340747476.6147,
      "ctimeMs": 1538340747476.6147,
      "birthtimeMs": 1537045258350.8496,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-09-30T20:52:27.477Z",
      "ctime": "2018-09-30T20:52:27.477Z",
      "birthtime": "2018-09-15T21:00:58.351Z"
    },
    "42305117-d7ec-5b49-8174-ed54df725c2e": {
      "id": "42305117-d7ec-5b49-8174-ed54df725c2e",
      "children": [
        "7dee6c2e-c05c-5f6e-a526-d7f5c9ef3265"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "5770cbcbbcf6cb67e59205bf9b284ee1",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/leveraging-immutability-in-react.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/leveraging-immutability-in-react.md",
      "relativePath": "leveraging-immutability-in-react.md",
      "extension": "md",
      "size": 5770,
      "prettySize": "5.77 kB",
      "modifiedTime": "2018-09-30T20:52:36.447Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-09-30T20:52:36.447Z",
      "birthTime": "2018-09-15T20:59:09.601Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "leveraging-immutability-in-react.md",
      "ext": ".md",
      "name": "leveraging-immutability-in-react",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 2991430,
      "blocks": 16,
      "atimeMs": 1540135197513.9014,
      "mtimeMs": 1538340756447.3223,
      "ctimeMs": 1538340756447.3223,
      "birthtimeMs": 1537045149600.5352,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-09-30T20:52:36.447Z",
      "ctime": "2018-09-30T20:52:36.447Z",
      "birthtime": "2018-09-15T20:59:09.601Z"
    },
    "4e6a29de-aee9-55df-9a0a-d01a48c0bae1": {
      "id": "4e6a29de-aee9-55df-9a0a-d01a48c0bae1",
      "children": [
        "4a464680-e690-5613-804d-8fcf3746b0d6"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "20abe98c7f496ef2b85dc7b366ee3702",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/hidden-costs-of-postgresql-jsonb.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/hidden-costs-of-postgresql-jsonb.md",
      "relativePath": "hidden-costs-of-postgresql-jsonb.md",
      "extension": "md",
      "size": 9368,
      "prettySize": "9.37 kB",
      "modifiedTime": "2018-10-19T03:26:37.843Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-10-19T03:26:37.843Z",
      "birthTime": "2018-10-18T03:20:23.620Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "hidden-costs-of-postgresql-jsonb.md",
      "ext": ".md",
      "name": "hidden-costs-of-postgresql-jsonb",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 6040264,
      "blocks": 24,
      "atimeMs": 1540135197513.8826,
      "mtimeMs": 1539919597843.195,
      "ctimeMs": 1539919597843.195,
      "birthtimeMs": 1539832823620.4905,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-10-19T03:26:37.843Z",
      "ctime": "2018-10-19T03:26:37.843Z",
      "birthtime": "2018-10-18T03:20:23.620Z"
    },
    "be8aa103-c052-5e96-831f-dbc6366bd760": {
      "id": "be8aa103-c052-5e96-831f-dbc6366bd760",
      "children": [
        "ed26cf70-9fc2-52f1-b8c2-ceb799c50288"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "03bb59474752cd4e2efc828ae5050185",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/regex-and-automated-test-fuzzing.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/regex-and-automated-test-fuzzing.md",
      "relativePath": "regex-and-automated-test-fuzzing.md",
      "extension": "md",
      "size": 7143,
      "prettySize": "7.14 kB",
      "modifiedTime": "2018-10-18T03:20:23.622Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-10-18T03:20:23.622Z",
      "birthTime": "2018-10-18T03:20:23.621Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "regex-and-automated-test-fuzzing.md",
      "ext": ".md",
      "name": "regex-and-automated-test-fuzzing",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 6040265,
      "blocks": 16,
      "atimeMs": 1540135197513.9397,
      "mtimeMs": 1539832823621.5322,
      "ctimeMs": 1539832823621.5322,
      "birthtimeMs": 1539832823621.4275,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-10-18T03:20:23.622Z",
      "ctime": "2018-10-18T03:20:23.622Z",
      "birthtime": "2018-10-18T03:20:23.621Z"
    },
    "1e15d6bd-bf32-5b07-9d7f-c146fa0ac901": {
      "id": "1e15d6bd-bf32-5b07-9d7f-c146fa0ac901",
      "children": [
        "7d3c9af5-23a6-5b5e-a90a-cd9cf594f734"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "3b7e09e233991241bd839f213138f65a",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/optimizing-elasticsearch-score.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/optimizing-elasticsearch-score.md",
      "relativePath": "optimizing-elasticsearch-score.md",
      "extension": "md",
      "size": 5583,
      "prettySize": "5.58 kB",
      "modifiedTime": "2018-10-12T03:21:08.438Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-10-12T03:21:08.438Z",
      "birthTime": "2018-10-12T02:38:53.562Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "optimizing-elasticsearch-score.md",
      "ext": ".md",
      "name": "optimizing-elasticsearch-score",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 5586813,
      "blocks": 16,
      "atimeMs": 1540135197513.9211,
      "mtimeMs": 1539314468437.9102,
      "ctimeMs": 1539314468437.9102,
      "birthtimeMs": 1539311933561.6282,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-10-12T03:21:08.438Z",
      "ctime": "2018-10-12T03:21:08.438Z",
      "birthtime": "2018-10-12T02:38:53.562Z"
    },
    "1859d7d4-53db-5da3-aba1-f0cf7f4ffa9b": {
      "id": "1859d7d4-53db-5da3-aba1-f0cf7f4ffa9b",
      "children": [
        "017704d5-98aa-5eef-8086-8c15e94510d0"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "cc382de322a21d2b7ce23f47acf05f8b",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/scraping-the-web-with-puppeteer-lessons-learned.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/scraping-the-web-with-puppeteer-lessons-learned.md",
      "relativePath": "scraping-the-web-with-puppeteer-lessons-learned.md",
      "extension": "md",
      "size": 7936,
      "prettySize": "7.94 kB",
      "modifiedTime": "2018-10-20T16:17:30.696Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-10-20T16:17:30.696Z",
      "birthTime": "2018-09-15T20:59:09.601Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "scraping-the-web-with-puppeteer-lessons-learned.md",
      "ext": ".md",
      "name": "scraping-the-web-with-puppeteer-lessons-learned",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 2991432,
      "blocks": 16,
      "atimeMs": 1540135197513.96,
      "mtimeMs": 1540052250696.1074,
      "ctimeMs": 1540052250696.1074,
      "birthtimeMs": 1537045149601.369,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-10-20T16:17:30.696Z",
      "ctime": "2018-10-20T16:17:30.696Z",
      "birthtime": "2018-09-15T20:59:09.601Z"
    },
    "6f826141-57a4-5754-9ef3-314cc8204e82": {
      "id": "6f826141-57a4-5754-9ef3-314cc8204e82",
      "children": [
        "86e7f107-140a-5cdd-bbf5-d54112d285b9"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "ceed23c81b9feacfb662ace905baf859",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres.md",
      "relativePath": "using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres.md",
      "extension": "md",
      "size": 4960,
      "prettySize": "4.96 kB",
      "modifiedTime": "2018-10-20T16:02:45.611Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-10-20T16:02:45.611Z",
      "birthTime": "2018-10-19T02:30:02.251Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres.md",
      "ext": ".md",
      "name": "using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 6104197,
      "blocks": 16,
      "atimeMs": 1540135197513.98,
      "mtimeMs": 1540051365610.8801,
      "ctimeMs": 1540051365610.8801,
      "birthtimeMs": 1539916202250.8716,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-10-20T16:02:45.611Z",
      "ctime": "2018-10-20T16:02:45.611Z",
      "birthtime": "2018-10-19T02:30:02.251Z"
    },
    "1e0fb61f-1c42-5e94-89eb-eed2ce2b86a3": {
      "id": "1e0fb61f-1c42-5e94-89eb-eed2ce2b86a3",
      "children": [
        "74ada755-89ba-515c-9224-f120cb838f38"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "138345102fff377f54b23cf5e8e00078",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/using-reduce.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/using-reduce.md",
      "relativePath": "using-reduce.md",
      "extension": "md",
      "size": 9538,
      "prettySize": "9.54 kB",
      "modifiedTime": "2018-09-30T20:52:54.462Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-09-30T20:52:54.462Z",
      "birthTime": "2018-09-15T20:59:09.602Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "using-reduce.md",
      "ext": ".md",
      "name": "using-reduce",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 2991433,
      "blocks": 24,
      "atimeMs": 1540135197513.9988,
      "mtimeMs": 1538340774462.2556,
      "ctimeMs": 1538340774462.2556,
      "birthtimeMs": 1537045149601.6443,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-09-30T20:52:54.462Z",
      "ctime": "2018-09-30T20:52:54.462Z",
      "birthtime": "2018-09-15T20:59:09.602Z"
    },
    "1f776510-745f-566f-a097-cfb98ce26bf3": {
      "id": "1f776510-745f-566f-a097-cfb98ce26bf3",
      "children": [
        "d0cf29a8-abdd-58fe-8c0b-770f6384a439"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "08c975661d0a5d663efe7b981a636962",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/you're-hiring-programmers-wrong-a-case-for-interview-standardization.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/you're-hiring-programmers-wrong-a-case-for-interview-standardization.md",
      "relativePath": "you're-hiring-programmers-wrong-a-case-for-interview-standardization.md",
      "extension": "md",
      "size": 4893,
      "prettySize": "4.89 kB",
      "modifiedTime": "2018-09-30T20:53:11.209Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-09-30T20:53:11.209Z",
      "birthTime": "2018-09-15T20:59:09.602Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "you're-hiring-programmers-wrong-a-case-for-interview-standardization.md",
      "ext": ".md",
      "name": "you're-hiring-programmers-wrong-a-case-for-interview-standardization",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 2991435,
      "blocks": 16,
      "atimeMs": 1540135197514.141,
      "mtimeMs": 1538340791208.7024,
      "ctimeMs": 1538340791208.7024,
      "birthtimeMs": 1537045149602.225,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-09-30T20:53:11.209Z",
      "ctime": "2018-09-30T20:53:11.209Z",
      "birthtime": "2018-09-15T20:59:09.602Z"
    },
    "c268b9c5-75fe-5248-b049-abd4c29e1488": {
      "id": "c268b9c5-75fe-5248-b049-abd4c29e1488",
      "children": [
        "a9b06f31-6aa6-5fc0-928d-666b4b718c09"
      ],
      "parent": "___SOURCE___",
      "internal": {
        "contentDigest": "e858c3109a4c548eca202b524f1e0fa6",
        "type": "File",
        "mediaType": "text/markdown",
        "description": "File \"content/_posts/write-your-own-redux-connect.md\"",
        "owner": "gatsby-source-filesystem"
      },
      "sourceInstanceName": "posts",
      "absolutePath": "/Users/nickdrane/projects/blog/content/_posts/write-your-own-redux-connect.md",
      "relativePath": "write-your-own-redux-connect.md",
      "extension": "md",
      "size": 17139,
      "prettySize": "17.1 kB",
      "modifiedTime": "2018-09-30T20:53:07.513Z",
      "accessTime": "2018-10-21T15:19:57.514Z",
      "changeTime": "2018-09-30T20:53:07.513Z",
      "birthTime": "2018-09-15T20:59:09.602Z",
      "root": "/",
      "dir": "/Users/nickdrane/projects/blog/content/_posts",
      "base": "write-your-own-redux-connect.md",
      "ext": ".md",
      "name": "write-your-own-redux-connect",
      "relativeDirectory": "",
      "dev": 16777220,
      "mode": 33188,
      "nlink": 1,
      "uid": 501,
      "gid": 20,
      "rdev": 0,
      "blksize": 4194304,
      "ino": 2991434,
      "blocks": 40,
      "atimeMs": 1540135197514.018,
      "mtimeMs": 1538340787512.51,
      "ctimeMs": 1538340787512.51,
      "birthtimeMs": 1537045149601.9722,
      "atime": "2018-10-21T15:19:57.514Z",
      "mtime": "2018-09-30T20:53:07.513Z",
      "ctime": "2018-09-30T20:53:07.513Z",
      "birthtime": "2018-09-15T20:59:09.602Z"
    },
    "44b69257-3ece-526d-a8d9-25a8319c3e69": {
      "id": "44b69257-3ece-526d-a8d9-25a8319c3e69",
      "children": [],
      "parent": "acd24474-09d5-5ec7-8a1b-0bf9a5e4dc60",
      "internal": {
        "content": "\nTwo months ago I purchased a GPS device and associated service plan from [SPOT](https://www.findmespot.com/en/). Today, upon trying to cancel the service, the customer service representative informed me that I had accidentally enrolled myself into a 1 year, $250 contract and that I was unable to cancel. He told me that if I blocked the monthly charges against my credit card that they would report the debt to a collections agency. I was initially upset but soon realized it was a great opportunity to talk about ethics in software engineering.\n\n<!-- more -->\n\n## SPOT's Marketing Materials\n\nThe representative told me to look at their website where he claimed it would be very clear that I was signing up for a contract service. When I Googled \"spot service plan\", the first link I clicked on directed me to these marketing materials:\n\n![Spot Marketing Website](/images/spot-marketing-material.png)\n\nIf you look at the image above, it appears as if several deliberate decisions have been made to disguise the total cost of the month-to-month arrangement.\n\n1. The font of the month-to-month price is large, bold, and colorful. It's designed to stand out\n2. The 12 month contractual stipulation, on the other hand, is written in unnecessarily small font\n3. The contractual stipulation is low contrast and visually underwhelming, particularly when juxtaposed against the big, bright, and bold \"monthly cost\" header\n4. The terms of the contract are visually separated from the price\n\nNone of this could be chalked up to accident, except perhaps point 3. Ultimately, this page's content is designed to mislead the consumer, hurting them for the business's gain.<sup>[1](#footnote1)</sup>\n\n## Software Ethics and Mainstream Media\n\nThe software industry is rife with ethics scandals, and they are rarely cut and dry. Some examples\n\n1. [Dozens of iPhone applications selling user location data](https://techcrunch.com/2018/09/07/a-dozen-popular-iphone-apps-caught-quietly-sending-user-locations-to-monetization-firms/)\n2. [United States surveillance software (PRISMA)](https://en.wikipedia.org/wiki/PRISM_(surveillance_program)\n3. [DOD's Project Maven (drone imaging AI)](https://money.cnn.com/2018/06/01/technology/google-maven-contract/index.html)\n\nTwo arguments exist in each of these scenarios. iPhone application engineers might argue that they aren't hurting consumers by selling their location information. Governments argue that they protect their citizens through surveillance software, even if that's at the cost of citizen privacy. An engineer of that very software might believe that the code itself is not unethical, only improper use of it. A utilitarian might argue that software that facilitates accurate drone strikes is ethical if the cost of the deaths of its targets is outweighed by its net good. Ultimately, it's often unclear what constitutes unethical. But it raises a good question: as software engineers, what ethical code should we live by?\n\n## Ethics for the Average Engineer\n\nIt's easy to get consumed in the minutia. The media tends to focus on ethics scandals that have immense gravity, either in the scope of the number of people they affect or in the magnitude with which they affect their targets. But the majority of software engineers aren't writing software that necessitates answering the above questions. Most of us are writing simple e-commerce applications or iPhone apps -- boring software.\n\nThe case study with SPOT is interesting precisely because of how boring it is. It's true to life, and it serves as a reminder that the average engineer also faces ethical dilemmas. And fortunately for us, these \"ethical dilemmas\" are sometimes pretty cut and dry, provided we have the awareness to recognize them and the courage to speak up. In the case with SPOT, I think most people would agree that making an effort to mislead the consumer is dishonest and thus unethical.\n\n## The Takeaway\n\nJust because you work on boring software does not mean that it can't negatively impact your users in an unethical way. The first step towards protecting users is increased developer awareness. As a developer, it's essential to ask how your software affects your users. If it might hurt them, step back and consider the the greater picture. If you think your code might manipulate or mislead your users, it's time to speak up.\n\n#### Footnotes\n\n<a name=\"footnote1\">1</a>: I think it's important to be fair here. The marketing material across the site is inconsistent and ranges in the clarity of its term of service. [This page](https://www.findmespot.com/en/index.php?cid=131) makes the 12 month contract abundantly clear. [This one](https://www.findmespot.com/en/index.php?cid=130) falls somewhere in the middle.\n",
        "type": "MarkdownRemark",
        "contentDigest": "b054b3d794f8ae22bb154045f829de3f",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Ethical Engineering for the Average Engineer",
        "date": "2018-09-16T15:41:16.000Z",
        "categories": [
          [
            "Software Ethics"
          ]
        ],
        "_PARENT": "acd24474-09d5-5ec7-8a1b-0bf9a5e4dc60"
      },
      "excerpt": "\nTwo months ago I purchased a GPS device and associated service plan from [SPOT](https://www.findmespot.com/en/). Today, upon trying to cancel the service, the customer service representative informed me that I had accidentally enrolled myself into a 1 year, $250 contract and that I was unable to cancel. He told me that if I blocked the monthly charges against my credit card that they would report the debt to a collections agency. I was initially upset but soon realized it was a great opportunity to talk about ethics in software engineering.\n\n",
      "rawMarkdownBody": "\nTwo months ago I purchased a GPS device and associated service plan from [SPOT](https://www.findmespot.com/en/). Today, upon trying to cancel the service, the customer service representative informed me that I had accidentally enrolled myself into a 1 year, $250 contract and that I was unable to cancel. He told me that if I blocked the monthly charges against my credit card that they would report the debt to a collections agency. I was initially upset but soon realized it was a great opportunity to talk about ethics in software engineering.\n\n<!-- more -->\n\n## SPOT's Marketing Materials\n\nThe representative told me to look at their website where he claimed it would be very clear that I was signing up for a contract service. When I Googled \"spot service plan\", the first link I clicked on directed me to these marketing materials:\n\n![Spot Marketing Website](/images/spot-marketing-material.png)\n\nIf you look at the image above, it appears as if several deliberate decisions have been made to disguise the total cost of the month-to-month arrangement.\n\n1. The font of the month-to-month price is large, bold, and colorful. It's designed to stand out\n2. The 12 month contractual stipulation, on the other hand, is written in unnecessarily small font\n3. The contractual stipulation is low contrast and visually underwhelming, particularly when juxtaposed against the big, bright, and bold \"monthly cost\" header\n4. The terms of the contract are visually separated from the price\n\nNone of this could be chalked up to accident, except perhaps point 3. Ultimately, this page's content is designed to mislead the consumer, hurting them for the business's gain.<sup>[1](#footnote1)</sup>\n\n## Software Ethics and Mainstream Media\n\nThe software industry is rife with ethics scandals, and they are rarely cut and dry. Some examples\n\n1. [Dozens of iPhone applications selling user location data](https://techcrunch.com/2018/09/07/a-dozen-popular-iphone-apps-caught-quietly-sending-user-locations-to-monetization-firms/)\n2. [United States surveillance software (PRISMA)](https://en.wikipedia.org/wiki/PRISM_(surveillance_program)\n3. [DOD's Project Maven (drone imaging AI)](https://money.cnn.com/2018/06/01/technology/google-maven-contract/index.html)\n\nTwo arguments exist in each of these scenarios. iPhone application engineers might argue that they aren't hurting consumers by selling their location information. Governments argue that they protect their citizens through surveillance software, even if that's at the cost of citizen privacy. An engineer of that very software might believe that the code itself is not unethical, only improper use of it. A utilitarian might argue that software that facilitates accurate drone strikes is ethical if the cost of the deaths of its targets is outweighed by its net good. Ultimately, it's often unclear what constitutes unethical. But it raises a good question: as software engineers, what ethical code should we live by?\n\n## Ethics for the Average Engineer\n\nIt's easy to get consumed in the minutia. The media tends to focus on ethics scandals that have immense gravity, either in the scope of the number of people they affect or in the magnitude with which they affect their targets. But the majority of software engineers aren't writing software that necessitates answering the above questions. Most of us are writing simple e-commerce applications or iPhone apps -- boring software.\n\nThe case study with SPOT is interesting precisely because of how boring it is. It's true to life, and it serves as a reminder that the average engineer also faces ethical dilemmas. And fortunately for us, these \"ethical dilemmas\" are sometimes pretty cut and dry, provided we have the awareness to recognize them and the courage to speak up. In the case with SPOT, I think most people would agree that making an effort to mislead the consumer is dishonest and thus unethical.\n\n## The Takeaway\n\nJust because you work on boring software does not mean that it can't negatively impact your users in an unethical way. The first step towards protecting users is increased developer awareness. As a developer, it's essential to ask how your software affects your users. If it might hurt them, step back and consider the the greater picture. If you think your code might manipulate or mislead your users, it's time to speak up.\n\n#### Footnotes\n\n<a name=\"footnote1\">1</a>: I think it's important to be fair here. The marketing material across the site is inconsistent and ranges in the clarity of its term of service. [This page](https://www.findmespot.com/en/index.php?cid=131) makes the 12 month contract abundantly clear. [This one](https://www.findmespot.com/en/index.php?cid=130) falls somewhere in the middle.\n",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/ethical-engineering-for-the-average-engineer.md"
    },
    "7dee6c2e-c05c-5f6e-a526-d7f5c9ef3265": {
      "id": "7dee6c2e-c05c-5f6e-a526-d7f5c9ef3265",
      "children": [],
      "parent": "42305117-d7ec-5b49-8174-ed54df725c2e",
      "internal": {
        "content": "\nReact has taken the web development community by a storm, and with it functional programming concepts have embedded themselves in the mainstream. One common statement you will often read is that all state in React should be immutable, and this practice is justified as necessary for performance reasons. This statement is entirely true, but it only tells half the truth. Immutability alone will not yield any performance gains in React (it'll actually make things slower).\n\n<!-- more -->\n\n## The Quick Answer\nYou can reap the gains from immutable state in React if you inherit from [React.PureComponent](https://reactjs.org/docs/react-api.html#reactpurecomponent) instead of `React.Component`\n\n```js\nclass MyComponent extends PureComponent {...}\n```\n\n## A Broader Perspective\nSo why does the above code work? That requires a little bit of knowledge about the rendering process.\n\n## Virtual DOM\nA component's `render` function returns a tree of React elements, also known as the virtual DOM. This data structure is a representation of the browser's DOM that React can manipulate in a performant way. In essence, it's a 1-1 mapping to the browser DOM.\n\nWhenever state or props change, instead of updating the browser DOM directly, React will call the `render` function of the updated component, getting its new virtual DOM, and it will compare that virtual DOM to the previous render tree (aka the old virtual DOM). This comparison process (known as reconciliation) allows it to identify the minimum set of changes that need to be made to the browser DOM, allowing for for massive performance gains. If the two trees are identical, then no changes need to be made to the UI.\n\n## Should a Component Re-render\nWhen `setState` is called on a component, that component (and all of the components it renders) go through a two step process for React determine if the UI should update.\n\nBefore the virtual DOM is rendered, React first runs a component's [shouldComponentUpdate](https://reactjs.org/docs/react-component.html#shouldcomponentupdate) lifecycle method\n\n## shouldComponentUpdate\n\nIt is your choice to implement this method. It takes as parameters the new state and props and should return a boolean indicating whether the component should re-render. React will only proceed with re-rendering if you return `true` from this function. By default, `shouldComponentUpdate` always returns `true`.\n\nLet's look at a simple example\n\n```js\nclass Counter extends React.Component() {\n  constructor() {\n    this.state = {\n      counter: 0\n    }\n  }\n\n  shouldComponentUpdate(nextProps, nextState) {\n    // We are comparing the state before setState\n    // is called to the state after it is called.\n    // This component will only re-render if\n    // state.counter changes\n    return this.state.counter !== nextState.counter\n  }\n\n  render() {\n    return <span>{this.state.counter}</span>\n  }\n}\n```\n\nCurrently `Counter` will only re-render if `state.counter` changes. Suppose we remove the `shouldComponentUpdate` above, is there another circumstance where `Counter`' re-renders? It would re-render if a component that renders `Counter` re-renders! It does not matter that `Counter`'s render function does not depend on props!\n\nSo, when `shouldComponentUpdate` returns `true`, React begins the second part of the re-render process, the part where it invokes the render function, generating its virtual DOM. It will then compare that render tree to the old virtual DOM. In the scenario above, these two trees will always be identical (after all, the component doesn't even have a way to change state), and the entire operation will a be waste CPU cycles.\n\nIf `shouldComponentUpdate` returns `false`, we inform React that a re-render will not be necessary, sparing it an unnecessary to call a component's render function and the comparison of the render trees.\n\n## React.pureComponent\nWriting `shouldComponentUpdate` is sometimes a tedious task, and React supplies a helper to handle a very common `shouldComponentUpdate` scenario. I mentioned `React.pureComponent` above.\n\n```js\nclass MyComponent extends PureComponent {...}\n```\n\n`React.pureComponent` is a handy class that we can inherit from that implements `shouldComponentUpdate` as a shallow comparison across the old props and new props (or the old state and new state). If there are no *shallow* differences between these objects, then `shouldComponentUpdate` will return false, and the virtual DOM's re-rendering process can be skipped entirely, resulting in potentially enormous performance savings.\n\n## Why Does Immutability Matter?\nImmutability matters because `React.pureComponent` is going to do shallow comparisons against the prop and state objects. And, as you hopefully know, a shallow comparison's referential equality checks will only yield the expected results if immutability is respected.\n\nIf you are not properly respecting immutability, then you are going to run into scenarios where your components fail to re-render when they actually should. A failure to respect immutability will cause a shallow comparison to return false (meaning `shouldComponentUpdate` returns false) because the old props and new props (or the old state and new state) reference the same object in memory, despite the fact that props/state changed.\n\nWhen used carefully however, `React.pureComponent` can yield enormous performance improvements at almost no cost. All you need to do is inherit your components from `React.pureComponent` instead of `React.Component` and respect immutability when changing state.\n\nI hope you found this article helpful. Please feel free to email me to reach out if you have questions.",
        "type": "MarkdownRemark",
        "contentDigest": "99205565bad1234a64b9c2849173f4ef",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Leveraging Immutability in React",
        "date": "2017-09-27T09:30:27.000Z",
        "categories": [
          [
            "React"
          ],
          [
            "Immutability"
          ]
        ],
        "_PARENT": "42305117-d7ec-5b49-8174-ed54df725c2e"
      },
      "excerpt": "\nReact has taken the web development community by a storm, and with it functional programming concepts have embedded themselves in the mainstream. One common statement you will often read is that all state in React should be immutable, and this practice is justified as necessary for performance reasons. This statement is entirely true, but it only tells half the truth. Immutability alone will not yield any performance gains in React (it'll actually make things slower).\n\n",
      "rawMarkdownBody": "\nReact has taken the web development community by a storm, and with it functional programming concepts have embedded themselves in the mainstream. One common statement you will often read is that all state in React should be immutable, and this practice is justified as necessary for performance reasons. This statement is entirely true, but it only tells half the truth. Immutability alone will not yield any performance gains in React (it'll actually make things slower).\n\n<!-- more -->\n\n## The Quick Answer\nYou can reap the gains from immutable state in React if you inherit from [React.PureComponent](https://reactjs.org/docs/react-api.html#reactpurecomponent) instead of `React.Component`\n\n```js\nclass MyComponent extends PureComponent {...}\n```\n\n## A Broader Perspective\nSo why does the above code work? That requires a little bit of knowledge about the rendering process.\n\n## Virtual DOM\nA component's `render` function returns a tree of React elements, also known as the virtual DOM. This data structure is a representation of the browser's DOM that React can manipulate in a performant way. In essence, it's a 1-1 mapping to the browser DOM.\n\nWhenever state or props change, instead of updating the browser DOM directly, React will call the `render` function of the updated component, getting its new virtual DOM, and it will compare that virtual DOM to the previous render tree (aka the old virtual DOM). This comparison process (known as reconciliation) allows it to identify the minimum set of changes that need to be made to the browser DOM, allowing for for massive performance gains. If the two trees are identical, then no changes need to be made to the UI.\n\n## Should a Component Re-render\nWhen `setState` is called on a component, that component (and all of the components it renders) go through a two step process for React determine if the UI should update.\n\nBefore the virtual DOM is rendered, React first runs a component's [shouldComponentUpdate](https://reactjs.org/docs/react-component.html#shouldcomponentupdate) lifecycle method\n\n## shouldComponentUpdate\n\nIt is your choice to implement this method. It takes as parameters the new state and props and should return a boolean indicating whether the component should re-render. React will only proceed with re-rendering if you return `true` from this function. By default, `shouldComponentUpdate` always returns `true`.\n\nLet's look at a simple example\n\n```js\nclass Counter extends React.Component() {\n  constructor() {\n    this.state = {\n      counter: 0\n    }\n  }\n\n  shouldComponentUpdate(nextProps, nextState) {\n    // We are comparing the state before setState\n    // is called to the state after it is called.\n    // This component will only re-render if\n    // state.counter changes\n    return this.state.counter !== nextState.counter\n  }\n\n  render() {\n    return <span>{this.state.counter}</span>\n  }\n}\n```\n\nCurrently `Counter` will only re-render if `state.counter` changes. Suppose we remove the `shouldComponentUpdate` above, is there another circumstance where `Counter`' re-renders? It would re-render if a component that renders `Counter` re-renders! It does not matter that `Counter`'s render function does not depend on props!\n\nSo, when `shouldComponentUpdate` returns `true`, React begins the second part of the re-render process, the part where it invokes the render function, generating its virtual DOM. It will then compare that render tree to the old virtual DOM. In the scenario above, these two trees will always be identical (after all, the component doesn't even have a way to change state), and the entire operation will a be waste CPU cycles.\n\nIf `shouldComponentUpdate` returns `false`, we inform React that a re-render will not be necessary, sparing it an unnecessary to call a component's render function and the comparison of the render trees.\n\n## React.pureComponent\nWriting `shouldComponentUpdate` is sometimes a tedious task, and React supplies a helper to handle a very common `shouldComponentUpdate` scenario. I mentioned `React.pureComponent` above.\n\n```js\nclass MyComponent extends PureComponent {...}\n```\n\n`React.pureComponent` is a handy class that we can inherit from that implements `shouldComponentUpdate` as a shallow comparison across the old props and new props (or the old state and new state). If there are no *shallow* differences between these objects, then `shouldComponentUpdate` will return false, and the virtual DOM's re-rendering process can be skipped entirely, resulting in potentially enormous performance savings.\n\n## Why Does Immutability Matter?\nImmutability matters because `React.pureComponent` is going to do shallow comparisons against the prop and state objects. And, as you hopefully know, a shallow comparison's referential equality checks will only yield the expected results if immutability is respected.\n\nIf you are not properly respecting immutability, then you are going to run into scenarios where your components fail to re-render when they actually should. A failure to respect immutability will cause a shallow comparison to return false (meaning `shouldComponentUpdate` returns false) because the old props and new props (or the old state and new state) reference the same object in memory, despite the fact that props/state changed.\n\nWhen used carefully however, `React.pureComponent` can yield enormous performance improvements at almost no cost. All you need to do is inherit your components from `React.pureComponent` instead of `React.Component` and respect immutability when changing state.\n\nI hope you found this article helpful. Please feel free to email me to reach out if you have questions.",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/leveraging-immutability-in-react.md"
    },
    "ed26cf70-9fc2-52f1-b8c2-ceb799c50288": {
      "id": "ed26cf70-9fc2-52f1-b8c2-ceb799c50288",
      "children": [],
      "parent": "be8aa103-c052-5e96-831f-dbc6366bd760",
      "internal": {
        "content": "\n\nI posted my article [Build Your Own Regex Engine](https://nickdrane.com/build-your-own-regex/) on Reddit the other day, and one of the commenters claimed that the implementation should be trivial to break. Since I had already tested my program against a customized suite of tests, the remark got me thinking about how I could further increase my confidence in the correctness of my program. One extremely low cost however effective strategy for identifying faults in software is known as fuzzing.\n\n<!-- more -->\n\n## What is Fuzzing?\n\nFuzzing is a automated testing technique where a program is provided a series of invalid or randomly generated inputs. If we were testing an HTTP API, we might send randomized combinations of query parameters and ensure that our server always returns a 2xx status code. Since Javascript comes with a regular expression engine, my fuzzer asserts that given the same random input, both engine's return the same output.\n\n## Specifying the Grammar\n\nThe first step is to specify the grammar that our regex engine supports.\n\n```js\nconst lowercase = \"abcdefghijklmnopqrstuvwxyz\".split(\"\");\nconst uppercase = lowercase.map(letter => letter.toUpperCase());\nconst special = [\"?\", \"*\", \".\"];\nconst regexGrammar = special.concat(lowercase, uppercase);\n```\n\nYou might notice we skipped the `^` and `$` characters. More on these in a little bit.\n\n## Generated Valid Regular Expressions\n\nWe want to write a function `generateRegex` that will select `n` random characters from the `regexGrammar` and concatenate them together into a string. This string will be used to create a test regex.\n\nHere are three possible returns values of `generateRegex`:\n\n1. `.AnrQ?QNLQX.syBsOcJlbJZd`\n2. `.LkuZ?Ynj`\n3. `.UN?eiyddhXvyNj`\n\n```js\nfunction generateRegex(n) {\n  let regexString = new Array(n)\n    .fill(0)\n    .map(chooseOne)\n    .join(\"\");\n\n  return regexString;\n}\n\n// Pick one element randomly from the grammar and return it\nfunction chooseOne() {\n  return regexGrammar[Math.floor(Math.random() * regexGrammar.length)];\n}\n```\n\n## Removing Invalid Regex Strings\n\nMy regex engine only deals with a very small subset of available regex syntax, and furthermore, it does not contain any error handling. What happens if `generateRegex` returns the pattern `**` or `^*`? My regex engine was never designed to handle these inputs, though they are possible outputs of `generateRegex`. We need to make a choice about how to handle these expressions. Since the primary goal of my regex engine is accessibility and simplicity of implementation, I'm not about to begin supporting these edge cases. That means my fuzzer should not generate them either.\n\nOne solution to determine if a given regex string is valid is to specify my regex engine's allowable grammar in [BNF](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form). BNF is a formal notation for specifying the syntax of a language. Given this BNF notation, I could ask another program if the randomly generated regex string can be created using my BNF specification. This sounds like a little more work than I want, however, since the invalid cases can simply be manually enumerated and filtered.\n\n```js\nfunction validRegex(regexString) {\n  return (\n    // None of the following sequences are properly\n    // defined by my regex engine\n    regexString.indexOf(\"**\") === -1 &&\n    regexString.indexOf(\"??\") === -1 &&\n    regexString.indexOf(\"*?\") === -1 &&\n    regexString.indexOf(\"?*\") === -1 &&\n    regexString.indexOf(\"^?\") === -1 &&\n    regexString.indexOf(\"^*\") === -1 &&\n    !regexString.startsWith(\"*\") &&\n    !regexString.startsWith(\"?\")\n  );\n}\n\nfunction generateRegex(n) {\n  let regexString = new Array(n)\n    .fill(0)\n    .map(chooseOne)\n    .join(\"\");\n\n  // If the generated string is valid, return it\n  if (validRegex(regexString)) {\n    return regexString;\n  // Otherwise generate a new string and return that\n  } else {\n    return generateRegexString(n);\n  }\n}\n```\n\nOne more modification to `generateRegex` is necessary to support `^` and `$`, and then we are basically done.\n\n```js\nfunction generateRegex(n) {\n  ...\n  // We need to ensure that '^' and '$' only go at the beginning\n  // and the end of the string, respectively.\n  // Give each a 10% probability of appearing in a string\n  if (Math.random() < 0.1) regexString = \"^\" + regexString;\n  if (Math.random() < 0.1) regexString = regexString + \"$\";\n  ...\n}\n```\n\n## Comparing Regex Implementations\n\nAll that is required now is to repeatedly invoke `generateRegex` a fixed number of times and then compare the output of the native JS implementation with the output of my implementation.\n\n```js\n// The corpus is the string of text we are matching the pattern against.\n// I used a segment of Gulliver's Travels from Project Gutenberg.\nfunction fuzzer(totalTests, corpus) {\n  const maxRegexLength = 50; // max will actually be 50 - 1\n  let testsRun = 0;\n  while (testsRun < totalTests) {\n    const regexLength = getRandomInt(1, maxRegexLength);\n    const regexString = generateRegexString(regexLength);\n    const testRegex = new RegExp(regexString);\n    try {\n      assert.equal(testRegex.test(corpus), myRegexEngine(regexString, corpus));\n    } catch (err) {\n      console.log(testRegex);\n    }\n    testsRun++;\n  }\n}\n\n// Thank you Mozzila :)\n// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/random\nfunction getRandomInt(min, max) {\n  min = Math.ceil(min);\n  max = Math.floor(max);\n  return Math.floor(Math.random() * (max - min)) + min;\n}\n```\n\n## Results\n\nI ran my fuzzer for a couple million randomly generated cases and ended up learning two things about my regex engine.\n\n1. My implementation fails extraordinarily with longer texts. I knew recursion would be a problem for any practical regex implementation (at least without [tail calls](https://en.wikipedia.org/wiki/Tail_call)) and would cause stack overflows, but I didn't expect it to fail with texts that were only a couple thousand words. I think this is because I make liberal use of backtracking algorithms in `matchQuestion` and `matchStar`. Since I was forced to test with a relatively short input text, it makes sense to use multiple text inputs to increase the probability of discovering an error.\n\n2. My implementation treats the `.` character differently than the native implementation. In the RegExp implementation, `.` will not match various line terminators (`\\n`, `\\r`, `\\u2028` or `\\u2029`). My implementation does.\n\n## Conclusion\n\nThe biggest takeaway is that fuzzing is an simple and inexpensive way to enumerate enormous sets of inputs and identify bugs in your software. This fuzzer took less than an hour to write.\n\nBut remember, this fuzzer's blessing of a couple million input combinations __does not__ verify the correctness of my program. Not even close. A fuzzer is a tool to identify potential errors. Unless you enumerate all possible inputs (completely impossible in this case where they are infinite), you are not guaranteed your program is error free.",
        "type": "MarkdownRemark",
        "contentDigest": "a99f00f74fc0ac71d65bda27bff2b92a",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Regex And Automated Test Fuzzing",
        "categories": [
          [
            "Regular Expressions"
          ],
          [
            "Javascript"
          ],
          [
            "Testing"
          ]
        ],
        "date": "2017-12-06T00:00:00.000Z",
        "_PARENT": "be8aa103-c052-5e96-831f-dbc6366bd760"
      },
      "excerpt": "\n\nI posted my article [Build Your Own Regex Engine](https://nickdrane.com/build-your-own-regex/) on Reddit the other day, and one of the commenters claimed that the implementation should be trivial to break. Since I had already tested my program against a customized suite of tests, the remark got me thinking about how I could further increase my confidence in the correctness of my program. One extremely low cost however effective strategy for identifying faults in software is known as fuzzing.\n\n",
      "rawMarkdownBody": "\n\nI posted my article [Build Your Own Regex Engine](https://nickdrane.com/build-your-own-regex/) on Reddit the other day, and one of the commenters claimed that the implementation should be trivial to break. Since I had already tested my program against a customized suite of tests, the remark got me thinking about how I could further increase my confidence in the correctness of my program. One extremely low cost however effective strategy for identifying faults in software is known as fuzzing.\n\n<!-- more -->\n\n## What is Fuzzing?\n\nFuzzing is a automated testing technique where a program is provided a series of invalid or randomly generated inputs. If we were testing an HTTP API, we might send randomized combinations of query parameters and ensure that our server always returns a 2xx status code. Since Javascript comes with a regular expression engine, my fuzzer asserts that given the same random input, both engine's return the same output.\n\n## Specifying the Grammar\n\nThe first step is to specify the grammar that our regex engine supports.\n\n```js\nconst lowercase = \"abcdefghijklmnopqrstuvwxyz\".split(\"\");\nconst uppercase = lowercase.map(letter => letter.toUpperCase());\nconst special = [\"?\", \"*\", \".\"];\nconst regexGrammar = special.concat(lowercase, uppercase);\n```\n\nYou might notice we skipped the `^` and `$` characters. More on these in a little bit.\n\n## Generated Valid Regular Expressions\n\nWe want to write a function `generateRegex` that will select `n` random characters from the `regexGrammar` and concatenate them together into a string. This string will be used to create a test regex.\n\nHere are three possible returns values of `generateRegex`:\n\n1. `.AnrQ?QNLQX.syBsOcJlbJZd`\n2. `.LkuZ?Ynj`\n3. `.UN?eiyddhXvyNj`\n\n```js\nfunction generateRegex(n) {\n  let regexString = new Array(n)\n    .fill(0)\n    .map(chooseOne)\n    .join(\"\");\n\n  return regexString;\n}\n\n// Pick one element randomly from the grammar and return it\nfunction chooseOne() {\n  return regexGrammar[Math.floor(Math.random() * regexGrammar.length)];\n}\n```\n\n## Removing Invalid Regex Strings\n\nMy regex engine only deals with a very small subset of available regex syntax, and furthermore, it does not contain any error handling. What happens if `generateRegex` returns the pattern `**` or `^*`? My regex engine was never designed to handle these inputs, though they are possible outputs of `generateRegex`. We need to make a choice about how to handle these expressions. Since the primary goal of my regex engine is accessibility and simplicity of implementation, I'm not about to begin supporting these edge cases. That means my fuzzer should not generate them either.\n\nOne solution to determine if a given regex string is valid is to specify my regex engine's allowable grammar in [BNF](https://en.wikipedia.org/wiki/Backus%E2%80%93Naur_form). BNF is a formal notation for specifying the syntax of a language. Given this BNF notation, I could ask another program if the randomly generated regex string can be created using my BNF specification. This sounds like a little more work than I want, however, since the invalid cases can simply be manually enumerated and filtered.\n\n```js\nfunction validRegex(regexString) {\n  return (\n    // None of the following sequences are properly\n    // defined by my regex engine\n    regexString.indexOf(\"**\") === -1 &&\n    regexString.indexOf(\"??\") === -1 &&\n    regexString.indexOf(\"*?\") === -1 &&\n    regexString.indexOf(\"?*\") === -1 &&\n    regexString.indexOf(\"^?\") === -1 &&\n    regexString.indexOf(\"^*\") === -1 &&\n    !regexString.startsWith(\"*\") &&\n    !regexString.startsWith(\"?\")\n  );\n}\n\nfunction generateRegex(n) {\n  let regexString = new Array(n)\n    .fill(0)\n    .map(chooseOne)\n    .join(\"\");\n\n  // If the generated string is valid, return it\n  if (validRegex(regexString)) {\n    return regexString;\n  // Otherwise generate a new string and return that\n  } else {\n    return generateRegexString(n);\n  }\n}\n```\n\nOne more modification to `generateRegex` is necessary to support `^` and `$`, and then we are basically done.\n\n```js\nfunction generateRegex(n) {\n  ...\n  // We need to ensure that '^' and '$' only go at the beginning\n  // and the end of the string, respectively.\n  // Give each a 10% probability of appearing in a string\n  if (Math.random() < 0.1) regexString = \"^\" + regexString;\n  if (Math.random() < 0.1) regexString = regexString + \"$\";\n  ...\n}\n```\n\n## Comparing Regex Implementations\n\nAll that is required now is to repeatedly invoke `generateRegex` a fixed number of times and then compare the output of the native JS implementation with the output of my implementation.\n\n```js\n// The corpus is the string of text we are matching the pattern against.\n// I used a segment of Gulliver's Travels from Project Gutenberg.\nfunction fuzzer(totalTests, corpus) {\n  const maxRegexLength = 50; // max will actually be 50 - 1\n  let testsRun = 0;\n  while (testsRun < totalTests) {\n    const regexLength = getRandomInt(1, maxRegexLength);\n    const regexString = generateRegexString(regexLength);\n    const testRegex = new RegExp(regexString);\n    try {\n      assert.equal(testRegex.test(corpus), myRegexEngine(regexString, corpus));\n    } catch (err) {\n      console.log(testRegex);\n    }\n    testsRun++;\n  }\n}\n\n// Thank you Mozzila :)\n// https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/random\nfunction getRandomInt(min, max) {\n  min = Math.ceil(min);\n  max = Math.floor(max);\n  return Math.floor(Math.random() * (max - min)) + min;\n}\n```\n\n## Results\n\nI ran my fuzzer for a couple million randomly generated cases and ended up learning two things about my regex engine.\n\n1. My implementation fails extraordinarily with longer texts. I knew recursion would be a problem for any practical regex implementation (at least without [tail calls](https://en.wikipedia.org/wiki/Tail_call)) and would cause stack overflows, but I didn't expect it to fail with texts that were only a couple thousand words. I think this is because I make liberal use of backtracking algorithms in `matchQuestion` and `matchStar`. Since I was forced to test with a relatively short input text, it makes sense to use multiple text inputs to increase the probability of discovering an error.\n\n2. My implementation treats the `.` character differently than the native implementation. In the RegExp implementation, `.` will not match various line terminators (`\\n`, `\\r`, `\\u2028` or `\\u2029`). My implementation does.\n\n## Conclusion\n\nThe biggest takeaway is that fuzzing is an simple and inexpensive way to enumerate enormous sets of inputs and identify bugs in your software. This fuzzer took less than an hour to write.\n\nBut remember, this fuzzer's blessing of a couple million input combinations __does not__ verify the correctness of my program. Not even close. A fuzzer is a tool to identify potential errors. Unless you enumerate all possible inputs (completely impossible in this case where they are infinite), you are not guaranteed your program is error free.",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/regex-and-automated-test-fuzzing.md"
    },
    "7d3c9af5-23a6-5b5e-a90a-cd9cf594f734": {
      "id": "7d3c9af5-23a6-5b5e-a90a-cd9cf594f734",
      "children": [],
      "parent": "1e15d6bd-bf32-5b07-9d7f-c146fa0ac901",
      "internal": {
        "content": "\nA client approached me with a puzzling problem:\n\nAt Fraight, we have an omnisearch interface backed by an Elasticsearch datastore. The interface allows users yo type a freetext query and get a list of database records sorted by relevancy. At it's core, this is a simple problem: if the user types in `Joe`, return all people whose name contains the word `Joe`. And indeed, returning all the `Joe's` in the system is trivial; the problem is that we worked hundreds, possibly even thousands of `Joes`. How do we identify the particular `Joe` that we care about?\n\n<!-- more -->\n\n## A Poor Solution\n\nWhen I worked at [Epic](https://www.epic.com/), we had a similar problem. We had a search interface that allowed us to look up patients. Unfortunately, our full names are not as unique as we like to believe, and a simple query for `Luke Smith` would surely bring the system to a halt. Epic solved this problem by providing additional fields. That's why (along with HIPPA reasons), when you call the doctor, they might ask for your birthdate or your address; this additional identifying information is used to pare down the search results. This solution is slow, cumbersome and was deemed wholly inadequate for us.\n\n## Fraight's Solution\n\nWe settled on two attributes that should influence the score of a particular record:\n\n1. How often we interact with an entity\n2. How recently we've interacted with an entity\n\nIt's important to know that most of the trucking organizations in our system have been worked with minimally. We needed a remove this noise from the search results. Phrased differently, if we regularly work with a particular `Great America Truckers` more than the other 1000 `Great America Truckers`, we want our partner to appear higher in the search results.\n\nSimilarly, if we have recently interacted with an organization, there's a good chance we will need to interact with them in the future. For example, if we've just initiated a conversation with a new business partner, there's a good chance we will continue interacting with them regularly in the near-term. It's important, however, to consider the time since our last interaction. If we interacted with someone yesterday or the day before, it's very important for them to be ranked higher than an organization we interacted with 10 days ago.\n\n## Getting the Data\n\nBoth of these solutions require populating our database with information regarding how recently we've interacted with particular entities. Fortunately, all of this can be done during the data ingestion phase when records are loaded in Elasticsearch. The only requirement is making sure to keep the information in Elasticsearch up to date with our PostgreSQL database\n\nThe one challenge with this solution is that it requires augmenting our documents with special data regarding total interactions and recency of interactions.  The huge advantage, however, is that it works seamlessly. We don't require there to be any special configuration when we begin working with a new partner.\n\n\n## Function Score Queries\n\nElasticsearch provides [function score queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html) that allow you to modify Elasticsearch's calculated score. Their documentation provides a really simple example explaining boost the relevancy of a document that has many `likes`:\n\n```JSON\nGET /_search\n{\n    \"query\": {\n        \"function_score\": {\n            \"field_value_factor\": {\n                \"field\": \"likes\",\n                \"factor\": 1.2,\n                \"modifier\": \"sqrt\",\n            }\n        }\n    }\n}\n```\n\nThis code says that every document's score will be `sqrt(1.2 * doc['likes'].value)`\n\nWe used a particular kind of Function Score Query known as a [Script Score](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-script-score).\n\nWe wanted our ultimate score to be a function of three values: the score returned from elasticsearch, the days since our last interaction, and our total interactions. Initially, our function looked something like this\n\n`newscore` = _score * doc['interactions'] *\n\nWe found that Elasticsearch's relevancy score was being overwhelmed by parters with a high number of interactions, so much so that given a search term of `nick drane`, we might return `nick smith`, simply because we've worked with nick smith more.\n\nWe needed to place an upperbound of the potential contribution of the total interactions. After initially looking at [logistic functions](https://en.wikipedia.org/wiki/Logistic_function) and other overly complicated strategies, we settled on a simple step function. If the number of interactions is greater than 25, then we multiply Elasticsearch's relevancy score by 2, otherwise we multiply it 1.\n\nThe frequency contribution's function was similarly simple.\n\n\n(could be some density/decay function that looks at density of interactions over time)\n\n## challenges\n\nKeeping the extra attributes up to date\n\nWe did try to combine two guassian curves but couldn't get it working\n\n\n\nSometimes was built using a `query_string` query against the `_all` field. Sometimes the returned results were good, but most of the time they fell short.  Their problem was simple: how do we ensure that the\n\n\n\n\nhealth care\nselect based on recent claim\nselect based on scheduled appointment\nselect based on upcoming event\nspecify table to search",
        "type": "MarkdownRemark",
        "contentDigest": "a032d6cd99701a523619f924277d8f61",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Optimizing Elasticsearch Score: How to Rank and Differentiate Similar Records",
        "date": "2017-10-11T00:00:00.000Z",
        "categories": [
          [
            "Elasticsearch"
          ]
        ],
        "_PARENT": "1e15d6bd-bf32-5b07-9d7f-c146fa0ac901"
      },
      "excerpt": "\nA client approached me with a puzzling problem:\n\nAt Fraight, we have an omnisearch interface backed by an Elasticsearch datastore. The interface allows users yo type a freetext query and get a list of database records sorted by relevancy. At it's core, this is a simple problem: if the user types in `Joe`, return all people whose name contains the word `Joe`. And indeed, returning all the `Joe's` in the system is trivial; the problem is that we worked hundreds, possibly even thousands of `Joes`. How do we identify the particular `Joe` that we care about?\n\n",
      "rawMarkdownBody": "\nA client approached me with a puzzling problem:\n\nAt Fraight, we have an omnisearch interface backed by an Elasticsearch datastore. The interface allows users yo type a freetext query and get a list of database records sorted by relevancy. At it's core, this is a simple problem: if the user types in `Joe`, return all people whose name contains the word `Joe`. And indeed, returning all the `Joe's` in the system is trivial; the problem is that we worked hundreds, possibly even thousands of `Joes`. How do we identify the particular `Joe` that we care about?\n\n<!-- more -->\n\n## A Poor Solution\n\nWhen I worked at [Epic](https://www.epic.com/), we had a similar problem. We had a search interface that allowed us to look up patients. Unfortunately, our full names are not as unique as we like to believe, and a simple query for `Luke Smith` would surely bring the system to a halt. Epic solved this problem by providing additional fields. That's why (along with HIPPA reasons), when you call the doctor, they might ask for your birthdate or your address; this additional identifying information is used to pare down the search results. This solution is slow, cumbersome and was deemed wholly inadequate for us.\n\n## Fraight's Solution\n\nWe settled on two attributes that should influence the score of a particular record:\n\n1. How often we interact with an entity\n2. How recently we've interacted with an entity\n\nIt's important to know that most of the trucking organizations in our system have been worked with minimally. We needed a remove this noise from the search results. Phrased differently, if we regularly work with a particular `Great America Truckers` more than the other 1000 `Great America Truckers`, we want our partner to appear higher in the search results.\n\nSimilarly, if we have recently interacted with an organization, there's a good chance we will need to interact with them in the future. For example, if we've just initiated a conversation with a new business partner, there's a good chance we will continue interacting with them regularly in the near-term. It's important, however, to consider the time since our last interaction. If we interacted with someone yesterday or the day before, it's very important for them to be ranked higher than an organization we interacted with 10 days ago.\n\n## Getting the Data\n\nBoth of these solutions require populating our database with information regarding how recently we've interacted with particular entities. Fortunately, all of this can be done during the data ingestion phase when records are loaded in Elasticsearch. The only requirement is making sure to keep the information in Elasticsearch up to date with our PostgreSQL database\n\nThe one challenge with this solution is that it requires augmenting our documents with special data regarding total interactions and recency of interactions.  The huge advantage, however, is that it works seamlessly. We don't require there to be any special configuration when we begin working with a new partner.\n\n\n## Function Score Queries\n\nElasticsearch provides [function score queries](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html) that allow you to modify Elasticsearch's calculated score. Their documentation provides a really simple example explaining boost the relevancy of a document that has many `likes`:\n\n```JSON\nGET /_search\n{\n    \"query\": {\n        \"function_score\": {\n            \"field_value_factor\": {\n                \"field\": \"likes\",\n                \"factor\": 1.2,\n                \"modifier\": \"sqrt\",\n            }\n        }\n    }\n}\n```\n\nThis code says that every document's score will be `sqrt(1.2 * doc['likes'].value)`\n\nWe used a particular kind of Function Score Query known as a [Script Score](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-script-score).\n\nWe wanted our ultimate score to be a function of three values: the score returned from elasticsearch, the days since our last interaction, and our total interactions. Initially, our function looked something like this\n\n`newscore` = _score * doc['interactions'] *\n\nWe found that Elasticsearch's relevancy score was being overwhelmed by parters with a high number of interactions, so much so that given a search term of `nick drane`, we might return `nick smith`, simply because we've worked with nick smith more.\n\nWe needed to place an upperbound of the potential contribution of the total interactions. After initially looking at [logistic functions](https://en.wikipedia.org/wiki/Logistic_function) and other overly complicated strategies, we settled on a simple step function. If the number of interactions is greater than 25, then we multiply Elasticsearch's relevancy score by 2, otherwise we multiply it 1.\n\nThe frequency contribution's function was similarly simple.\n\n\n(could be some density/decay function that looks at density of interactions over time)\n\n## challenges\n\nKeeping the extra attributes up to date\n\nWe did try to combine two guassian curves but couldn't get it working\n\n\n\nSometimes was built using a `query_string` query against the `_all` field. Sometimes the returned results were good, but most of the time they fell short.  Their problem was simple: how do we ensure that the\n\n\n\n\nhealth care\nselect based on recent claim\nselect based on scheduled appointment\nselect based on upcoming event\nspecify table to search",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/optimizing-elasticsearch-score.md"
    },
    "017704d5-98aa-5eef-8086-8c15e94510d0": {
      "id": "017704d5-98aa-5eef-8086-8c15e94510d0",
      "children": [],
      "parent": "1859d7d4-53db-5da3-aba1-f0cf7f4ffa9b",
      "internal": {
        "content": "\nI'm currently contracted to create a web service using some data from a third party Angular application. I worked off a proof of concept codebase that used Chrome's new [Puppeteer](https://github.com/GoogleChrome/puppeteer) API to scrape this site. I strongly regret not starting from scratch.\n\n<!-- more -->\n\n## What is Puppeteer?\n\nPuppeteer is a Node API that allows you to control Google's headless Chrome browser. Imagine a version of your browser that can send and receive requests but has no GUI. It works in the background, performing actions as instructed by an API. This makes Puppeteer great for end to end testing a web application. You can truly simulate the user experience, typing where they type and clicking where they click. Another use case for Puppeteer is web scraping a single page web application. Let's explore how this might work.\n\n## A Simple Example\n\nFor any Puppeteer project, the first task is to create an instance of the headless browser.\n\n```js\nconst browser = await puppeteer.launch()\n\n// We will use this page instance and it's API frequently\nconst page = await this.browser.newPage();\n```\n\nAfter that's done, it's trivial to navigate to and begin interacting with a webpage. Let's suppose I want to fill out a login form and submit an authentication request to a website. In an ideal situation, it looks like this.\n\n```js\n// Navigate to the website\nawait page.goto(\"https://website/login\");\n\n// Provide the selector of an input box and the content to type\nawait page.type(\"input#username\", CREDENTIALS.username);\nawait page.type(\"input#password\", CREDENTIALS.password);\nawait page.click(\"button#login\"); // Click the login button\n\n// Wait until the screen changes and a node matching\n// the selector #logged-in-successfully appears,\n// at which point we know the login was successful\nawait page.waitForSelector(\"#logged-in-successfully\");\n```\n\nWe just successfully filled out a form, submitted an HTTP request containing our form data, and waited for the page to change upon successful login. This is where Puppeteer shines. Let's look at a more complicated example.\n\n## A More Complicated Example\n\nToday's web uses a mix of simple html driven forms as well as more complicated, javascript-driven forms, rich with functionality like autocomplete and dynamic dropdown menus. I'm sure you've used a calendar date picker. These components each need to be treated differently.\n\nHere is a modified version of some code I wrote for my client:\n\n```js\n// The form inputs I want to fill out\n// and their corresponding selectors\nconst fields = {\n  type: 'input[ng-model=\"type\"]',\n  origin: 'input[ng-model=\"origin\"]',\n  destination: 'input[ng-model=\"destination\"]',\n  date: 'input[ng-model=\"date\"]'\n};\n\nfunction search(searchParams) {\n  const page = await this.browser.newPage();\n  await page.goto(\"https://website/search\");\n\n  // We need to click a button to make the search form\n  // appear, but first make sure that button has rendered\n  await page.waitForSelector(\".new-search\");\n  await page.click(\".new-search\");\n\n  // Fill out the form\n  for (const field of Object.keys(fields)) {\n    if (searchParams[field]) {\n      const selector = fields[field];\n\n      // We want to make sure each DOM node is rendered\n      // before we try to do anything to it.\n      await page.waitForSelector(selector);\n\n      // Some inputs need to be focused first for page.type to work\n      // Might as well focus on all of them\n      await page.focus(selector);\n\n      // Some inputs have defaults that need to be\n      // erased before typing your own input\n      if (field === \"date\" || field === \"type\") {\n        // This is a helper function I wrote\n        await deleteInput(page, selector);\n      }\n\n      await page.type(selector, searchParams[field]);\n\n      // This field won't register the typed data\n      // until Enter is pressed.\n      // This is because the 'type' field is a dropdown\n      // where one of a specific set of inputs must be clicked.\n      if (field === \"type\") {\n        await page.keyboard.press(\"Enter\");\n      }\n    }\n  }\n}\n```\n\nThe first thing to notice is that the code is messy and full of weird exceptions. To make matters worse, it doesn't always work.\n\n## Brace Yourself for Flaky Behavior\n\nWorking with Puppeteer was an exercise in guesswork. Given the same inputs, Puppeteer did not always produce the same outputs<sup>[1](#footnote1)</sup>. This flaky behavior made the project unnecessarily challenging and required me to do additional engineering to increase reliability, which was frustrating considering the alternative.\n\n## Puppeteer Was Completely Unnecessary\n\nPuppeteer has a API that allows you to execute arbitrary code against the DOM. After scraping form results with this API and getting the same flaky behavior described above, I ditched the approach and started grabbing data from the HTTP response objects themselves. Below is a primitive version of some code I wrote to do this.\n\n```js\nwaitForUrl(page, urlPrefix) {\n  return new Promise(resolve => {\n    page.on(\"response\", res => {\n      if (!res.url.startsWith(urlPrefix)) {\n        return;\n      }\n      resolve(res.json());\n    };\n  });\n}\n```\n\nThe page passed into this function is the same page object created above by the Puppeteer API. Among other things, it is an event emitter that allows me to listen for any HTTP responses. I've essentially created a promise that resolves to the response body of a particular ajax request. This allowed me interact with the server API directly and removed the DOM from the data retrieval process, greatly reducing the chance for flaky behavior. But that begs the question, why use Puppeteer at all? Why not simply send http requests to the server API manually and ditch the complicated form submission code above That's how I should have started all along.\n\n## When is Puppeteer the Right Solution\n\nI can only think of a single scenario where using Puppeteer for scraping is superior to the alternative: if the information you want is generated using a combination of API data and javascript code. After all, you would have no other way to simulate the javascript code without rewriting it.\n\nIf, however, all you need is data from the server, go the simple route and hit the API with an HTTP library like [axios](https://github.com/axios/axios) or [request](https://github.com/request/request). If you are scraping a server side rendered application, you can combine one of the aforementioned tools with [Cheerio](https://github.com/cheeriojs/cheerio), giving you a far more user friendly DOM manipulation API than that offered my Puppeteer.\n\n_If you need help with webscraping or puppeteer, I do [consulting](/hire-me) work and am currently looking for new clients. Please contact me for more details._\n\n#### Footnotes\n\n<a name=\"footnote1\">1</a>: You might be curious why Puppeteer exhibited flaky behavior. One issue that I ran into was animations. I might attempt to click a DOM node, but if the animation had not finished, the click would not register. In essence, it would appear as if the click had worked, but once the animation finished, the DOM would reset itself, undoing my click. I think this is simply how Angular's digest loop reacted to a click at an unexpected time. Unfortunately, Puppeteer provides no functionality to determine when an animation has finished (after all, how would it!). I tried a couple solutions. One entailed a sleep function to wait a couple hundred milliseconds for the animation to finish, but it simply exacerbated the flaky behavior. A second involved executing the click only once the DOM node had a particular class that indicated the animation had finished. At one point, I even tried to disable all animations across the website. All these solutions were half-baked.\n",
        "type": "MarkdownRemark",
        "contentDigest": "7562d9436ea473576fd787da6e78ecfb",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Scraping the Web with Puppeteer: Lessons Learned",
        "categories": [
          [
            "Javascript"
          ],
          [
            "Web Scraping"
          ],
          [
            "Failures"
          ]
        ],
        "date": "2017-12-09T15:35:13.000Z",
        "_PARENT": "1859d7d4-53db-5da3-aba1-f0cf7f4ffa9b"
      },
      "excerpt": "\nI'm currently contracted to create a web service using some data from a third party Angular application. I worked off a proof of concept codebase that used Chrome's new [Puppeteer](https://github.com/GoogleChrome/puppeteer) API to scrape this site. I strongly regret not starting from scratch.\n\n",
      "rawMarkdownBody": "\nI'm currently contracted to create a web service using some data from a third party Angular application. I worked off a proof of concept codebase that used Chrome's new [Puppeteer](https://github.com/GoogleChrome/puppeteer) API to scrape this site. I strongly regret not starting from scratch.\n\n<!-- more -->\n\n## What is Puppeteer?\n\nPuppeteer is a Node API that allows you to control Google's headless Chrome browser. Imagine a version of your browser that can send and receive requests but has no GUI. It works in the background, performing actions as instructed by an API. This makes Puppeteer great for end to end testing a web application. You can truly simulate the user experience, typing where they type and clicking where they click. Another use case for Puppeteer is web scraping a single page web application. Let's explore how this might work.\n\n## A Simple Example\n\nFor any Puppeteer project, the first task is to create an instance of the headless browser.\n\n```js\nconst browser = await puppeteer.launch()\n\n// We will use this page instance and it's API frequently\nconst page = await this.browser.newPage();\n```\n\nAfter that's done, it's trivial to navigate to and begin interacting with a webpage. Let's suppose I want to fill out a login form and submit an authentication request to a website. In an ideal situation, it looks like this.\n\n```js\n// Navigate to the website\nawait page.goto(\"https://website/login\");\n\n// Provide the selector of an input box and the content to type\nawait page.type(\"input#username\", CREDENTIALS.username);\nawait page.type(\"input#password\", CREDENTIALS.password);\nawait page.click(\"button#login\"); // Click the login button\n\n// Wait until the screen changes and a node matching\n// the selector #logged-in-successfully appears,\n// at which point we know the login was successful\nawait page.waitForSelector(\"#logged-in-successfully\");\n```\n\nWe just successfully filled out a form, submitted an HTTP request containing our form data, and waited for the page to change upon successful login. This is where Puppeteer shines. Let's look at a more complicated example.\n\n## A More Complicated Example\n\nToday's web uses a mix of simple html driven forms as well as more complicated, javascript-driven forms, rich with functionality like autocomplete and dynamic dropdown menus. I'm sure you've used a calendar date picker. These components each need to be treated differently.\n\nHere is a modified version of some code I wrote for my client:\n\n```js\n// The form inputs I want to fill out\n// and their corresponding selectors\nconst fields = {\n  type: 'input[ng-model=\"type\"]',\n  origin: 'input[ng-model=\"origin\"]',\n  destination: 'input[ng-model=\"destination\"]',\n  date: 'input[ng-model=\"date\"]'\n};\n\nfunction search(searchParams) {\n  const page = await this.browser.newPage();\n  await page.goto(\"https://website/search\");\n\n  // We need to click a button to make the search form\n  // appear, but first make sure that button has rendered\n  await page.waitForSelector(\".new-search\");\n  await page.click(\".new-search\");\n\n  // Fill out the form\n  for (const field of Object.keys(fields)) {\n    if (searchParams[field]) {\n      const selector = fields[field];\n\n      // We want to make sure each DOM node is rendered\n      // before we try to do anything to it.\n      await page.waitForSelector(selector);\n\n      // Some inputs need to be focused first for page.type to work\n      // Might as well focus on all of them\n      await page.focus(selector);\n\n      // Some inputs have defaults that need to be\n      // erased before typing your own input\n      if (field === \"date\" || field === \"type\") {\n        // This is a helper function I wrote\n        await deleteInput(page, selector);\n      }\n\n      await page.type(selector, searchParams[field]);\n\n      // This field won't register the typed data\n      // until Enter is pressed.\n      // This is because the 'type' field is a dropdown\n      // where one of a specific set of inputs must be clicked.\n      if (field === \"type\") {\n        await page.keyboard.press(\"Enter\");\n      }\n    }\n  }\n}\n```\n\nThe first thing to notice is that the code is messy and full of weird exceptions. To make matters worse, it doesn't always work.\n\n## Brace Yourself for Flaky Behavior\n\nWorking with Puppeteer was an exercise in guesswork. Given the same inputs, Puppeteer did not always produce the same outputs<sup>[1](#footnote1)</sup>. This flaky behavior made the project unnecessarily challenging and required me to do additional engineering to increase reliability, which was frustrating considering the alternative.\n\n## Puppeteer Was Completely Unnecessary\n\nPuppeteer has a API that allows you to execute arbitrary code against the DOM. After scraping form results with this API and getting the same flaky behavior described above, I ditched the approach and started grabbing data from the HTTP response objects themselves. Below is a primitive version of some code I wrote to do this.\n\n```js\nwaitForUrl(page, urlPrefix) {\n  return new Promise(resolve => {\n    page.on(\"response\", res => {\n      if (!res.url.startsWith(urlPrefix)) {\n        return;\n      }\n      resolve(res.json());\n    };\n  });\n}\n```\n\nThe page passed into this function is the same page object created above by the Puppeteer API. Among other things, it is an event emitter that allows me to listen for any HTTP responses. I've essentially created a promise that resolves to the response body of a particular ajax request. This allowed me interact with the server API directly and removed the DOM from the data retrieval process, greatly reducing the chance for flaky behavior. But that begs the question, why use Puppeteer at all? Why not simply send http requests to the server API manually and ditch the complicated form submission code above That's how I should have started all along.\n\n## When is Puppeteer the Right Solution\n\nI can only think of a single scenario where using Puppeteer for scraping is superior to the alternative: if the information you want is generated using a combination of API data and javascript code. After all, you would have no other way to simulate the javascript code without rewriting it.\n\nIf, however, all you need is data from the server, go the simple route and hit the API with an HTTP library like [axios](https://github.com/axios/axios) or [request](https://github.com/request/request). If you are scraping a server side rendered application, you can combine one of the aforementioned tools with [Cheerio](https://github.com/cheeriojs/cheerio), giving you a far more user friendly DOM manipulation API than that offered my Puppeteer.\n\n_If you need help with webscraping or puppeteer, I do [consulting](/hire-me) work and am currently looking for new clients. Please contact me for more details._\n\n#### Footnotes\n\n<a name=\"footnote1\">1</a>: You might be curious why Puppeteer exhibited flaky behavior. One issue that I ran into was animations. I might attempt to click a DOM node, but if the animation had not finished, the click would not register. In essence, it would appear as if the click had worked, but once the animation finished, the DOM would reset itself, undoing my click. I think this is simply how Angular's digest loop reacted to a click at an unexpected time. Unfortunately, Puppeteer provides no functionality to determine when an animation has finished (after all, how would it!). I tried a couple solutions. One entailed a sleep function to wait a couple hundred milliseconds for the animation to finish, but it simply exacerbated the flaky behavior. A second involved executing the click only once the DOM node had a particular class that indicated the animation had finished. At one point, I even tried to disable all animations across the website. All these solutions were half-baked.\n",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/scraping-the-web-with-puppeteer-lessons-learned.md"
    },
    "86e7f107-140a-5cdd-bbf5-d54112d285b9": {
      "id": "86e7f107-140a-5cdd-bbf5-d54112d285b9",
      "children": [],
      "parent": "6f826141-57a4-5754-9ef3-314cc8204e82",
      "internal": {
        "content": "\nI recently wanted to ingest a [line-delimited](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON) JSON file into [Postgres](https://www.postgresql.org/) for some quick data exploration. I was surprised when I couldn't find a simple CLI solution that parsed the JSON and loaded each field into its own column. Every approach I found instead inserted the entire JSON object in a JSONB field. Here is my solution.\n\n<!-- more -->\n\n## Downloading 250000 Hacker News Comments\n\nLet's say we want to download all of the [Hacker News](https://news.ycombinator.com/) comments from the month of May. A line-delimited JSON file is available from [pushshift](https://files.pushshift.io/hackernews/HNI_2018-05.bz2). Fetching and decompressing the file is simple:\n\n```bash\ncurl https://files.pushshift.io/hackernews/HNI_2018-05.bz2 | bzip2 -d\n```\n\nHere is what the dataset looks like:\n\n```JSON\n{\n  \"by\": \"criddell\",\n  \"id\": 16966059,\n  \"kids\": [\n    16966312,\n    16966776,\n    16969455,\n    16966323\n  ],\n  \"parent\": 16965363,\n  \"retrieved_on\": 1528401399,\n  \"text\": \"Yeah - there's always a HATEOAS comment somewhere and...\",\n  \"time\": 1525173078,\n  \"type\": \"comment\"\n}\n```\n\n## Formatting the Data\n\nYou might think that Postgres has a simple utility for loading line-delimited JSON. Like me, you'd be wrong. It's all the more surprising given that it has a [COPY](https://www.postgresql.org/docs/current/static/sql-copy.html) utility that's designed to load data from files. Unfortunately, that utility only supports `text`, `csv`, and `binary` formats.\n\nTransforming our data into a CSV is a breeze with [jq](https://stedolan.github.io/jq/). We can pipe the JSON stream into the following command to extract the `id`, `by`, `parent`, and `text` fields. You can customize the command to extract whatever fields you like.\n\n```bash\njq -r '[.id, .by, .parent, .text] | @csv'\n```\n\nThe `-r` option indicates that we would like a raw string output, as opposed to JSON formatted with quotes. The `[.id, .by, .parent, .text]` part produces an array containing the desired fields and the pipe into `@csv` specifies the format. All that's left is to load the data into Postgres.\n\n## Ingesting the Data\n\nAfter creating the database\n\n`createdb comment_db`\n\nand applying the schema\n\n```SQL\nCREATE TABLE comment (\n    id INTEGER PRIMARY KEY,\n    by VARCHAR,\n    parent INTEGER,\n    text TEXT\n);\n```\n\nwe can hydrate our comments into `comment_db` using [psql](https://www.postgresql.org/docs/current/static/app-psql.html)\n\n```bash\npsql comment_db -c \"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"\n```\n\nNote that the fields specified above need to be in the same order as the fields in the CSV stream generated by `jq`.\n\nHere is the final command\n\n```bash\ncurl https://files.pushshift.io/hackernews/HNI_2018-05.bz2 \\\n  | bzip2 -d \\\n  | jq -r '[.id, .by, .parent, .text] | @csv' \\\n  | psql comment_db -c \"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"\n```\n\n## Supporting Referential Integrity\n\nYou will notice that despite the fact that the `comment.parent` refers to a comment id, we have omitted a foreign key constraint from our schema. This omission is because our command does not control for the order in which comments are loaded. We would have received constraint errors if we specified the foreign key relationship.\n\nWe can overcome this obstacle by sorting our incoming comments by id.\n\n```bash\ncurl https://files.pushshift.io/hackernews/HNI_2018-05.bz2 \\\n  | bzip2 -d \\\n  | jq -s -r 'sort_by(.id) | .[] | [.id, .by, .parent, .text] | @csv' \\\n  | psql comment_db -c \"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"\n```\n\nIf you have a primary key that doesn't serially increase - perhaps you're using a [natural key](https://en.wikipedia.org/wiki/Natural_key) or a UUID as your primary key - then you could also sort on a `created_at` timestamp\n\n## Tradeoffs\n\nEverything in software engineering has a tradeoff, and I would be remiss to to not mention them here. That `-s` option we specified above instructs `jq` to download the entire dataset into memory, a requirement for sorting. If you dataset is too large, then the command will fail (`jq` failed for me at 769MB).\n\nThe first option does not suffer this limitation and will work for arbitrarily large datasets. This is because it leverages [streams](https://en.wikipedia.org/wiki/Stream_(computing) to only work on small chunks of data at once. If your dataset is large and you want foreign key constraints, you could use this streaming approach and then apply the constraints after data ingestion completes.\n\n_If you have a data ingestion or PostgreSQL related problem, I do [consulting](/hire-me) work out of Chicago area and am currently looking for new clients. Please contact me for more details_",
        "type": "MarkdownRemark",
        "contentDigest": "b5ef0b07c6bce39b5cea90b03c728ecc",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Using Shell Commands to Effortlessly Ingest Line-delimited JSON into PostgreSQL",
        "date": "2018-10-18T00:00:00.000Z",
        "categories": [
          [
            "Shell"
          ],
          [
            "Postgres"
          ]
        ],
        "_PARENT": "6f826141-57a4-5754-9ef3-314cc8204e82"
      },
      "excerpt": "\nI recently wanted to ingest a [line-delimited](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON) JSON file into [Postgres](https://www.postgresql.org/) for some quick data exploration. I was surprised when I couldn't find a simple CLI solution that parsed the JSON and loaded each field into its own column. Every approach I found instead inserted the entire JSON object in a JSONB field. Here is my solution.\n\n",
      "rawMarkdownBody": "\nI recently wanted to ingest a [line-delimited](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON) JSON file into [Postgres](https://www.postgresql.org/) for some quick data exploration. I was surprised when I couldn't find a simple CLI solution that parsed the JSON and loaded each field into its own column. Every approach I found instead inserted the entire JSON object in a JSONB field. Here is my solution.\n\n<!-- more -->\n\n## Downloading 250000 Hacker News Comments\n\nLet's say we want to download all of the [Hacker News](https://news.ycombinator.com/) comments from the month of May. A line-delimited JSON file is available from [pushshift](https://files.pushshift.io/hackernews/HNI_2018-05.bz2). Fetching and decompressing the file is simple:\n\n```bash\ncurl https://files.pushshift.io/hackernews/HNI_2018-05.bz2 | bzip2 -d\n```\n\nHere is what the dataset looks like:\n\n```JSON\n{\n  \"by\": \"criddell\",\n  \"id\": 16966059,\n  \"kids\": [\n    16966312,\n    16966776,\n    16969455,\n    16966323\n  ],\n  \"parent\": 16965363,\n  \"retrieved_on\": 1528401399,\n  \"text\": \"Yeah - there's always a HATEOAS comment somewhere and...\",\n  \"time\": 1525173078,\n  \"type\": \"comment\"\n}\n```\n\n## Formatting the Data\n\nYou might think that Postgres has a simple utility for loading line-delimited JSON. Like me, you'd be wrong. It's all the more surprising given that it has a [COPY](https://www.postgresql.org/docs/current/static/sql-copy.html) utility that's designed to load data from files. Unfortunately, that utility only supports `text`, `csv`, and `binary` formats.\n\nTransforming our data into a CSV is a breeze with [jq](https://stedolan.github.io/jq/). We can pipe the JSON stream into the following command to extract the `id`, `by`, `parent`, and `text` fields. You can customize the command to extract whatever fields you like.\n\n```bash\njq -r '[.id, .by, .parent, .text] | @csv'\n```\n\nThe `-r` option indicates that we would like a raw string output, as opposed to JSON formatted with quotes. The `[.id, .by, .parent, .text]` part produces an array containing the desired fields and the pipe into `@csv` specifies the format. All that's left is to load the data into Postgres.\n\n## Ingesting the Data\n\nAfter creating the database\n\n`createdb comment_db`\n\nand applying the schema\n\n```SQL\nCREATE TABLE comment (\n    id INTEGER PRIMARY KEY,\n    by VARCHAR,\n    parent INTEGER,\n    text TEXT\n);\n```\n\nwe can hydrate our comments into `comment_db` using [psql](https://www.postgresql.org/docs/current/static/app-psql.html)\n\n```bash\npsql comment_db -c \"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"\n```\n\nNote that the fields specified above need to be in the same order as the fields in the CSV stream generated by `jq`.\n\nHere is the final command\n\n```bash\ncurl https://files.pushshift.io/hackernews/HNI_2018-05.bz2 \\\n  | bzip2 -d \\\n  | jq -r '[.id, .by, .parent, .text] | @csv' \\\n  | psql comment_db -c \"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"\n```\n\n## Supporting Referential Integrity\n\nYou will notice that despite the fact that the `comment.parent` refers to a comment id, we have omitted a foreign key constraint from our schema. This omission is because our command does not control for the order in which comments are loaded. We would have received constraint errors if we specified the foreign key relationship.\n\nWe can overcome this obstacle by sorting our incoming comments by id.\n\n```bash\ncurl https://files.pushshift.io/hackernews/HNI_2018-05.bz2 \\\n  | bzip2 -d \\\n  | jq -s -r 'sort_by(.id) | .[] | [.id, .by, .parent, .text] | @csv' \\\n  | psql comment_db -c \"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"\n```\n\nIf you have a primary key that doesn't serially increase - perhaps you're using a [natural key](https://en.wikipedia.org/wiki/Natural_key) or a UUID as your primary key - then you could also sort on a `created_at` timestamp\n\n## Tradeoffs\n\nEverything in software engineering has a tradeoff, and I would be remiss to to not mention them here. That `-s` option we specified above instructs `jq` to download the entire dataset into memory, a requirement for sorting. If you dataset is too large, then the command will fail (`jq` failed for me at 769MB).\n\nThe first option does not suffer this limitation and will work for arbitrarily large datasets. This is because it leverages [streams](https://en.wikipedia.org/wiki/Stream_(computing) to only work on small chunks of data at once. If your dataset is large and you want foreign key constraints, you could use this streaming approach and then apply the constraints after data ingestion completes.\n\n_If you have a data ingestion or PostgreSQL related problem, I do [consulting](/hire-me) work out of Chicago area and am currently looking for new clients. Please contact me for more details_",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres.md"
    },
    "6e5a1b24-f308-5104-8f33-b43745551d5f": {
      "id": "6e5a1b24-f308-5104-8f33-b43745551d5f",
      "children": [],
      "parent": "2546894c-d49f-53f5-b060-a64985442bba",
      "internal": {
        "content": "\nThe other day at work, one of my colleagues was frustrated that he was unable to encode nested objects in a query string and still maintain a readable URL. I went home that night and coded up a simple solution to this problem, and I thought I'd share it here today. This [Github repo](https://github.com/nadrane/querystring-encoder) contains specs and the solution code.\n\n<!-- more -->\n\n## Motivation\n\nToday, in the Node.js ecosystem, numerous modules exist to encode query strings, but they generally have one of two flaws:\n\n1.  They do not permit the encoding of nested objects.\n\n2.  They can encode nested objects, but they delimit nesting using unsafe URL characters, yielding an operation and a result that look like this <sup>[1](#footnote1)</sup>:\n\n```js\nencode({\n  a: { b: 'c' }\n});\n>>>`a%5Bb%5D=c`\n```\n\n## The Problem in Detail\n\nNode.js provides a [`querystring`](https://nodejs.org/api/querystring.html) module to encode objects to query strings. The only problem is that conforms to an official specification that doesn't allow nested objects. Unfortunately, this specification does not allow for enough flexibility when creating a RESTful API.\n\nFor example, suppose the client wants to filter a collection of cars by make and model. The route might look like this:\n\n`/api/cars?make=honda&model=civic`\n\nThis URI makes it reasonably clear that we want to filter cars by their make and model.\n\nWhat if we wanted to do something more complicated. What if we wanted to filter cars and order them by price?\n\n`/api/cars?order=price&make=honda&model=civic`\n\nIt's no longer clear which query parameters describe the ordering and which describe the filter. Ideally, we want the url to look like this:\n\n`/api/cars?order=price&filter.make=honda&filter.model=civic`\n\nIf we were to represent the query string of the above URI as a Javascript object, it would probably look like this:\n\n```js\n{\n    order: \"price\",\n    filter: {\n        make: \"honda\",\n        model: \"civic\"\n    }\n}\n```\n\nAnd then we quickly run into our problem. We need to encode the object above into\n\n`order=price&filter.make=honda&filter.model=civic`\n\nbut Node.js's [`querystring`](https://nodejs.org/api/querystring.html) can't encode nested objects.\n\n## Existing Modules Supporting Nested Querystrings\n\nBy default, the [qs](https://www.npmjs.com/package/qs) module creates ugly urls when it encodes nested query strings. If we encode our object above, we get\n\n`order=price&filter[make]=honda&filter[model]=civic`\n\nThe `[` and `]` characters are both considered unsafe in a URL and are required to be escaped. The URL becomes unreadable after this percent encoding operation.\n\n`order=price&filter%5Bmake%5D=honda&filter%5Bmodel%5D=civic`\n\nFortunately, the `.` is not considered unsafe and does not need to be escaped, making it the perfect character to express object nesting.\n\n## The Solution\n\nThe solution is broken down into two parts. The first is _encoding_ a nested object into a query string. The second part is _decoding_ a query string back into a nested object.\n\n### Encoding<sup>[2](#footnote2)</sup>\n\n#### Just Nested Objects\n\nLet's write some code to encode\n\n```js\n{\n  filter: {\n    make: \"honda\";\n    model: \"civic\";\n  }\n}\n```\n\ninto the query string `filter.make=honda&filter.model=civic`\n\n```js\nconst { escape } = require(\"querystring\");\n\nfunction encode(queryObj, nesting = \"\") {\n  let queryString = \"\";\n\n  const pairs = Object.entries(queryObj).map(([key, val]) => {\n    // Handle the nested, recursive case, where the value to encode is an object itself\n    if (typeof val === \"object\") {\n      return encode(val, nesting + `${key}.`);\n    } else {\n      // Handle base case, where the value to encode is simply a string.\n      return [nesting + key, val].map(escape).join(\"=\");\n    }\n  });\n  return pairs.join(\"&\");\n}\n```\n\nNotice that we use the [escape](https://nodejs.org/api/querystring.html#querystring_querystring_escape_str) function provided in Node.js core to percent encode specific characters.\n\n#### Encoding Arrays as Values\n\nIf we want to add support to encode an object with array values, like the following:\n\n```js\n{\n    name: \"nick\",\n    hobbies: [\"cooking\", \"coding\"]\n}\n```\n\nthen we only need to add another base case to our function\n\n```js\nfunction encode(queryObj, nesting = \"\") {\n  let queryString = \"\";\n\n  const pairs = Object.entries(queryObj).map(([key, val]) => {\n    // Handle a second base case where the value to encode is an array\n    if (Array.isArray(val)) {\n      return val.map(subVal => [nesting + key, subVal].map(escape).join(\"=\")).join(\"&\");\n    } else if (typeof val === \"object\") {\n      return encode(val, nesting + `${key}.`);\n    } else {\n      return [nesting + key, val].map(escape).join(\"=\");\n    }\n  });\n  return pairs.join(\"&\");\n}\n```\n\n### Decoding\n\nAn encoding function is not very useful unless you can decode the encoded string back to it's original form.\n\n#### Just Nested Objects\n\nWe want to write a function that will decode `filter.make=honda&filter.model=civic` back into a nested object\n\n```js\n{\n  filter: {\n    make: \"honda\";\n    model: \"civic\";\n  }\n}\n```\n\nThe code to do this is fairly straightforward if we use a [Lodash](https://lodash.com/docs) utility called [set](https://lodash.com/docs/4.17.5#set) that allows us to set an arbitrarily nested key in an object.\n\n```js\nconst set = require(\"lodash.set\");\n\nfunction decode(queryString) {\n  const queryStringPieces = queryString.split(\"&\");\n  const decodedQueryString = {};\n\n  for (const piece of queryStringPieces) {\n    let [key, value] = piece.split(\"=\");\n    value = value || \"\"; // If a value is not defined, it should be decoded as an empty string\n    set(decodedQueryString, key, value);\n  }\n  return decodedQueryString;\n}\n```\n\n#### Decoding Arrays as Values\n\nIf we want to add support to decode arrays like we did above, then we need to do a little additional work. Fortunately, two additional [Lodash](https://lodash.com/docs) utilities, [has](https://lodash.com/docs/4.17.5#has) and [get](https://lodash.com/docs/4.17.5#get), allow us to check for the existence of a nested key and to get the value associated with a nested key, respectively, greatly simplifying our problem.\n\n```js\nconst set = require(\"lodash.set\");\nconst has = require(\"lodash.has\");\nconst get = require(\"lodash.get\");\n\nfunction decode(queryString) {\n  const queryStringPieces = queryString.split(\"&\");\n  const decodedQueryString = {};\n\n  for (const piece of queryStringPieces) {\n    let [key, value] = piece.split(\"=\");\n    value = value || \"\";\n    if (has(decodedQueryString, key)) {\n      const currentValueForKey = get(decodedQueryString, key);\n      if (!Array.isArray(currentValueForKey)) {\n        set(decodedQueryString, key, [currentValueForKey, value]);\n      } else {\n        currentValueForKey.push(value);\n      }\n    } else {\n      set(decodedQueryString, key, value);\n    }\n  }\n  return decodedQueryString;\n}\n```\n\n## Conclusion\n\nAnd that's it! The whole thing, encoding and decoding, only takes ~40 lines of code. Perhaps next time you encounter something that feels a little too fundamental to code yourself, you won't hesitate to write some code if you can't find a sufficient open source package.\n\n#### Footnotes\n\n<a name=\"footnote1\">1</a>: This example is straight from the [qs](https://www.npmjs.com/package/qs) documentation. Incidentally, qs provides an option to encode using a url safe character, which would result in readable urls, but this is not the default.\n\n<a name=\"footnote2\">2</a>: It's worth noting that you might not want to use this code in production. I've written the code in a functional style for clarity and conciseness. If you have a high read volume, given that this code might potentially run on a significant portion of GET requests, it should probably be written in an imperative style that doesn't disregard performance. Even more importantly, this code does not protect against potential attackers who might try to create an arbitrarily deeply nested object or might include an unwieldy number of query parameters.\n",
        "type": "MarkdownRemark",
        "contentDigest": "5a0a73d8cfea406b62997778a2c65eab",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Build Your Own Nested Query String Encoder/Decoder",
        "categories": [
          [
            "Javascript"
          ],
          [
            "Recursion"
          ],
          [
            "Web"
          ],
          [
            "Build Your Own"
          ]
        ],
        "date": "2018-04-13T15:17:00.000Z",
        "_PARENT": "2546894c-d49f-53f5-b060-a64985442bba"
      },
      "excerpt": "\nThe other day at work, one of my colleagues was frustrated that he was unable to encode nested objects in a query string and still maintain a readable URL. I went home that night and coded up a simple solution to this problem, and I thought I'd share it here today. This [Github repo](https://github.com/nadrane/querystring-encoder) contains specs and the solution code.\n\n",
      "rawMarkdownBody": "\nThe other day at work, one of my colleagues was frustrated that he was unable to encode nested objects in a query string and still maintain a readable URL. I went home that night and coded up a simple solution to this problem, and I thought I'd share it here today. This [Github repo](https://github.com/nadrane/querystring-encoder) contains specs and the solution code.\n\n<!-- more -->\n\n## Motivation\n\nToday, in the Node.js ecosystem, numerous modules exist to encode query strings, but they generally have one of two flaws:\n\n1.  They do not permit the encoding of nested objects.\n\n2.  They can encode nested objects, but they delimit nesting using unsafe URL characters, yielding an operation and a result that look like this <sup>[1](#footnote1)</sup>:\n\n```js\nencode({\n  a: { b: 'c' }\n});\n>>>`a%5Bb%5D=c`\n```\n\n## The Problem in Detail\n\nNode.js provides a [`querystring`](https://nodejs.org/api/querystring.html) module to encode objects to query strings. The only problem is that conforms to an official specification that doesn't allow nested objects. Unfortunately, this specification does not allow for enough flexibility when creating a RESTful API.\n\nFor example, suppose the client wants to filter a collection of cars by make and model. The route might look like this:\n\n`/api/cars?make=honda&model=civic`\n\nThis URI makes it reasonably clear that we want to filter cars by their make and model.\n\nWhat if we wanted to do something more complicated. What if we wanted to filter cars and order them by price?\n\n`/api/cars?order=price&make=honda&model=civic`\n\nIt's no longer clear which query parameters describe the ordering and which describe the filter. Ideally, we want the url to look like this:\n\n`/api/cars?order=price&filter.make=honda&filter.model=civic`\n\nIf we were to represent the query string of the above URI as a Javascript object, it would probably look like this:\n\n```js\n{\n    order: \"price\",\n    filter: {\n        make: \"honda\",\n        model: \"civic\"\n    }\n}\n```\n\nAnd then we quickly run into our problem. We need to encode the object above into\n\n`order=price&filter.make=honda&filter.model=civic`\n\nbut Node.js's [`querystring`](https://nodejs.org/api/querystring.html) can't encode nested objects.\n\n## Existing Modules Supporting Nested Querystrings\n\nBy default, the [qs](https://www.npmjs.com/package/qs) module creates ugly urls when it encodes nested query strings. If we encode our object above, we get\n\n`order=price&filter[make]=honda&filter[model]=civic`\n\nThe `[` and `]` characters are both considered unsafe in a URL and are required to be escaped. The URL becomes unreadable after this percent encoding operation.\n\n`order=price&filter%5Bmake%5D=honda&filter%5Bmodel%5D=civic`\n\nFortunately, the `.` is not considered unsafe and does not need to be escaped, making it the perfect character to express object nesting.\n\n## The Solution\n\nThe solution is broken down into two parts. The first is _encoding_ a nested object into a query string. The second part is _decoding_ a query string back into a nested object.\n\n### Encoding<sup>[2](#footnote2)</sup>\n\n#### Just Nested Objects\n\nLet's write some code to encode\n\n```js\n{\n  filter: {\n    make: \"honda\";\n    model: \"civic\";\n  }\n}\n```\n\ninto the query string `filter.make=honda&filter.model=civic`\n\n```js\nconst { escape } = require(\"querystring\");\n\nfunction encode(queryObj, nesting = \"\") {\n  let queryString = \"\";\n\n  const pairs = Object.entries(queryObj).map(([key, val]) => {\n    // Handle the nested, recursive case, where the value to encode is an object itself\n    if (typeof val === \"object\") {\n      return encode(val, nesting + `${key}.`);\n    } else {\n      // Handle base case, where the value to encode is simply a string.\n      return [nesting + key, val].map(escape).join(\"=\");\n    }\n  });\n  return pairs.join(\"&\");\n}\n```\n\nNotice that we use the [escape](https://nodejs.org/api/querystring.html#querystring_querystring_escape_str) function provided in Node.js core to percent encode specific characters.\n\n#### Encoding Arrays as Values\n\nIf we want to add support to encode an object with array values, like the following:\n\n```js\n{\n    name: \"nick\",\n    hobbies: [\"cooking\", \"coding\"]\n}\n```\n\nthen we only need to add another base case to our function\n\n```js\nfunction encode(queryObj, nesting = \"\") {\n  let queryString = \"\";\n\n  const pairs = Object.entries(queryObj).map(([key, val]) => {\n    // Handle a second base case where the value to encode is an array\n    if (Array.isArray(val)) {\n      return val.map(subVal => [nesting + key, subVal].map(escape).join(\"=\")).join(\"&\");\n    } else if (typeof val === \"object\") {\n      return encode(val, nesting + `${key}.`);\n    } else {\n      return [nesting + key, val].map(escape).join(\"=\");\n    }\n  });\n  return pairs.join(\"&\");\n}\n```\n\n### Decoding\n\nAn encoding function is not very useful unless you can decode the encoded string back to it's original form.\n\n#### Just Nested Objects\n\nWe want to write a function that will decode `filter.make=honda&filter.model=civic` back into a nested object\n\n```js\n{\n  filter: {\n    make: \"honda\";\n    model: \"civic\";\n  }\n}\n```\n\nThe code to do this is fairly straightforward if we use a [Lodash](https://lodash.com/docs) utility called [set](https://lodash.com/docs/4.17.5#set) that allows us to set an arbitrarily nested key in an object.\n\n```js\nconst set = require(\"lodash.set\");\n\nfunction decode(queryString) {\n  const queryStringPieces = queryString.split(\"&\");\n  const decodedQueryString = {};\n\n  for (const piece of queryStringPieces) {\n    let [key, value] = piece.split(\"=\");\n    value = value || \"\"; // If a value is not defined, it should be decoded as an empty string\n    set(decodedQueryString, key, value);\n  }\n  return decodedQueryString;\n}\n```\n\n#### Decoding Arrays as Values\n\nIf we want to add support to decode arrays like we did above, then we need to do a little additional work. Fortunately, two additional [Lodash](https://lodash.com/docs) utilities, [has](https://lodash.com/docs/4.17.5#has) and [get](https://lodash.com/docs/4.17.5#get), allow us to check for the existence of a nested key and to get the value associated with a nested key, respectively, greatly simplifying our problem.\n\n```js\nconst set = require(\"lodash.set\");\nconst has = require(\"lodash.has\");\nconst get = require(\"lodash.get\");\n\nfunction decode(queryString) {\n  const queryStringPieces = queryString.split(\"&\");\n  const decodedQueryString = {};\n\n  for (const piece of queryStringPieces) {\n    let [key, value] = piece.split(\"=\");\n    value = value || \"\";\n    if (has(decodedQueryString, key)) {\n      const currentValueForKey = get(decodedQueryString, key);\n      if (!Array.isArray(currentValueForKey)) {\n        set(decodedQueryString, key, [currentValueForKey, value]);\n      } else {\n        currentValueForKey.push(value);\n      }\n    } else {\n      set(decodedQueryString, key, value);\n    }\n  }\n  return decodedQueryString;\n}\n```\n\n## Conclusion\n\nAnd that's it! The whole thing, encoding and decoding, only takes ~40 lines of code. Perhaps next time you encounter something that feels a little too fundamental to code yourself, you won't hesitate to write some code if you can't find a sufficient open source package.\n\n#### Footnotes\n\n<a name=\"footnote1\">1</a>: This example is straight from the [qs](https://www.npmjs.com/package/qs) documentation. Incidentally, qs provides an option to encode using a url safe character, which would result in readable urls, but this is not the default.\n\n<a name=\"footnote2\">2</a>: It's worth noting that you might not want to use this code in production. I've written the code in a functional style for clarity and conciseness. If you have a high read volume, given that this code might potentially run on a significant portion of GET requests, it should probably be written in an imperative style that doesn't disregard performance. Even more importantly, this code does not protect against potential attackers who might try to create an arbitrarily deeply nested object or might include an unwieldy number of query parameters.\n",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/build-your-own-nested-query-string-encoder.md"
    },
    "17642430-0738-5531-88a3-12996ff69b04": {
      "id": "17642430-0738-5531-88a3-12996ff69b04",
      "children": [],
      "parent": "031a5100-2916-5e59-a79a-799fbfff2048",
      "internal": {
        "content": "\nI stumbled upon an [article](https://www.cs.princeton.edu/courses/archive/spr09/cos333/beautiful.html) the other day where Rob Pike implements a rudimentary regular expression engine in c. I converted his code to Javascript and added test specs so that someone can self-guide themselves through the creation of the regex engine. The specs and solution can be found in this [GitHub repository](https://github.com/nadrane/build-your-own-regex). This blog post walks through my solution.\n\n<!-- more -->\n\n## The Problem\n\nOur regex engine will support the following syntax:\n\n| Syntax | Meaning | Example | matches |\n|--------|---------|---------|---------|\n| a | Matches the specified character literal | q | q |\n| * | Matches 0 or more of the previous character | a* | \"\", a, aa, aaa  |\n| ? | Matches 0 or 1 of the previous character | a? | \"\", a |\n| . | Matches any character literal | . | a, b, c, d, e ... |\n| ^ | Matches the start of a string | ^c | c, ca, caa, cbb ... |\n| $ | Matches the end of a string | a$ | ba, baaa, qwerta ... |\n\nThe goal is to provide a syntax robust enough to match a large portion of regex use cases with minimal code.\n\n## Matching One Character\n\nThe first step is to write a function that takes in a one character pattern and a one character text string and returns a boolean indicating if they match. A pattern of `.` is considered a wildcard and matches against any character literal.\n\nHere are some examples\n\n`matchOne('a', 'a')` -> `true`\n`matchOne('.', 'z')` -> `true`\n`matchOne('', 'h')`  -> `true`\n`matchOne('a', 'b')` -> `false`\n`matchOne('p', '')`  -> `false`\n\n\n```js\nfunction matchOne(pattern, text) {\n  if (!pattern) return true // Any text matches an empty pattern\n  if (!text) return false   // If the pattern is defined but the text is empty, there cannot be a match\n  if (pattern === \".\") return true // Any inputted text matches the wildcard\n  return pattern === text\n}\n```\n\n## Matching Same Length Strings\n\nNow we want to add support for patterns and text strings of greater length. For now, let's only consider a pattern/text pair of the same length. I happen to know that the solution lends itself very naturally to recursion, so we will use it here. We are going to want to repeatedly invoke `matchOne` on successive pairs of characters from the pattern/text combination.\n\n```js\nfunction match(pattern, text) {\n  if (pattern === \"\") return true  // Our base case - if the pattern is empty, any inputted text is a match\n  else return matchOne(pattern[0], text[0]) && match(pattern.slice(1), text.slice(1))\n}\n```\n\nThe above code advances character by character across the the pattern/text pair. It first compares `pattern[0]` to `text[0]` and then `pattern[1]` to `text[1]` and continues comparing `pattern[i]` to `text[i]` until `i === pattern.length - 1`. If they ever don't match, then we know that the pattern cannot match the text.\n\nLet's take an example. Suppose we invoke `match('a.c', 'abc')`, which returns `matchOne('a', 'a') && match('.c', 'bc')`.\n\nIf we continue evaluating these functions, we get `matchOne('a', 'a') && matchOne('.', 'b') && matchOne('c', 'c') && match(\"\", \"\")`, which is just equal to `true && true && true && true`, So we have a match!\n\n## The $ Character\n\nLet's add support for the special pattern character `$` that allows us to match the end of a string. The solution simply requires adding an additional base case to the match function.\n\n```js\nfunction match(pattern, text) {\n  if (pattern === \"\") return true\n  if (pattern === \"$\" && text === \"\") return true\n  else return matchOne(pattern[0], text[0]) && match(pattern.slice(1), text.slice(1))\n}\n```\n\n## The ^ Character\n\nLet's add support for the special pattern character `^` that allows us to match the beginning of a string. I'm going to introduce a new function called `search`.\n\n```js\nfunction search(pattern, text) {\n  if (pattern[0] === \"^\") {\n    return match(pattern.slice(1), text)\n  }\n}\n```\n\nThis function will be the new entry point to our code. Up till this point, we were only matching patterns that began at the beginning of the text. We are simply making that more clear now by forcing the user to preface the pattern with a `^`. But how do we support patterns that appear anywhere within the text?\n\n## Matches Starting Anywhere\n\nCurrently, the following return `true`\n\n`search(\"^abc\", \"abc\")`\n`search(\"^abcd\", \"abcd\")`\n\nBut `search(\"bc\", \"abcd\")` will just return `undefined`. We want it to return `true`\n\nIf the user does not specify that the pattern matches the beginning of the text, then we want to search for that pattern at every possible starting point within the text. We will default to this behavior if the pattern does not begin with `^`<sup>[1](#footnote1)</sup>.\n\n```js\nfunction search(pattern, text) {\n  if (pattern[0] === \"^\") {\n    return match(pattern.slice(1), text)\n  } else {\n    // This code will run match(pattern, text.slice(index)) on every index of the text.\n    // This means that we test the pattern against every starting point of the text.\n    return text.split(\"\").some((_, index) => {\n      return match(pattern, text.slice(index))\n    })\n  }\n}\n```\n\n## The ? Character\n\nWe want to be able to match 0 to 1 of the character before `?`.\n\nHere are some examples\n\n`search(\"ab?c\", \"ac\")`    -> `true`\n`search(\"ab?c\", \"abc\")`   -> `true`\n`search(\"a?b?c?\", \"abc\")` -> `true`\n`search(\"a?b?c?\", \"\")`    -> `true`\n\nThe first step is to modify `match` to detect when a `?` character is present and then delegate to the `matchQuestion` function, which we will define shortly.\n\n```js\nfunction match(pattern, text) {\n  if (pattern === \"\") {\n    return true\n  } else if (pattern === \"$\" && text === \"\") {\n    return true\n  // Notice that we are looking at pattern[1] instead of pattern[0].\n  // pattern[0] is the character to match 0 or 1 of.\n  } else if (pattern[1] === \"?\") {\n    return matchQuestion(pattern, text)\n  } else {\n    return matchOne(pattern[0], text[0]) && match(pattern.slice(1), text.slice(1))\n  }\n}\n```\n\n`matchQuestion` needs to handle two cases:\n1. Where the character before the `?` is not matched but the text matches the remainder of the pattern (everything after the `?`).\n2. Where the character before the `?` is matched and the rest of the text (minus the 1 matched character) matches the remainder of the pattern.\n\nIf either of these cases is truthy, then `matchQuestion` can return `true`.\n\nLet's consider the first case. How do we check if the text matches everything in the pattern except the `_?` syntax? In order words, how do we check if the character before the `?` appears 0 times? We strip 2 characters off the pattern (the first character is the one before the `?` and the second is the `?` itself) and invoke the match function.\n\n```js\nfunction matchQuestion(pattern, text) {\n  return match(pattern.slice(2), text);\n}\n```\n\nThe second case is a little more challenging, but just like before, it reuses functions we've already written\n\n```js\nfunction matchQuestion(pattern, text) {\n  if (matchOne(pattern[0], text[0]) && match(pattern.slice(2), text.slice(1))) {\n    return true;\n  } else {\n    return match(pattern.slice(2), text);\n  }\n}\n```\n\nIf the `text[0]` matches `pattern[0]`, and the rest of the text (minus the part that is matched by `matchOne`) matches the remainder of the pattern, then we are golden. Note that we could rewrite the code like this:\n\n```js\nfunction matchQuestion(pattern, text) {\n  return (matchOne(pattern[0], text[0]) && match(pattern.slice(2), text.slice(1))) || match(pattern.slice(2), text);\n}\n```\n\nThe one thing I like about this latter approach is that the boolean OR makes it explicitly clear that there are two cases, either of which may be true.\n\n## The * Character\n\nWe want to be able to match the character before the `*` 0 or more times.\n\nAll of these should return `true`.\n\n`search(\"a*\", \"\")`\n`search(\"a*\", \"aaaaaaa\")`\n`search(\"a*b\", \"aaaaaaab\")`\n\nSimilar to what we did when supporting `?`, we wan to delegate to a `matchStar` function within our `match` function\n\n```js\nfunction match(pattern, text) {\n  if (pattern === \"\") {\n    return true\n  } else if (pattern === \"$\" && text === \"\") {\n    return true\n  } else if (pattern[1] === \"?\") {\n    return matchQuestion(pattern, text)\n  } else if (pattern[1] === \"*\") {\n    return matchStar(pattern, text)\n  } else {\n    return matchOne(pattern[0], text[0]) && match(pattern.slice(1), text.slice(1))\n  }\n}\n```\n\n`matchStar`, like `matchQuestion`, also needs to handle two cases:\n1. Where the character before the `*` is not matched but the text matches the remainder of the pattern (everything after the `*`).\n2. Where the character before the `*` is matched one or more times and the rest of the text matches the remainder of the pattern.\n\nSince there are two cases that both result in a match (0 matches OR more matches), we know that `matchStar` can be implemented with a boolean OR. Furthermore, case 1 for `matchStar` is exactly the same as it was for `matchQuestion` and can be implemented identically using `match(pattern.slice(2), text)`. That means we only need to formulate an expression that satisfies case 2.\n\n```js\nfunction matchStar(pattern, text) {\n  return (matchOne(pattern[0], text[0]) && match(pattern, text.slice(1))) || match(pattern.slice(2), text);\n}\n```\n\n## Refactoring\n\nWe can now go back and cleverly simplify `search` using a trick I learned in Peter Norvig's [class](https://www.udacity.com/course/design-of-computer-programs--cs212).\n\n```js\nfunction search(pattern, text) {\n  if (pattern[0] === \"^\") {\n    return match(pattern.slice(1), text)\n  } else {\n    return match(\".*\" + pattern, text)\n  }\n}\n```\n\nWe use the `*` character itself to allow for the pattern to appear anywhere in the string. The prepended `.*` says that any number of any character can appear before the pattern we wish to match.\n\n## Conclusion\n\nIt's remarkable how simple and elegant the code for such a sophisticated and generalized program can be. The full source is available in this [GitHub repository](https://github.com/nadrane/build-your-own-regex)\n\nHere is a [follow up article](https://nickdrane.com/regex-and-automated-test-fuzzing/) where I fuzz test the regex engine.\n\n\n#### Footnotes\n<a name=\"footnote1\">1</a>: There is a small bug in this code that I'm choosing to ignore. We don't account for the case that text is an empty string. Currently when `text === ''`, `text.split(\"\")` will return `[]` and will not appropriately call `match`.",
        "type": "MarkdownRemark",
        "contentDigest": "396dc3ace105d30367e295b02a2ac305",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Build a Regex Engine in Less than 40 Lines of Code",
        "date": "2017-11-28T11:36:04.000Z",
        "categories": [
          [
            "Regular Expressions"
          ],
          [
            "Javascript"
          ],
          [
            "Recursion"
          ],
          [
            "Build Your Own"
          ]
        ],
        "_PARENT": "031a5100-2916-5e59-a79a-799fbfff2048"
      },
      "excerpt": "\nI stumbled upon an [article](https://www.cs.princeton.edu/courses/archive/spr09/cos333/beautiful.html) the other day where Rob Pike implements a rudimentary regular expression engine in c. I converted his code to Javascript and added test specs so that someone can self-guide themselves through the creation of the regex engine. The specs and solution can be found in this [GitHub repository](https://github.com/nadrane/build-your-own-regex). This blog post walks through my solution.\n\n",
      "rawMarkdownBody": "\nI stumbled upon an [article](https://www.cs.princeton.edu/courses/archive/spr09/cos333/beautiful.html) the other day where Rob Pike implements a rudimentary regular expression engine in c. I converted his code to Javascript and added test specs so that someone can self-guide themselves through the creation of the regex engine. The specs and solution can be found in this [GitHub repository](https://github.com/nadrane/build-your-own-regex). This blog post walks through my solution.\n\n<!-- more -->\n\n## The Problem\n\nOur regex engine will support the following syntax:\n\n| Syntax | Meaning | Example | matches |\n|--------|---------|---------|---------|\n| a | Matches the specified character literal | q | q |\n| * | Matches 0 or more of the previous character | a* | \"\", a, aa, aaa  |\n| ? | Matches 0 or 1 of the previous character | a? | \"\", a |\n| . | Matches any character literal | . | a, b, c, d, e ... |\n| ^ | Matches the start of a string | ^c | c, ca, caa, cbb ... |\n| $ | Matches the end of a string | a$ | ba, baaa, qwerta ... |\n\nThe goal is to provide a syntax robust enough to match a large portion of regex use cases with minimal code.\n\n## Matching One Character\n\nThe first step is to write a function that takes in a one character pattern and a one character text string and returns a boolean indicating if they match. A pattern of `.` is considered a wildcard and matches against any character literal.\n\nHere are some examples\n\n`matchOne('a', 'a')` -> `true`\n`matchOne('.', 'z')` -> `true`\n`matchOne('', 'h')`  -> `true`\n`matchOne('a', 'b')` -> `false`\n`matchOne('p', '')`  -> `false`\n\n\n```js\nfunction matchOne(pattern, text) {\n  if (!pattern) return true // Any text matches an empty pattern\n  if (!text) return false   // If the pattern is defined but the text is empty, there cannot be a match\n  if (pattern === \".\") return true // Any inputted text matches the wildcard\n  return pattern === text\n}\n```\n\n## Matching Same Length Strings\n\nNow we want to add support for patterns and text strings of greater length. For now, let's only consider a pattern/text pair of the same length. I happen to know that the solution lends itself very naturally to recursion, so we will use it here. We are going to want to repeatedly invoke `matchOne` on successive pairs of characters from the pattern/text combination.\n\n```js\nfunction match(pattern, text) {\n  if (pattern === \"\") return true  // Our base case - if the pattern is empty, any inputted text is a match\n  else return matchOne(pattern[0], text[0]) && match(pattern.slice(1), text.slice(1))\n}\n```\n\nThe above code advances character by character across the the pattern/text pair. It first compares `pattern[0]` to `text[0]` and then `pattern[1]` to `text[1]` and continues comparing `pattern[i]` to `text[i]` until `i === pattern.length - 1`. If they ever don't match, then we know that the pattern cannot match the text.\n\nLet's take an example. Suppose we invoke `match('a.c', 'abc')`, which returns `matchOne('a', 'a') && match('.c', 'bc')`.\n\nIf we continue evaluating these functions, we get `matchOne('a', 'a') && matchOne('.', 'b') && matchOne('c', 'c') && match(\"\", \"\")`, which is just equal to `true && true && true && true`, So we have a match!\n\n## The $ Character\n\nLet's add support for the special pattern character `$` that allows us to match the end of a string. The solution simply requires adding an additional base case to the match function.\n\n```js\nfunction match(pattern, text) {\n  if (pattern === \"\") return true\n  if (pattern === \"$\" && text === \"\") return true\n  else return matchOne(pattern[0], text[0]) && match(pattern.slice(1), text.slice(1))\n}\n```\n\n## The ^ Character\n\nLet's add support for the special pattern character `^` that allows us to match the beginning of a string. I'm going to introduce a new function called `search`.\n\n```js\nfunction search(pattern, text) {\n  if (pattern[0] === \"^\") {\n    return match(pattern.slice(1), text)\n  }\n}\n```\n\nThis function will be the new entry point to our code. Up till this point, we were only matching patterns that began at the beginning of the text. We are simply making that more clear now by forcing the user to preface the pattern with a `^`. But how do we support patterns that appear anywhere within the text?\n\n## Matches Starting Anywhere\n\nCurrently, the following return `true`\n\n`search(\"^abc\", \"abc\")`\n`search(\"^abcd\", \"abcd\")`\n\nBut `search(\"bc\", \"abcd\")` will just return `undefined`. We want it to return `true`\n\nIf the user does not specify that the pattern matches the beginning of the text, then we want to search for that pattern at every possible starting point within the text. We will default to this behavior if the pattern does not begin with `^`<sup>[1](#footnote1)</sup>.\n\n```js\nfunction search(pattern, text) {\n  if (pattern[0] === \"^\") {\n    return match(pattern.slice(1), text)\n  } else {\n    // This code will run match(pattern, text.slice(index)) on every index of the text.\n    // This means that we test the pattern against every starting point of the text.\n    return text.split(\"\").some((_, index) => {\n      return match(pattern, text.slice(index))\n    })\n  }\n}\n```\n\n## The ? Character\n\nWe want to be able to match 0 to 1 of the character before `?`.\n\nHere are some examples\n\n`search(\"ab?c\", \"ac\")`    -> `true`\n`search(\"ab?c\", \"abc\")`   -> `true`\n`search(\"a?b?c?\", \"abc\")` -> `true`\n`search(\"a?b?c?\", \"\")`    -> `true`\n\nThe first step is to modify `match` to detect when a `?` character is present and then delegate to the `matchQuestion` function, which we will define shortly.\n\n```js\nfunction match(pattern, text) {\n  if (pattern === \"\") {\n    return true\n  } else if (pattern === \"$\" && text === \"\") {\n    return true\n  // Notice that we are looking at pattern[1] instead of pattern[0].\n  // pattern[0] is the character to match 0 or 1 of.\n  } else if (pattern[1] === \"?\") {\n    return matchQuestion(pattern, text)\n  } else {\n    return matchOne(pattern[0], text[0]) && match(pattern.slice(1), text.slice(1))\n  }\n}\n```\n\n`matchQuestion` needs to handle two cases:\n1. Where the character before the `?` is not matched but the text matches the remainder of the pattern (everything after the `?`).\n2. Where the character before the `?` is matched and the rest of the text (minus the 1 matched character) matches the remainder of the pattern.\n\nIf either of these cases is truthy, then `matchQuestion` can return `true`.\n\nLet's consider the first case. How do we check if the text matches everything in the pattern except the `_?` syntax? In order words, how do we check if the character before the `?` appears 0 times? We strip 2 characters off the pattern (the first character is the one before the `?` and the second is the `?` itself) and invoke the match function.\n\n```js\nfunction matchQuestion(pattern, text) {\n  return match(pattern.slice(2), text);\n}\n```\n\nThe second case is a little more challenging, but just like before, it reuses functions we've already written\n\n```js\nfunction matchQuestion(pattern, text) {\n  if (matchOne(pattern[0], text[0]) && match(pattern.slice(2), text.slice(1))) {\n    return true;\n  } else {\n    return match(pattern.slice(2), text);\n  }\n}\n```\n\nIf the `text[0]` matches `pattern[0]`, and the rest of the text (minus the part that is matched by `matchOne`) matches the remainder of the pattern, then we are golden. Note that we could rewrite the code like this:\n\n```js\nfunction matchQuestion(pattern, text) {\n  return (matchOne(pattern[0], text[0]) && match(pattern.slice(2), text.slice(1))) || match(pattern.slice(2), text);\n}\n```\n\nThe one thing I like about this latter approach is that the boolean OR makes it explicitly clear that there are two cases, either of which may be true.\n\n## The * Character\n\nWe want to be able to match the character before the `*` 0 or more times.\n\nAll of these should return `true`.\n\n`search(\"a*\", \"\")`\n`search(\"a*\", \"aaaaaaa\")`\n`search(\"a*b\", \"aaaaaaab\")`\n\nSimilar to what we did when supporting `?`, we wan to delegate to a `matchStar` function within our `match` function\n\n```js\nfunction match(pattern, text) {\n  if (pattern === \"\") {\n    return true\n  } else if (pattern === \"$\" && text === \"\") {\n    return true\n  } else if (pattern[1] === \"?\") {\n    return matchQuestion(pattern, text)\n  } else if (pattern[1] === \"*\") {\n    return matchStar(pattern, text)\n  } else {\n    return matchOne(pattern[0], text[0]) && match(pattern.slice(1), text.slice(1))\n  }\n}\n```\n\n`matchStar`, like `matchQuestion`, also needs to handle two cases:\n1. Where the character before the `*` is not matched but the text matches the remainder of the pattern (everything after the `*`).\n2. Where the character before the `*` is matched one or more times and the rest of the text matches the remainder of the pattern.\n\nSince there are two cases that both result in a match (0 matches OR more matches), we know that `matchStar` can be implemented with a boolean OR. Furthermore, case 1 for `matchStar` is exactly the same as it was for `matchQuestion` and can be implemented identically using `match(pattern.slice(2), text)`. That means we only need to formulate an expression that satisfies case 2.\n\n```js\nfunction matchStar(pattern, text) {\n  return (matchOne(pattern[0], text[0]) && match(pattern, text.slice(1))) || match(pattern.slice(2), text);\n}\n```\n\n## Refactoring\n\nWe can now go back and cleverly simplify `search` using a trick I learned in Peter Norvig's [class](https://www.udacity.com/course/design-of-computer-programs--cs212).\n\n```js\nfunction search(pattern, text) {\n  if (pattern[0] === \"^\") {\n    return match(pattern.slice(1), text)\n  } else {\n    return match(\".*\" + pattern, text)\n  }\n}\n```\n\nWe use the `*` character itself to allow for the pattern to appear anywhere in the string. The prepended `.*` says that any number of any character can appear before the pattern we wish to match.\n\n## Conclusion\n\nIt's remarkable how simple and elegant the code for such a sophisticated and generalized program can be. The full source is available in this [GitHub repository](https://github.com/nadrane/build-your-own-regex)\n\nHere is a [follow up article](https://nickdrane.com/regex-and-automated-test-fuzzing/) where I fuzz test the regex engine.\n\n\n#### Footnotes\n<a name=\"footnote1\">1</a>: There is a small bug in this code that I'm choosing to ignore. We don't account for the case that text is an empty string. Currently when `text === ''`, `text.split(\"\")` will return `[]` and will not appropriately call `match`.",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/build-your-own-regex.md"
    },
    "4a464680-e690-5613-804d-8fcf3746b0d6": {
      "id": "4a464680-e690-5613-804d-8fcf3746b0d6",
      "children": [],
      "parent": "4e6a29de-aee9-55df-9a0a-d01a48c0bae1",
      "internal": {
        "content": "\n[Postgres](https://www.postgresql.org/) introduced the [JSONB](https://www.postgresql.org/docs/current/static/datatype-json.html) type in version 9.4 with considerable excitement. JSONB promised to marry a favorite relational database with the noSQL world, permitting data to be stored in the database as JSON without the need for re-parsing whenever a field is accessed. Moreover, the binary storage format permits indexing and complex queries against the stored JSON blobs. This data format embodies the flexible schema and was readily adopted at [Fraight](https://fraight.ai/).\n\n<!-- more -->\n\n## Background\n\nAt Fraight, we've built a centralized communication platform that collates all inbound/outbound communications between our brokerage company and thousands of trucking partners. One of our main objectives is to build a system that parses and automatically responds to inbound text messages, emails, and faxes. We knew we would eventually need the nitty-gritty details of these messages, so we captured the data by dumping entire http response bodies into a JSONB column named `meta` in our databases `message` table.\n\nIn an ideal world, the third party API responses we collected would have been broken down into discrete chunks and stored in separate columns, but our approach was a pragmatic design decision. We knew it wasn't worth the engineering effort to try to understand the multitude of fields we receive from a half-dozen APIs, particularly when we had no idea at the time how we might use this information. But we did know it was valuable. And since we wanted it to be queryable, we chose JSONB over it's more inert and sometimes more size efficient cousin, the JSON datatype.\n\n## Discovery\n\nFraights CTO approached me the other day and explained that query performance over the `message` table had deteriorated. He elaborated that the queries were only slow when the `meta` column was included in the result set. We had previously experienced slowdown in the `message` table when entire email attachment bodies were getting serialized and stored in the `meta` column, and I suspected that the root cause of our current performance problem was in a similar vein. A quick [query](#footnote1) revealed that our `meta` column was often quite large.\n\nThe average size of the `meta` column was 3.7 kb. That might not seem large, but for our 100,000 row table, that meant 400mb of (mostly unused) metadata. At the high end of the spectrum, some messages were up to 17mb in size. The precise details of why this dataset slows down queries are a bit esoteric<sup>[2](#footnote2)</sup>, but it was clear that we were storing too much information. You can read more about how Postgres stores large data values using [TOAST](https://www.postgresql.org/docs/current/static/storage-toast.html) and [how I validated this was a problem](#footnote3).\n\n\n## Further Complications\n\nThe first queries against the `meta` column were for very simple tasks like showing the raw contents of an email to a user, perhaps in the case where message content extraction failed. Over time we began using specific fields to drive business logic, and our application code started to expect that `meta`'s JSON would adhere to a specific shape. Any proposed solution would likely necessitate changes to this application code.\n\nSo the problem was twofold:\n\n1. We needed to extract the actual metadata (as opposed to fields that drove business logic) from the `meta` column and place it in a location where it would not affect query performance over the `message` table.\n\n2. We wanted to make the metadata less easily accessible. We recognized that making it accessible through the ORM made it ripe for misuse. We wanted to give it an alternative API that would lessen a developer's likeliness to rely on its structure.\n\n## Solutioning\n\nWe looked at several solutions. Below are the three we considered most seriously:\n\n1. Use the ORM to omit the `meta` column from all queries unless specifically included. Since we unnecessarily perform `select *` <sup>[4](#footnote4)</sup> queries over the `message` table, this strategy, despite its messiness, would increase the performance in most cases (with the occasional slow query when metadata was actually required) and would in theory be simple to implement. It wouldn't, however, resolve our initial design shortcut.\n\n2. Keep the `meta` column as-is but extract specific keys to [S3](https://aws.amazon.com/s3/). In general, greater than 99% of the size of any given column's `meta` field was from a single key. For example, many of the emails we capture include large attachments. Other times message bodies retain long, historical email chains. By extracting these problematic fields and uploading them to S3, the database would only need to store a reference to the S3 content. Then, upon request, the server could generate a [pre-signed URL](https://docs.aws.amazon.com/AmazonS3/latest/dev//ShareObjectPreSignedURL.html), allowing the client to download large files directly from S3.\n\n3. Create a separate `metadata` table in Postgres. It would have a foreign key back to the original table where the metadata belonged. This solution solves the `select *` problem described above and offers an additional advantage: since the `meta` column pattern exists on more than just the `message` table, it offers a unified strategy for storing metadata.\n\nOur instinct was to go with #1  the most simple and straightforward solution  and omit the `meta` column from most queries. Unfortunately, bugs in our ORM made it impossible to omit columns across joins without the occasional crash. We needed an alternate approach.\n\nThis hiccup left the choice between S3 and a separate Postgres table. We agreed that pre-signed S3 URLs offered an ideal long-term alternative but ultimately chose a separate Postgres table for the same reason that we wanted to choose option #1: minimal risk and complexity. Our team is exceptionally experienced with Postgres, and we knew we could hit the ground running. S3, in contrast, had more unknowns, and given how rarely we access most of this metadata today, the value it would add over Postgres was tenuous at best.\n\n\nIn addition to migrating metadata to its own table, we took a couple additional steps:\n\n1. We extracted the handful of regularly used fields from the metadata and migrated them into their own columns in the `message` table. This meant that we didn't need to retrieve large, megabyte sized blobs whenever we wanted a single field<sup>[5](#footnote5)</sup>. As an added advantage, we regained simple access to database [constraints](https://www.postgresql.org/docs/current/static/ddl-constraints.html).\n\n2. When we created the new `metadata` table in Postgres, we made sure not to define a relationship between it and its related tables at the ORM layer, only the database layer. This makes it far more difficult for a developer to hobble performance by absentmindedly joining large metadata into queries. We introduced an API for accessing the metadata instead. The added advantage of this API is that we can now change the underlying implementation to use S3 (or anything else) in the future, without modifying dependent application code.\n\n\nMy name is Nick Drane. I do [consulting](/hire-me) work in the Chicago area and am always looking for new opportunities.\n\n#### Footnotes\n\n<a name=\"footnote1\">1</a>: I used the following query:\n\n```sql\nSELECT avg(octet_length(m.\"meta\"::text))\nFROM message as m\n```\n\n<a name=\"footnote2\">2</a>: I should clarify that performance measurements were done with a local Postgres installation where network congestion/throughput is not a relevant factor.\n\n<a name=\"footnote3\">3</a>:\nI verified that the `meta` column was toasting with a little help from the [Postgres docs](https://www.postgresql.org/docs/10/static/disk-usage.html):\n\n```sql\nSELECT relname, relpages, relpages * 8191 / (1024 * 1024) as size\nFROM pg_class,\n     (SELECT reltoastrelid\n      FROM pg_class\n      WHERE relname = 'message') AS ss\nWHERE oid = ss.reltoastrelid OR\n      oid = (SELECT indexrelid\n             FROM pg_index\n             WHERE indrelid = ss.reltoastrelid)\n```\n\nFor specifically the `message` table, this query returns the number of disk pages in the toast table and their total size in megabytes.\n\n| table       | disk pages | size (mb) |\n|-------------|------------|-----------|\n| toast\ttable | 17731      | 139       |\n| toast index | 209        | 1         |\n\n\n<a name=\"footnote4\">4</a>: We were not literally doing any `select *` queries. Our ORM's message model did, however, specify all of the columns of the `message` table. In retrospect, this was probably our biggest mistake. It meant that a call to `Message.find`, which is used all over the place, retrieved all columns on the `message` table, unless specifically directed otherwise. Usually, for a RESTful API, this is an acceptable performance tradeoff, but it didn't hold true in this circumstance.\n\n<a name=\"footnote5\">5</a>: As I write this, I wonder if it wouldn't have been possible to leverage indexes to only retrieve specific pieces of the `meta` field. I wonder if our ORM provides any support for firstclass fields that are subfields of another field.\n",
        "type": "MarkdownRemark",
        "contentDigest": "1201288631e3c8dbeccdd4a792f24ccd",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "The Hidden Costs of PostgreSQL's JSONB Datatype",
        "date": "2018-09-30T00:00:00.000Z",
        "categories": [
          [
            "Postgres"
          ],
          [
            "Architecture"
          ]
        ],
        "_PARENT": "4e6a29de-aee9-55df-9a0a-d01a48c0bae1"
      },
      "excerpt": "\n[Postgres](https://www.postgresql.org/) introduced the [JSONB](https://www.postgresql.org/docs/current/static/datatype-json.html) type in version 9.4 with considerable excitement. JSONB promised to marry a favorite relational database with the noSQL world, permitting data to be stored in the database as JSON without the need for re-parsing whenever a field is accessed. Moreover, the binary storage format permits indexing and complex queries against the stored JSON blobs. This data format embodies the flexible schema and was readily adopted at [Fraight](https://fraight.ai/).\n\n",
      "rawMarkdownBody": "\n[Postgres](https://www.postgresql.org/) introduced the [JSONB](https://www.postgresql.org/docs/current/static/datatype-json.html) type in version 9.4 with considerable excitement. JSONB promised to marry a favorite relational database with the noSQL world, permitting data to be stored in the database as JSON without the need for re-parsing whenever a field is accessed. Moreover, the binary storage format permits indexing and complex queries against the stored JSON blobs. This data format embodies the flexible schema and was readily adopted at [Fraight](https://fraight.ai/).\n\n<!-- more -->\n\n## Background\n\nAt Fraight, we've built a centralized communication platform that collates all inbound/outbound communications between our brokerage company and thousands of trucking partners. One of our main objectives is to build a system that parses and automatically responds to inbound text messages, emails, and faxes. We knew we would eventually need the nitty-gritty details of these messages, so we captured the data by dumping entire http response bodies into a JSONB column named `meta` in our databases `message` table.\n\nIn an ideal world, the third party API responses we collected would have been broken down into discrete chunks and stored in separate columns, but our approach was a pragmatic design decision. We knew it wasn't worth the engineering effort to try to understand the multitude of fields we receive from a half-dozen APIs, particularly when we had no idea at the time how we might use this information. But we did know it was valuable. And since we wanted it to be queryable, we chose JSONB over it's more inert and sometimes more size efficient cousin, the JSON datatype.\n\n## Discovery\n\nFraights CTO approached me the other day and explained that query performance over the `message` table had deteriorated. He elaborated that the queries were only slow when the `meta` column was included in the result set. We had previously experienced slowdown in the `message` table when entire email attachment bodies were getting serialized and stored in the `meta` column, and I suspected that the root cause of our current performance problem was in a similar vein. A quick [query](#footnote1) revealed that our `meta` column was often quite large.\n\nThe average size of the `meta` column was 3.7 kb. That might not seem large, but for our 100,000 row table, that meant 400mb of (mostly unused) metadata. At the high end of the spectrum, some messages were up to 17mb in size. The precise details of why this dataset slows down queries are a bit esoteric<sup>[2](#footnote2)</sup>, but it was clear that we were storing too much information. You can read more about how Postgres stores large data values using [TOAST](https://www.postgresql.org/docs/current/static/storage-toast.html) and [how I validated this was a problem](#footnote3).\n\n\n## Further Complications\n\nThe first queries against the `meta` column were for very simple tasks like showing the raw contents of an email to a user, perhaps in the case where message content extraction failed. Over time we began using specific fields to drive business logic, and our application code started to expect that `meta`'s JSON would adhere to a specific shape. Any proposed solution would likely necessitate changes to this application code.\n\nSo the problem was twofold:\n\n1. We needed to extract the actual metadata (as opposed to fields that drove business logic) from the `meta` column and place it in a location where it would not affect query performance over the `message` table.\n\n2. We wanted to make the metadata less easily accessible. We recognized that making it accessible through the ORM made it ripe for misuse. We wanted to give it an alternative API that would lessen a developer's likeliness to rely on its structure.\n\n## Solutioning\n\nWe looked at several solutions. Below are the three we considered most seriously:\n\n1. Use the ORM to omit the `meta` column from all queries unless specifically included. Since we unnecessarily perform `select *` <sup>[4](#footnote4)</sup> queries over the `message` table, this strategy, despite its messiness, would increase the performance in most cases (with the occasional slow query when metadata was actually required) and would in theory be simple to implement. It wouldn't, however, resolve our initial design shortcut.\n\n2. Keep the `meta` column as-is but extract specific keys to [S3](https://aws.amazon.com/s3/). In general, greater than 99% of the size of any given column's `meta` field was from a single key. For example, many of the emails we capture include large attachments. Other times message bodies retain long, historical email chains. By extracting these problematic fields and uploading them to S3, the database would only need to store a reference to the S3 content. Then, upon request, the server could generate a [pre-signed URL](https://docs.aws.amazon.com/AmazonS3/latest/dev//ShareObjectPreSignedURL.html), allowing the client to download large files directly from S3.\n\n3. Create a separate `metadata` table in Postgres. It would have a foreign key back to the original table where the metadata belonged. This solution solves the `select *` problem described above and offers an additional advantage: since the `meta` column pattern exists on more than just the `message` table, it offers a unified strategy for storing metadata.\n\nOur instinct was to go with #1  the most simple and straightforward solution  and omit the `meta` column from most queries. Unfortunately, bugs in our ORM made it impossible to omit columns across joins without the occasional crash. We needed an alternate approach.\n\nThis hiccup left the choice between S3 and a separate Postgres table. We agreed that pre-signed S3 URLs offered an ideal long-term alternative but ultimately chose a separate Postgres table for the same reason that we wanted to choose option #1: minimal risk and complexity. Our team is exceptionally experienced with Postgres, and we knew we could hit the ground running. S3, in contrast, had more unknowns, and given how rarely we access most of this metadata today, the value it would add over Postgres was tenuous at best.\n\n\nIn addition to migrating metadata to its own table, we took a couple additional steps:\n\n1. We extracted the handful of regularly used fields from the metadata and migrated them into their own columns in the `message` table. This meant that we didn't need to retrieve large, megabyte sized blobs whenever we wanted a single field<sup>[5](#footnote5)</sup>. As an added advantage, we regained simple access to database [constraints](https://www.postgresql.org/docs/current/static/ddl-constraints.html).\n\n2. When we created the new `metadata` table in Postgres, we made sure not to define a relationship between it and its related tables at the ORM layer, only the database layer. This makes it far more difficult for a developer to hobble performance by absentmindedly joining large metadata into queries. We introduced an API for accessing the metadata instead. The added advantage of this API is that we can now change the underlying implementation to use S3 (or anything else) in the future, without modifying dependent application code.\n\n\nMy name is Nick Drane. I do [consulting](/hire-me) work in the Chicago area and am always looking for new opportunities.\n\n#### Footnotes\n\n<a name=\"footnote1\">1</a>: I used the following query:\n\n```sql\nSELECT avg(octet_length(m.\"meta\"::text))\nFROM message as m\n```\n\n<a name=\"footnote2\">2</a>: I should clarify that performance measurements were done with a local Postgres installation where network congestion/throughput is not a relevant factor.\n\n<a name=\"footnote3\">3</a>:\nI verified that the `meta` column was toasting with a little help from the [Postgres docs](https://www.postgresql.org/docs/10/static/disk-usage.html):\n\n```sql\nSELECT relname, relpages, relpages * 8191 / (1024 * 1024) as size\nFROM pg_class,\n     (SELECT reltoastrelid\n      FROM pg_class\n      WHERE relname = 'message') AS ss\nWHERE oid = ss.reltoastrelid OR\n      oid = (SELECT indexrelid\n             FROM pg_index\n             WHERE indrelid = ss.reltoastrelid)\n```\n\nFor specifically the `message` table, this query returns the number of disk pages in the toast table and their total size in megabytes.\n\n| table       | disk pages | size (mb) |\n|-------------|------------|-----------|\n| toast\ttable | 17731      | 139       |\n| toast index | 209        | 1         |\n\n\n<a name=\"footnote4\">4</a>: We were not literally doing any `select *` queries. Our ORM's message model did, however, specify all of the columns of the `message` table. In retrospect, this was probably our biggest mistake. It meant that a call to `Message.find`, which is used all over the place, retrieved all columns on the `message` table, unless specifically directed otherwise. Usually, for a RESTful API, this is an acceptable performance tradeoff, but it didn't hold true in this circumstance.\n\n<a name=\"footnote5\">5</a>: As I write this, I wonder if it wouldn't have been possible to leverage indexes to only retrieve specific pieces of the `meta` field. I wonder if our ORM provides any support for firstclass fields that are subfields of another field.\n",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/hidden-costs-of-postgresql-jsonb.md"
    },
    "74ada755-89ba-515c-9224-f120cb838f38": {
      "id": "74ada755-89ba-515c-9224-f120cb838f38",
      "children": [],
      "parent": "1e0fb61f-1c42-5e94-89eb-eed2ce2b86a3",
      "internal": {
        "content": "\nMy first introduction to functional programming was a couple years ago when I read through the famous [SICP](https://mitpress.mit.edu/sicp/full-text/book/book.html). As someone who had up to this point worked with mostly in object oriented and imperative languages, I had rarely seen `map`, `fitler`, and `reduce` before that time. The purpose of the former two felt obvious; the latter one not so much. This blog post is geared for someone who knows how `reduce` works but feels like they struggle to use it practically.\n\n<!-- more -->\n\n## Reduce Basics\n_Feel free to skip this section if you understand the basics of reduce_\n\n[Reduce](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/Reduce?v=a) is known as a higher order function (HOF). A HOF is defined as a function that either takes in or returns another function.\n\nReduce takes two parameters: the first is a function, and the second is known as the initial accumulator. We will invoke this function over every element of an array. The ultimate goal is to transform the array into something new.\n\nSuppose we want to take an array of characters and concatenate them into a single string. In this example, the first argument we pass reduce is a HOF that will operate over every character. The HOF takes two arguments: the first is the accumulator and is the value that we ultimately want to assemble, and the second is a particular element of the array.\n\nLet's simulate what happens when we run a solution to the problem of concatenating strings.\n\n```js\nconst arrToConcat = ['a', 'b', 'c', 'd'];\narrToConcat.reduce(function(resultantString, nextCharacter) {\n  return resultantString + nextCharacter;\n}, \"\")  // initial accumulator. This is the second argument to reduce!\n```\n\nYou can use the below table to help guide yourself along\n\n| invocation # | resultantString | nextCharacter | return value\n|:------------:|:---------------:|:-------------:|:-----------:|\n| 1 |  \"\" | 'a' | 'a'\n| 2 |  \"a\" | 'b' | 'ab'\n| 3 |  \"ab\" | 'c' | 'abc'\n| 4 |  \"abc\" | 'd' | 'abcd'\n\nWe will ultimately call our HOF 4 times, passing in each of the 4 elements in the initial array.\n\nThe first time we invoke the HOF, the first argument is always the initial accumulator. Notice the `\"\"` passed into reduce above. The second argument is `'a'` because that's the first element in our array. We return 'a', and this return value is the `resultantString` that is passed as an argument to the second invocation of our HOF, along with the next element of the array, `'b'`. The process continues. `'ab'` is returned from the HOF's second invocation, and `'ab'` and `'c'` are passed into the third invocation of the HOF. Ultimately, `reduce` returns `'abcd'`.\n\nWe built up the result step by step through a series of concatenation steps. Every call to the HOF will return a string with one additional character added to the previous accumulator.\n\n## Know Your Return Type\nI think one of the reasons `map` and `filter` are easier than `reduce` is because they always return an array. That's not the case for `reduce`. `reduce` is ultimately designed to transform one type into another.\n\nWhen you begin writing a `reduce` statement, your first question to yourself should be \"What type am I returning?\". Here's a hint, it's probably one of the following: `String`, `Number`, `Object`, `Array`. Once you know what type you will return you can begin writing your reduce statement. I say this because the initial accumulator (reduce's second argument) can easily be determined by what type you expect `reduce` to return. Here is a table to guide your decision.\n\n| return type | initial accumulator |\n|:-----------:|:--------------------:|\n| string |  \"\" |\n| number | 0 |\n| object | {} |\n| array | [] |\n\n**Notice that the initial accumulator and the return type are always the same!**\n\n_You probably won't want return an array from a reduce function very often because chances are you could have more simply used a combination of `map`, `filter`, and/or some other function instead._\n\n## Summing Odd Numbers\n\nSo let's say we want to reduce over a set of numbers and return the sum of all the odd numbers. Since we are returning a `number`, we can use the above table to determine that the starting accumulator should be 0. That gives us a pretty decent starting point!\n\n```js\nconst arrToSum = [1, 2, 3, 4, 5];\narrToSum.reduce(function(currentSum, nextNumber) {\n\n}, 0)  // This is the second argument to reduce!\n```\n\nNow we just need to complete the guts of the function such that it increments the sum when the `nextNumber` is odd. If `nextNumber` is even, it should return `currentSum` unchanged.\n\n```js\nconst arrToSum = [1, 2, 3, 4, 5];\narrToSum.reduce(function(currentSum, nextNumber) {\n  // nextNumber is odd\n  if (nextNumber % 2 === 1) {\n    return currentSum + nextNumber\n  // nextNumber is even\n  } else {\n    return currentSum\n  }\n}, 0)\n```\n\nOne interesting thing to note about the above code is that EVERY code path (every if-else block) returns something. This is a very common theme throughout the functional programming paradigm and holds true when writing reduce's higher order function. In general it's a decent litmus test to establish if you've made a bug somewhere. **More specifically, if every code path does not return something, and if that something is not the same type as your accumulator, you probably have a bug.**\n\n## Frequency Counter\n\nLet's do something a little different. Suppose I have an array of words, and I want to count how many times each word appears in the array. We can represent this information using an object that maps words in the array to a number indicating their frequency of occurrence.\n\n```js\n// We will input something like this\n['luke', 'anakin', 'chewy', 'luke', 'chewy', 'princess', 'leia', 'chewy'] ->\n\n// And output something like this\n{\n  luke: 2,\n  anakin: 1,\n  princess: 1,\n  chewy: 3,\n  leia: 1\n}\n```\n\nConsulting the table above, we know that the starting accumulator needs to be an empty object. This means that each iteration over the array returns an object as well.\n\n```js\nconst frequencyArray = ['luke', 'anakin', 'chewy', 'luke', 'chewy', 'princess', 'leia', 'chewy'];\nfrequencyArray.reduce(function(resultantObject, nextWord) {\n\n}, {})\n```\n\nEvery step of the higher order function needs to examine the next word and determine if it has been seen before. If it has not, then we need to add an entry to the accumulator and return it. If it has, then we need to increment the existing entry inside the accumulator object and return it.\n\n```js\nfrequencyArray.reduce(function(resultantObject, nextWord) {\n  // If the word is not in the object\n  if (!resultantObject.hasOwnProperty(nextWord)) {\n    resultantObject[nextWord] = 1 //Set it to 1 since we have seen the word before\n    return resultantObject\n  } else {\n    // Otherwise increment the counter for that word inside the resulting object\n    resultantObject[nextWord]++\n    return resultantObject\n  }\n}, {})\n```\n\nJust like the first example, every code path returns the type of the accumulator.\n\n## Merging Objects\nLet's end with something a little more complicated.\n\nSuppose we want to implement a merge function to combines several objects into a single object. We're going to implement something really similar to [Object.assign](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/assign). Here are some examples.\n\n```js\nmerge([ { a: 4, b: 3 }, { c:10 } ]) --> { a: 4, b: 3, c: 10 }\n\nmerge([ { a: 4, b: 3 }, { c:4 }, {f: 7} ]) --> { a: 4, b: 3, c: 4, f: 7}\n\n// Duplicate keys take the value of the latter object\nmerge([ { a: 4, b: 3 }, { c:4 }, {a: 7} ]) --> { a: 7, b: 3, c: 4}\n```\n\nWe know that the return value from this operation is an object, so that will be our starting accumulator. Every iteration of the array will produce a new object that contains the merging of the previous accumulator and the next object.\n\n```js\n// This merge function will take in two objects and output a new object\n// containing both the keys and values from the two input objects.\nfunction mergeTwo(obj1, obj2) {\n  const newObj = {}\n  for (let key in obj1) {\n    newObj[key] = obj1[key]\n  }\n  for (let key in obj2) {\n    newObj[key] = obj2[key]\n  }\n  return newObj\n}\n\nfunction merge(arr) {\n  return arr.reduce(function(resultantObj, nextObj) {\n    return mergeTwo(resultantObj, nextObj)\n  }, {})\n}\n```\n\n## Summary\n\nTo summarize\n\n1. Always start by asking what the type of your output is. The type of your accumulator will follow by using the mapping table.\n3. The logic inside your higher order function should assemble your resulting accumulator piecemeal. Since the return value of the previous iteration is the input to the next iteration, every code path must return a value\n3. Furthermore, unless you want a horrible experience, every code path MUST return a value whose type is the same as that of your initial accumulator.\n\n\n## Conclusion\nAt the end of the day, just about everything you can do with reduce can be done with some combination of `map` and `fitler` and perhaps another [functional method](https://lodash.com/docs/4.17.4]). And the alternative solution is almost always simpler and more readable. So, in practice, you probably don't want to use `reduce` that often. With that said, `reduce` is a building block on which every other functional method can be built, and we will explore this unique trait in my next blog post.",
        "type": "MarkdownRemark",
        "contentDigest": "7209a508713705cf8a8e8e7535cbb4d1",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Using Reduce",
        "date": "2017-09-28T10:36:30.000Z",
        "categories": "Functional Programming",
        "_PARENT": "1e0fb61f-1c42-5e94-89eb-eed2ce2b86a3"
      },
      "excerpt": "\nMy first introduction to functional programming was a couple years ago when I read through the famous [SICP](https://mitpress.mit.edu/sicp/full-text/book/book.html). As someone who had up to this point worked with mostly in object oriented and imperative languages, I had rarely seen `map`, `fitler`, and `reduce` before that time. The purpose of the former two felt obvious; the latter one not so much. This blog post is geared for someone who knows how `reduce` works but feels like they struggle to use it practically.\n\n",
      "rawMarkdownBody": "\nMy first introduction to functional programming was a couple years ago when I read through the famous [SICP](https://mitpress.mit.edu/sicp/full-text/book/book.html). As someone who had up to this point worked with mostly in object oriented and imperative languages, I had rarely seen `map`, `fitler`, and `reduce` before that time. The purpose of the former two felt obvious; the latter one not so much. This blog post is geared for someone who knows how `reduce` works but feels like they struggle to use it practically.\n\n<!-- more -->\n\n## Reduce Basics\n_Feel free to skip this section if you understand the basics of reduce_\n\n[Reduce](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/Reduce?v=a) is known as a higher order function (HOF). A HOF is defined as a function that either takes in or returns another function.\n\nReduce takes two parameters: the first is a function, and the second is known as the initial accumulator. We will invoke this function over every element of an array. The ultimate goal is to transform the array into something new.\n\nSuppose we want to take an array of characters and concatenate them into a single string. In this example, the first argument we pass reduce is a HOF that will operate over every character. The HOF takes two arguments: the first is the accumulator and is the value that we ultimately want to assemble, and the second is a particular element of the array.\n\nLet's simulate what happens when we run a solution to the problem of concatenating strings.\n\n```js\nconst arrToConcat = ['a', 'b', 'c', 'd'];\narrToConcat.reduce(function(resultantString, nextCharacter) {\n  return resultantString + nextCharacter;\n}, \"\")  // initial accumulator. This is the second argument to reduce!\n```\n\nYou can use the below table to help guide yourself along\n\n| invocation # | resultantString | nextCharacter | return value\n|:------------:|:---------------:|:-------------:|:-----------:|\n| 1 |  \"\" | 'a' | 'a'\n| 2 |  \"a\" | 'b' | 'ab'\n| 3 |  \"ab\" | 'c' | 'abc'\n| 4 |  \"abc\" | 'd' | 'abcd'\n\nWe will ultimately call our HOF 4 times, passing in each of the 4 elements in the initial array.\n\nThe first time we invoke the HOF, the first argument is always the initial accumulator. Notice the `\"\"` passed into reduce above. The second argument is `'a'` because that's the first element in our array. We return 'a', and this return value is the `resultantString` that is passed as an argument to the second invocation of our HOF, along with the next element of the array, `'b'`. The process continues. `'ab'` is returned from the HOF's second invocation, and `'ab'` and `'c'` are passed into the third invocation of the HOF. Ultimately, `reduce` returns `'abcd'`.\n\nWe built up the result step by step through a series of concatenation steps. Every call to the HOF will return a string with one additional character added to the previous accumulator.\n\n## Know Your Return Type\nI think one of the reasons `map` and `filter` are easier than `reduce` is because they always return an array. That's not the case for `reduce`. `reduce` is ultimately designed to transform one type into another.\n\nWhen you begin writing a `reduce` statement, your first question to yourself should be \"What type am I returning?\". Here's a hint, it's probably one of the following: `String`, `Number`, `Object`, `Array`. Once you know what type you will return you can begin writing your reduce statement. I say this because the initial accumulator (reduce's second argument) can easily be determined by what type you expect `reduce` to return. Here is a table to guide your decision.\n\n| return type | initial accumulator |\n|:-----------:|:--------------------:|\n| string |  \"\" |\n| number | 0 |\n| object | {} |\n| array | [] |\n\n**Notice that the initial accumulator and the return type are always the same!**\n\n_You probably won't want return an array from a reduce function very often because chances are you could have more simply used a combination of `map`, `filter`, and/or some other function instead._\n\n## Summing Odd Numbers\n\nSo let's say we want to reduce over a set of numbers and return the sum of all the odd numbers. Since we are returning a `number`, we can use the above table to determine that the starting accumulator should be 0. That gives us a pretty decent starting point!\n\n```js\nconst arrToSum = [1, 2, 3, 4, 5];\narrToSum.reduce(function(currentSum, nextNumber) {\n\n}, 0)  // This is the second argument to reduce!\n```\n\nNow we just need to complete the guts of the function such that it increments the sum when the `nextNumber` is odd. If `nextNumber` is even, it should return `currentSum` unchanged.\n\n```js\nconst arrToSum = [1, 2, 3, 4, 5];\narrToSum.reduce(function(currentSum, nextNumber) {\n  // nextNumber is odd\n  if (nextNumber % 2 === 1) {\n    return currentSum + nextNumber\n  // nextNumber is even\n  } else {\n    return currentSum\n  }\n}, 0)\n```\n\nOne interesting thing to note about the above code is that EVERY code path (every if-else block) returns something. This is a very common theme throughout the functional programming paradigm and holds true when writing reduce's higher order function. In general it's a decent litmus test to establish if you've made a bug somewhere. **More specifically, if every code path does not return something, and if that something is not the same type as your accumulator, you probably have a bug.**\n\n## Frequency Counter\n\nLet's do something a little different. Suppose I have an array of words, and I want to count how many times each word appears in the array. We can represent this information using an object that maps words in the array to a number indicating their frequency of occurrence.\n\n```js\n// We will input something like this\n['luke', 'anakin', 'chewy', 'luke', 'chewy', 'princess', 'leia', 'chewy'] ->\n\n// And output something like this\n{\n  luke: 2,\n  anakin: 1,\n  princess: 1,\n  chewy: 3,\n  leia: 1\n}\n```\n\nConsulting the table above, we know that the starting accumulator needs to be an empty object. This means that each iteration over the array returns an object as well.\n\n```js\nconst frequencyArray = ['luke', 'anakin', 'chewy', 'luke', 'chewy', 'princess', 'leia', 'chewy'];\nfrequencyArray.reduce(function(resultantObject, nextWord) {\n\n}, {})\n```\n\nEvery step of the higher order function needs to examine the next word and determine if it has been seen before. If it has not, then we need to add an entry to the accumulator and return it. If it has, then we need to increment the existing entry inside the accumulator object and return it.\n\n```js\nfrequencyArray.reduce(function(resultantObject, nextWord) {\n  // If the word is not in the object\n  if (!resultantObject.hasOwnProperty(nextWord)) {\n    resultantObject[nextWord] = 1 //Set it to 1 since we have seen the word before\n    return resultantObject\n  } else {\n    // Otherwise increment the counter for that word inside the resulting object\n    resultantObject[nextWord]++\n    return resultantObject\n  }\n}, {})\n```\n\nJust like the first example, every code path returns the type of the accumulator.\n\n## Merging Objects\nLet's end with something a little more complicated.\n\nSuppose we want to implement a merge function to combines several objects into a single object. We're going to implement something really similar to [Object.assign](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/assign). Here are some examples.\n\n```js\nmerge([ { a: 4, b: 3 }, { c:10 } ]) --> { a: 4, b: 3, c: 10 }\n\nmerge([ { a: 4, b: 3 }, { c:4 }, {f: 7} ]) --> { a: 4, b: 3, c: 4, f: 7}\n\n// Duplicate keys take the value of the latter object\nmerge([ { a: 4, b: 3 }, { c:4 }, {a: 7} ]) --> { a: 7, b: 3, c: 4}\n```\n\nWe know that the return value from this operation is an object, so that will be our starting accumulator. Every iteration of the array will produce a new object that contains the merging of the previous accumulator and the next object.\n\n```js\n// This merge function will take in two objects and output a new object\n// containing both the keys and values from the two input objects.\nfunction mergeTwo(obj1, obj2) {\n  const newObj = {}\n  for (let key in obj1) {\n    newObj[key] = obj1[key]\n  }\n  for (let key in obj2) {\n    newObj[key] = obj2[key]\n  }\n  return newObj\n}\n\nfunction merge(arr) {\n  return arr.reduce(function(resultantObj, nextObj) {\n    return mergeTwo(resultantObj, nextObj)\n  }, {})\n}\n```\n\n## Summary\n\nTo summarize\n\n1. Always start by asking what the type of your output is. The type of your accumulator will follow by using the mapping table.\n3. The logic inside your higher order function should assemble your resulting accumulator piecemeal. Since the return value of the previous iteration is the input to the next iteration, every code path must return a value\n3. Furthermore, unless you want a horrible experience, every code path MUST return a value whose type is the same as that of your initial accumulator.\n\n\n## Conclusion\nAt the end of the day, just about everything you can do with reduce can be done with some combination of `map` and `fitler` and perhaps another [functional method](https://lodash.com/docs/4.17.4]). And the alternative solution is almost always simpler and more readable. So, in practice, you probably don't want to use `reduce` that often. With that said, `reduce` is a building block on which every other functional method can be built, and we will explore this unique trait in my next blog post.",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/using-reduce.md"
    },
    "d0cf29a8-abdd-58fe-8c0b-770f6384a439": {
      "id": "d0cf29a8-abdd-58fe-8c0b-770f6384a439",
      "children": [],
      "parent": "1f776510-745f-566f-a097-cfb98ce26bf3",
      "internal": {
        "content": "\nI've conducted about 100 technical interviews over the past 6 months for a software development recruiting company called [Triplebyte](https://triplebyte.com/iv/RrzqKKw/bp). I've also been doing consulting work, which has required me to take numerous technical interviews. It's interesting contrasting the experiences to identify what works and what doesn't.\n\nEvery Triplebyte interview begins with the candidate coding a short game. We have a series of steps, and each step precisely defines simple requirements for the program to handle. I can generally tell a couple minutes into the 2 hour interview whether the candidate will be successful. There are certainly outliers (as well as mechanisms to prevent bias), but in general, I can quickly ascertain how well a candidate stacks up technically. So then, it begs the question, why is it so hard to hire engineers? The answer is, most of us are doing it wrong.\n\n<!-- more -->\n\n## Interview Standardization\n\nAbout 12 months ago, while running [Fullstack Academy's](https://www.fullstackacademy.com/) Chicago campus, I was looking to hire a qualified instructor. Our interview consisted of live lecture as well as a mock 1-1 teaching session, where the candidate was required to guide me (playing the role of a struggling student) through a simple recursion problem. The latter exercise was almost always a better indicator of instructor fit than the former. The reason was simple: the latter exercise was assigned to the candidate whereas the lecture topic was their choice. This meant that every lecture topic was different and made lectures difficult to compare. In contrast, after having seen dozens of candidates attempt to teach me the same recursion problem, it was obvious who stood out.\n\nThe first mistake that companies make is that they fail to standardize the interview across candidates. As interviewers, it's tempting to say, \"let's have the candidate pair with us, that way we can see how they perform on real day-to-day challenges.\" Or another fault I've seen committed: give some candidates one problem and other candidates a different problem. This way a company can protect against its problems getting leaked online, right?\n\nThe issue is, as an interviewer, unless you have witnessed a dozen candidates attempt a given problem, it's very challenging to assess their abilities. It's easy to fall victim to the \"I solved it, why can't they\" bias. You need to compare candidates to each other to assess who did a good job and who didn't. Which brings me to my next point.\n\n## Objective Scoring\n\nYou need to compare candidates objectively, scoring them on every exercise across a variety of factors. There must be well-defined criteria to differentiate scores, and scoring should be done immediately upon completion of the exercise.\n\nSuppose our interviewer tasks the candidate to construct a simple web application. You assign a score of 1 through 4 to the following factors:\n\n1.  Productivity\n2.  Programming Style\n3.  Language/Framework Familiarity\n4.  Debugging Ability\n\nBeforehand, you would need to write a short description of what qualifies each score for each of the above factors. Then, during the interview, you will need a rubric to help you determine what score a candidate earns for each factor. Productivity can be measured exceptionally objectively if you have the candidate follow a prescribed series of steps to solve the programming challenge. For example, at Triplebyte, we literally tell them (loosely) what functions to write and in what order.\n\nAfter a week or two of interviewing a half dozen candidates, this scoring becomes critical and allows you avoid biases such as the [Serial-position effect](https://en.wikipedia.org/wiki/Serial-position_effect).\n\n## Culture Fit\n\nSurprisingly, a lot of this advice can also be applied to assessing culture fit. You can have the same person, ask the same questions, in the same order, using the same rubric to compare every answer. The risk is that you will inadvertently dehumanize the candidate, but it is possible to do this objective assessment such that the candidate is unaware of it. At the very least, you need to document your impression of the candidate immediately after the interview, lest you rely on your memory/feelings a week later when making a decision.\n\nYou could take this philosophy to the absurd, and it would certainly eliminate bias, though I personally believe there's a point of diminishing returns, at which you degrade the experience of the interviewee. Nevertheless, some standardization across interviews is critical to avoid implicit bias and identify good candidates, and I encourage every tech company to examine their interview process to assess their level of standardization.\n",
        "type": "MarkdownRemark",
        "contentDigest": "00f8e0fe527c06e8aae3932fc2421380",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "You're Hiring Programmers Wrong: A Case for Interview Standardization",
        "categories": [
          [
            "Technical Hiring"
          ]
        ],
        "date": "2018-03-20T17:03:01.000Z",
        "_PARENT": "1f776510-745f-566f-a097-cfb98ce26bf3"
      },
      "excerpt": "\nI've conducted about 100 technical interviews over the past 6 months for a software development recruiting company called [Triplebyte](https://triplebyte.com/iv/RrzqKKw/bp). I've also been doing consulting work, which has required me to take numerous technical interviews. It's interesting contrasting the experiences to identify what works and what doesn't.\n\nEvery Triplebyte interview begins with the candidate coding a short game. We have a series of steps, and each step precisely defines simple requirements for the program to handle. I can generally tell a couple minutes into the 2 hour interview whether the candidate will be successful. There are certainly outliers (as well as mechanisms to prevent bias), but in general, I can quickly ascertain how well a candidate stacks up technically. So then, it begs the question, why is it so hard to hire engineers? The answer is, most of us are doing it wrong.\n\n",
      "rawMarkdownBody": "\nI've conducted about 100 technical interviews over the past 6 months for a software development recruiting company called [Triplebyte](https://triplebyte.com/iv/RrzqKKw/bp). I've also been doing consulting work, which has required me to take numerous technical interviews. It's interesting contrasting the experiences to identify what works and what doesn't.\n\nEvery Triplebyte interview begins with the candidate coding a short game. We have a series of steps, and each step precisely defines simple requirements for the program to handle. I can generally tell a couple minutes into the 2 hour interview whether the candidate will be successful. There are certainly outliers (as well as mechanisms to prevent bias), but in general, I can quickly ascertain how well a candidate stacks up technically. So then, it begs the question, why is it so hard to hire engineers? The answer is, most of us are doing it wrong.\n\n<!-- more -->\n\n## Interview Standardization\n\nAbout 12 months ago, while running [Fullstack Academy's](https://www.fullstackacademy.com/) Chicago campus, I was looking to hire a qualified instructor. Our interview consisted of live lecture as well as a mock 1-1 teaching session, where the candidate was required to guide me (playing the role of a struggling student) through a simple recursion problem. The latter exercise was almost always a better indicator of instructor fit than the former. The reason was simple: the latter exercise was assigned to the candidate whereas the lecture topic was their choice. This meant that every lecture topic was different and made lectures difficult to compare. In contrast, after having seen dozens of candidates attempt to teach me the same recursion problem, it was obvious who stood out.\n\nThe first mistake that companies make is that they fail to standardize the interview across candidates. As interviewers, it's tempting to say, \"let's have the candidate pair with us, that way we can see how they perform on real day-to-day challenges.\" Or another fault I've seen committed: give some candidates one problem and other candidates a different problem. This way a company can protect against its problems getting leaked online, right?\n\nThe issue is, as an interviewer, unless you have witnessed a dozen candidates attempt a given problem, it's very challenging to assess their abilities. It's easy to fall victim to the \"I solved it, why can't they\" bias. You need to compare candidates to each other to assess who did a good job and who didn't. Which brings me to my next point.\n\n## Objective Scoring\n\nYou need to compare candidates objectively, scoring them on every exercise across a variety of factors. There must be well-defined criteria to differentiate scores, and scoring should be done immediately upon completion of the exercise.\n\nSuppose our interviewer tasks the candidate to construct a simple web application. You assign a score of 1 through 4 to the following factors:\n\n1.  Productivity\n2.  Programming Style\n3.  Language/Framework Familiarity\n4.  Debugging Ability\n\nBeforehand, you would need to write a short description of what qualifies each score for each of the above factors. Then, during the interview, you will need a rubric to help you determine what score a candidate earns for each factor. Productivity can be measured exceptionally objectively if you have the candidate follow a prescribed series of steps to solve the programming challenge. For example, at Triplebyte, we literally tell them (loosely) what functions to write and in what order.\n\nAfter a week or two of interviewing a half dozen candidates, this scoring becomes critical and allows you avoid biases such as the [Serial-position effect](https://en.wikipedia.org/wiki/Serial-position_effect).\n\n## Culture Fit\n\nSurprisingly, a lot of this advice can also be applied to assessing culture fit. You can have the same person, ask the same questions, in the same order, using the same rubric to compare every answer. The risk is that you will inadvertently dehumanize the candidate, but it is possible to do this objective assessment such that the candidate is unaware of it. At the very least, you need to document your impression of the candidate immediately after the interview, lest you rely on your memory/feelings a week later when making a decision.\n\nYou could take this philosophy to the absurd, and it would certainly eliminate bias, though I personally believe there's a point of diminishing returns, at which you degrade the experience of the interviewee. Nevertheless, some standardization across interviews is critical to avoid implicit bias and identify good candidates, and I encourage every tech company to examine their interview process to assess their level of standardization.\n",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/you're-hiring-programmers-wrong-a-case-for-interview-standardization.md"
    },
    "a9b06f31-6aa6-5fc0-928d-666b4b718c09": {
      "id": "a9b06f31-6aa6-5fc0-928d-666b4b718c09",
      "children": [],
      "parent": "c268b9c5-75fe-5248-b049-abd4c29e1488",
      "internal": {
        "content": "\n_My inspiration for this blog post came from [this video](https://www.youtube.com/watch?v=VJ38wSFbM3A) where Dan Abramov walks through the source code to react-redux_\n\nAs frontend web developers, it's not uncommon that we follow well-specified patterns - often blindly. The frontend landscape is changing rapidly, and sometimes there isn't time to investigate why we use a specific pattern; we just know we should.\n\nOne widely used pattern in [react-redux](https://github.com/reactjs/react-redux) applications looks like this\n\n```js\nconnect(mapStateToProps, mapDispatchToProps)(MyComponent)\n```\n\nI'll assume you know how to implement this pattern, but why do we use it and how does it work under the hood?\n\n<!-- more -->\n\n# Why Do we Need React-Redux?\nReact and Redux are two completely independent tools that have nothing to do with each other. React is a tool for creating user interfaces in the browser. Redux is a tool for managing state. Either tool can be used without the other. We often use them together because they both solve separate but very important and closely related problems. The purpose of react-redux is to get these two tools to talk.\n\nBut first, what would we do without react-redux? How would React and Redux talk?\n\n# How to Integrate React and Redux Without react-redux\nMore precisely, how do we ensure that a React component re-renders when the Redux store changes? The answer lies in Redux's [subscribe](http://redux.js.org/docs/api/Store.html#subscribe) API.\n\n```js\nimport store from './store'\nimport { Component } from 'react'\n\nclass MyComponent extends Component {\n  constructor() {\n    super();\n    // One solution is to make each component\n    // store the entirety of the redux state.\n    this.state = { storeState: store.getState() };\n  }\n\n  componentDidMount() {\n    // Callbacks passed to store.subscribe will be\n    // invoked every time the store's state changes.\n    // Our callback can get the state of the\n    // store and add it to the component's local state.\n    this.unsubscribe = store.subscribe(() => {\n      this.setState({ storeState: store.getState() });\n    });\n  }\n\n  // We need to make sure that we don't accidentally\n  // subscribe to the store multiple times in the case\n  // where a component mounts, unmounts, and then mounts a second time.\n  // Fortunately, Redux makes this easy by returning\n  // an unsubscribe function when store.subscribe is invoked.\n  componentWillUnmount() {\n    this.unsubscribe();\n  }\n}\n```\n\nIf we insert the above boilerplate into every one of our React component's, then every component could have access to the store and would be informed through a subscription the moment the store's state changes. This configuration has three  flaws.\n\n1. The boilerplate of subscribing and unsubscribing to the store is highly error prone and unnecessarily verbose.\n2. All of our React component's are dependent upon knowledge of the Redux store. This is a complete failure of [separation of concerns](https://en.wikipedia.org/wiki/Separation_of_concerns).\n3. Every component is dependent upon the entirety of the store's state tree. This means that whenever an action is dispatched, `setState` is called on every mounted component, causing each one to re-render, regardless of whether its render function depends on the store state that changed. Woah! Let that sink in for a moment.\n\nLet's write a rudimentary implementation of connect that resolves the first problem.\n\n# Understanding The Syntax of Connect\nTypically, we invoke `connect` like this:\n\n```js\nconnect(mapStateToProps, mapDispatchToProps)(WrappedComponent);\n```\n\n`connect` takes in two functions as arguments and returns a function. Yes, you heard me, `connect` returns a function, not a component. Suppose I invoke `connect` and neglect to pass in a component.\n\n```js\nconst connectFunc = connect(mapStateToProps, mapDispatchToProps);\nconst connctedComponent = connectFunc(WrappedComponent);\n```\n\n `connect` will return to me a function. It's that function that takes in my component (`connect` is implemented this way as opposed to simply taking in 3 arguments to support decorator syntax. The Dan Abramov video I linked above explains this.)\n\nThus, the very first few lines of `connect` must look like this:\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n\n  };\n}\n```\n\n# Higher Order Components\n\nAnd what does the function we returned above do? This function is implemented as a [higher order component](https://reactjs.org/docs/higher-order-components.html) (HOC). A HOC is a function that takes in a component as a parameter and returns a new component. The new component is generally a modified or augmented version of the original component.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    // We are returning a brand new component.\n    // Note that this new component does\n    // not inherit from WrappedComponent.\n    return class WrapperComponent extends Component {\n      // All we are doing is returning a new component\n      // that renders our original component.\n      render() {\n        // Notice that we need to pass WrappedComponent\n        // WrapperComponent's props.\n        // If we didn't do this, then WrappedComponent\n        // would never have access to any props.\n        return <WrappedComponent {...this.props} />;\n      }\n    };\n  };\n}\n```\n\nIf we were to run the above `connect` function on a component, the connected component would behave identically to original component. Furthermore, we could nest `connect` as many times as we want\n\n```js\nconnect(null, null)(connect(null, null)(App))\n```\n\nand still never distort the behavior of the original component. Our current implementation is effectively [idempotent](https://stackoverflow.com/questions/1077412/what-is-an-idempotent-operation).\n\n# Eliminating Boilerplate\n\nOur next step is to eliminate some of the boilerplate code. We don't want to have to subscribe to the store every time we create a new component, so let's have our new `connect` function do it instead.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        // Since the whole point of this HOC is to get WrappedComponent\n        // access to the store, we need to pass that state down as props.\n        const storeState = this.state.storeState;\n        return <WrappedComponent {...this.props} {...storeState} />;\n      }\n    };\n  };\n}\n```\n\nWe just made huge progress! Now, whenever we invoke\n\n```js\nconnect(null, null)(MyComponent)\n```\n\nwe get a component that is subscribed to state changes on the store, and this state will be passed down to our component as props.\n\n# Implementing Support for mapStateToProps\n\nOur connected components still all depend on the entirety of the store's state tree. Look up above, the entire state is passed down as props to every connected component. To reiterate, this means that if any piece of the store's state is updated, our component will re-render.\n\nThis is where `mapStateToProps` comes to the rescue. `mapStateToProps` takes as its argument the store's state, and it allows us to return the particular pieces of the store's state that a component depends on. It then passes that state as props to our component instead.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        // Now, instead of passing down all of the store state,\n        // we only pass down the subset of state return from\n        // mapStateToProps\n        const storeProps = mapStateToProps(this.state.storeState);\n        return <WrappedComponent {...this.props} {...storeProps} />;\n      }\n    };\n  };\n}\n```\n\nAll we did was insert a call to `mapStateToProps`, allowing us to make each connected component dependent upon only the state it cares about, as defined by the return value of `mapStateToProps`. `mapStateToProps` is a wonderful form of explicit documentation, clearly stating the slices of the state tree each component depends on. Unfortunately, our change does not fix the efficiency problems noted above. More on that below.\n\n# mapStateToProps and ownProps\n\nAn astute reader might note that `mapStateToProps` actually takes two arguments: the first is a copy of the store's state, and the second are the props that are originally passed down to `WrapperComponent`. `react-redux` does not pass these down to the wrapped component by default as we do in the example immediately above. Let's modify our implementation to mirror `react-redux`.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        const newProps = mapStateToProps(this.state.storeState, this.props);\n        return <WrappedComponent {...newProps} />;\n      }\n    };\n  };\n}\n```\n\nNow the implementer of `mapStateToProps` can choose which of `WrapperComponent`'s props it would like to keep and which it would like to disregard.\n\n# What's the Point of mapDispatchToProps?\n\n`mapDispatchToProps` is designed to eliminate React's dependency upon Redux. If we were to use the above implementation of `connect`, every component that dispatch's an action must import `store.dispatch`, and the implementation would look like this:\n\n```js\nimport store from \"./store\";\nimport { Component } from \"react\";\nimport { updateThing } from \"./store/actions\";\n\nclass ExampleComponent extends Component {\n  handleChange(e) {\n    store.dispatch(updateThing(e.target));\n  }\n}\n```\n\nThe above component 'knows' that it is part of a Redux application because it is explicitly referencing the store to dispatch actions. But we should always try to minimize the interaction of different pieces of architecture, esspecially when they have no need to interact. Ultimately, React components should not been intertwined with Redux code!\n\n## Implementing Support for mapDispatchToProps\n\n`connect` resolves this problem for us by injecting the `store.dispatch` dependency into `mapDispatchToProps`, allowing us to explicitly define functions that dispatch actions without requiring that our [presentation components](https://medium.com/@dan_abramov/smart-and-dumb-components-7ca2f9a7c7d0) have a dependency on the store. Just as the return value of `mapStateToProps` is passed down to `WrappedComponent`, the return value of `mapDispatchToProps` will be passed down as well.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        // Now we merge the results from mapStateToProps\n        // and mapDispatchToProps and pass everything down\n        const newProps = Object.assign(\n          {},\n          mapStateToProps(this.state.storeState, this.props),\n          // If you aren't intimately familiar with the this keyword,\n          // it's okay if you don't understand why we use bind here\n          mapDispatchToProps(store.dispatch.bind(this))\n        );\n        return <WrappedComponent {...newProps} />;\n      }\n    };\n  };\n}\n```\n\n\n# More Efficiency Issues - Hello shouldComponentUpdate\n\nWe never actually fixed any of the performance issues noted above. The crux of the problem is that every time the store updates, `WrapperComponent` re-renders (because of its Redux store subscription that calls `setState`) and that means `WrappedComponent` re-renders. This [re-rendering](/leveraging-immutability-in-react) happens despite the fact that `WrappedComponent`'s props might be unchanged between two invocations of `setState`. In fact, this scenario is highly probable and will occur whenever a piece of state in the store changes that your component does not depend on (aka, a piece of store state not returned from from `mapStateToProps`).\n\nReact has a handy lifecycle method called [`shouldComponentUpdate`](https://reactjs.org/docs/react-component.html#shouldcomponentupdate) that allows us to return a boolean that indicates whether a component should re-render. In essence, if we implement this method on `WrapperComponent` and it returns `false`, then React will not re-render `WrapperComponent`. And it follows that `WrappedComponent` won't re-render either.\n\nSo, in the above scenario, when `WrapperComponent` calls `setState`, React first calls the `shouldComponentUpdate` method to see if a re-render should actually happen. Let's implement it below.\n\n```js\n// Just a simple shallow equality function\nimport shallowEqual from \"shallow-equal/objects\"\n\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      shouldComponentUpdate() {\n        // If the props to WrapperComponent do not change\n        // between setState calls, then we don't need to re-render.\n        // On the previous re-render, we cached the results of\n        // mapStateToProps. That's what this.oldProps is.\n        const newProps = mapStateToProps(this.state.storeState, this.props);\n        return !shallowEqual(newProps, this.oldProps);\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        // We need to hang onto the previous result of\n        // mapStateToProps to use the next time\n        // shouldComponentUpdate runs\n        this.oldProps = mapStateToProps(this.state.storeState, this.props)\n        const newProps = Object.assign(\n          {},\n          this.oldProps,\n          mapDispatchToProps(store.dispatch.bind(this))\n        );\n        return <WrappedComponent {...newProps} />;\n      }\n    };\n  };\n}\n```\nI've created a demo [here](https://codesandbox.io/s/o43p70k66). Open the console and prove to yourself that `shouldComponentUpdate` is doing its job.\n\n_I should note that this is not exactly what react-redux does because of edge cases, but the concept is still the same._\n\nNow our wrapper and wrapped components will only re-render when the props returned from `mapStateToProps` change! This is a huge performance gain. This implementation of `connect` explains why adherence to [immutability is so important](http://redux.js.org/docs/faq/ReactRedux.html#react-not-rerendering) in redux's reducers. If you fail to respect immutability, the shallow comparison in the `shouldComponentUpdate` in `WrapperComponent` will likely return `false`, causing your connected component to not re-render when it should.\n\n# Wrapping up\n\nReact-redux's `connect` method is remarkably simple and only performs a handful of operations.\n\n1. It manages our component's subscription to the store so that our component can update when the store updates.\n2. It allows us to explicitly define the slice of state our component is dependent upon using `mapStateToProps`.\n3. It gives our component access to `store.dispatch` without requiring a direct dependency on the store.\n4. It defines `shouldComponentUpdate`, ensuring that our components only re-render when the store state they depend on changes.\n\nI hope you found this article helpful. Please feel free to email me and reach out if you have questions. I put a [gist](https://gist.github.com/nadrane/5221c64c421efe421bda9fdaab167dc2) online containing the same code as the demo.",
        "type": "MarkdownRemark",
        "contentDigest": "6abdcf2affced80723b68540b1f458d0",
        "owner": "gatsby-transformer-remark"
      },
      "frontmatter": {
        "title": "Write Your Own React-Redux Connect",
        "date": "2017-09-29T13:14:18.000Z",
        "categories": [
          [
            "React"
          ],
          [
            "Redux"
          ],
          [
            "Build Your Own"
          ]
        ],
        "_PARENT": "c268b9c5-75fe-5248-b049-abd4c29e1488"
      },
      "excerpt": "\n_My inspiration for this blog post came from [this video](https://www.youtube.com/watch?v=VJ38wSFbM3A) where Dan Abramov walks through the source code to react-redux_\n\nAs frontend web developers, it's not uncommon that we follow well-specified patterns - often blindly. The frontend landscape is changing rapidly, and sometimes there isn't time to investigate why we use a specific pattern; we just know we should.\n\nOne widely used pattern in [react-redux](https://github.com/reactjs/react-redux) applications looks like this\n\n```js\nconnect(mapStateToProps, mapDispatchToProps)(MyComponent)\n```\n\nI'll assume you know how to implement this pattern, but why do we use it and how does it work under the hood?\n\n",
      "rawMarkdownBody": "\n_My inspiration for this blog post came from [this video](https://www.youtube.com/watch?v=VJ38wSFbM3A) where Dan Abramov walks through the source code to react-redux_\n\nAs frontend web developers, it's not uncommon that we follow well-specified patterns - often blindly. The frontend landscape is changing rapidly, and sometimes there isn't time to investigate why we use a specific pattern; we just know we should.\n\nOne widely used pattern in [react-redux](https://github.com/reactjs/react-redux) applications looks like this\n\n```js\nconnect(mapStateToProps, mapDispatchToProps)(MyComponent)\n```\n\nI'll assume you know how to implement this pattern, but why do we use it and how does it work under the hood?\n\n<!-- more -->\n\n# Why Do we Need React-Redux?\nReact and Redux are two completely independent tools that have nothing to do with each other. React is a tool for creating user interfaces in the browser. Redux is a tool for managing state. Either tool can be used without the other. We often use them together because they both solve separate but very important and closely related problems. The purpose of react-redux is to get these two tools to talk.\n\nBut first, what would we do without react-redux? How would React and Redux talk?\n\n# How to Integrate React and Redux Without react-redux\nMore precisely, how do we ensure that a React component re-renders when the Redux store changes? The answer lies in Redux's [subscribe](http://redux.js.org/docs/api/Store.html#subscribe) API.\n\n```js\nimport store from './store'\nimport { Component } from 'react'\n\nclass MyComponent extends Component {\n  constructor() {\n    super();\n    // One solution is to make each component\n    // store the entirety of the redux state.\n    this.state = { storeState: store.getState() };\n  }\n\n  componentDidMount() {\n    // Callbacks passed to store.subscribe will be\n    // invoked every time the store's state changes.\n    // Our callback can get the state of the\n    // store and add it to the component's local state.\n    this.unsubscribe = store.subscribe(() => {\n      this.setState({ storeState: store.getState() });\n    });\n  }\n\n  // We need to make sure that we don't accidentally\n  // subscribe to the store multiple times in the case\n  // where a component mounts, unmounts, and then mounts a second time.\n  // Fortunately, Redux makes this easy by returning\n  // an unsubscribe function when store.subscribe is invoked.\n  componentWillUnmount() {\n    this.unsubscribe();\n  }\n}\n```\n\nIf we insert the above boilerplate into every one of our React component's, then every component could have access to the store and would be informed through a subscription the moment the store's state changes. This configuration has three  flaws.\n\n1. The boilerplate of subscribing and unsubscribing to the store is highly error prone and unnecessarily verbose.\n2. All of our React component's are dependent upon knowledge of the Redux store. This is a complete failure of [separation of concerns](https://en.wikipedia.org/wiki/Separation_of_concerns).\n3. Every component is dependent upon the entirety of the store's state tree. This means that whenever an action is dispatched, `setState` is called on every mounted component, causing each one to re-render, regardless of whether its render function depends on the store state that changed. Woah! Let that sink in for a moment.\n\nLet's write a rudimentary implementation of connect that resolves the first problem.\n\n# Understanding The Syntax of Connect\nTypically, we invoke `connect` like this:\n\n```js\nconnect(mapStateToProps, mapDispatchToProps)(WrappedComponent);\n```\n\n`connect` takes in two functions as arguments and returns a function. Yes, you heard me, `connect` returns a function, not a component. Suppose I invoke `connect` and neglect to pass in a component.\n\n```js\nconst connectFunc = connect(mapStateToProps, mapDispatchToProps);\nconst connctedComponent = connectFunc(WrappedComponent);\n```\n\n `connect` will return to me a function. It's that function that takes in my component (`connect` is implemented this way as opposed to simply taking in 3 arguments to support decorator syntax. The Dan Abramov video I linked above explains this.)\n\nThus, the very first few lines of `connect` must look like this:\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n\n  };\n}\n```\n\n# Higher Order Components\n\nAnd what does the function we returned above do? This function is implemented as a [higher order component](https://reactjs.org/docs/higher-order-components.html) (HOC). A HOC is a function that takes in a component as a parameter and returns a new component. The new component is generally a modified or augmented version of the original component.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    // We are returning a brand new component.\n    // Note that this new component does\n    // not inherit from WrappedComponent.\n    return class WrapperComponent extends Component {\n      // All we are doing is returning a new component\n      // that renders our original component.\n      render() {\n        // Notice that we need to pass WrappedComponent\n        // WrapperComponent's props.\n        // If we didn't do this, then WrappedComponent\n        // would never have access to any props.\n        return <WrappedComponent {...this.props} />;\n      }\n    };\n  };\n}\n```\n\nIf we were to run the above `connect` function on a component, the connected component would behave identically to original component. Furthermore, we could nest `connect` as many times as we want\n\n```js\nconnect(null, null)(connect(null, null)(App))\n```\n\nand still never distort the behavior of the original component. Our current implementation is effectively [idempotent](https://stackoverflow.com/questions/1077412/what-is-an-idempotent-operation).\n\n# Eliminating Boilerplate\n\nOur next step is to eliminate some of the boilerplate code. We don't want to have to subscribe to the store every time we create a new component, so let's have our new `connect` function do it instead.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        // Since the whole point of this HOC is to get WrappedComponent\n        // access to the store, we need to pass that state down as props.\n        const storeState = this.state.storeState;\n        return <WrappedComponent {...this.props} {...storeState} />;\n      }\n    };\n  };\n}\n```\n\nWe just made huge progress! Now, whenever we invoke\n\n```js\nconnect(null, null)(MyComponent)\n```\n\nwe get a component that is subscribed to state changes on the store, and this state will be passed down to our component as props.\n\n# Implementing Support for mapStateToProps\n\nOur connected components still all depend on the entirety of the store's state tree. Look up above, the entire state is passed down as props to every connected component. To reiterate, this means that if any piece of the store's state is updated, our component will re-render.\n\nThis is where `mapStateToProps` comes to the rescue. `mapStateToProps` takes as its argument the store's state, and it allows us to return the particular pieces of the store's state that a component depends on. It then passes that state as props to our component instead.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        // Now, instead of passing down all of the store state,\n        // we only pass down the subset of state return from\n        // mapStateToProps\n        const storeProps = mapStateToProps(this.state.storeState);\n        return <WrappedComponent {...this.props} {...storeProps} />;\n      }\n    };\n  };\n}\n```\n\nAll we did was insert a call to `mapStateToProps`, allowing us to make each connected component dependent upon only the state it cares about, as defined by the return value of `mapStateToProps`. `mapStateToProps` is a wonderful form of explicit documentation, clearly stating the slices of the state tree each component depends on. Unfortunately, our change does not fix the efficiency problems noted above. More on that below.\n\n# mapStateToProps and ownProps\n\nAn astute reader might note that `mapStateToProps` actually takes two arguments: the first is a copy of the store's state, and the second are the props that are originally passed down to `WrapperComponent`. `react-redux` does not pass these down to the wrapped component by default as we do in the example immediately above. Let's modify our implementation to mirror `react-redux`.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        const newProps = mapStateToProps(this.state.storeState, this.props);\n        return <WrappedComponent {...newProps} />;\n      }\n    };\n  };\n}\n```\n\nNow the implementer of `mapStateToProps` can choose which of `WrapperComponent`'s props it would like to keep and which it would like to disregard.\n\n# What's the Point of mapDispatchToProps?\n\n`mapDispatchToProps` is designed to eliminate React's dependency upon Redux. If we were to use the above implementation of `connect`, every component that dispatch's an action must import `store.dispatch`, and the implementation would look like this:\n\n```js\nimport store from \"./store\";\nimport { Component } from \"react\";\nimport { updateThing } from \"./store/actions\";\n\nclass ExampleComponent extends Component {\n  handleChange(e) {\n    store.dispatch(updateThing(e.target));\n  }\n}\n```\n\nThe above component 'knows' that it is part of a Redux application because it is explicitly referencing the store to dispatch actions. But we should always try to minimize the interaction of different pieces of architecture, esspecially when they have no need to interact. Ultimately, React components should not been intertwined with Redux code!\n\n## Implementing Support for mapDispatchToProps\n\n`connect` resolves this problem for us by injecting the `store.dispatch` dependency into `mapDispatchToProps`, allowing us to explicitly define functions that dispatch actions without requiring that our [presentation components](https://medium.com/@dan_abramov/smart-and-dumb-components-7ca2f9a7c7d0) have a dependency on the store. Just as the return value of `mapStateToProps` is passed down to `WrappedComponent`, the return value of `mapDispatchToProps` will be passed down as well.\n\n```js\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        // Now we merge the results from mapStateToProps\n        // and mapDispatchToProps and pass everything down\n        const newProps = Object.assign(\n          {},\n          mapStateToProps(this.state.storeState, this.props),\n          // If you aren't intimately familiar with the this keyword,\n          // it's okay if you don't understand why we use bind here\n          mapDispatchToProps(store.dispatch.bind(this))\n        );\n        return <WrappedComponent {...newProps} />;\n      }\n    };\n  };\n}\n```\n\n\n# More Efficiency Issues - Hello shouldComponentUpdate\n\nWe never actually fixed any of the performance issues noted above. The crux of the problem is that every time the store updates, `WrapperComponent` re-renders (because of its Redux store subscription that calls `setState`) and that means `WrappedComponent` re-renders. This [re-rendering](/leveraging-immutability-in-react) happens despite the fact that `WrappedComponent`'s props might be unchanged between two invocations of `setState`. In fact, this scenario is highly probable and will occur whenever a piece of state in the store changes that your component does not depend on (aka, a piece of store state not returned from from `mapStateToProps`).\n\nReact has a handy lifecycle method called [`shouldComponentUpdate`](https://reactjs.org/docs/react-component.html#shouldcomponentupdate) that allows us to return a boolean that indicates whether a component should re-render. In essence, if we implement this method on `WrapperComponent` and it returns `false`, then React will not re-render `WrapperComponent`. And it follows that `WrappedComponent` won't re-render either.\n\nSo, in the above scenario, when `WrapperComponent` calls `setState`, React first calls the `shouldComponentUpdate` method to see if a re-render should actually happen. Let's implement it below.\n\n```js\n// Just a simple shallow equality function\nimport shallowEqual from \"shallow-equal/objects\"\n\nfunction connect(mapStateToProps, mapDispatchToProps) {\n  return function(WrappedComponent) {\n    return class WrapperComponent extends Component {\n      constructor() {\n        super();\n        this.state = { storeState: store.getState() };\n      }\n\n      shouldComponentUpdate() {\n        // If the props to WrapperComponent do not change\n        // between setState calls, then we don't need to re-render.\n        // On the previous re-render, we cached the results of\n        // mapStateToProps. That's what this.oldProps is.\n        const newProps = mapStateToProps(this.state.storeState, this.props);\n        return !shallowEqual(newProps, this.oldProps);\n      }\n\n      componentDidMount() {\n        this.unsubscribe = store.subscribe(() => {\n          this.setState({ storeState: store.getState() });\n        });\n      }\n\n      componentWillUnmount() {\n        this.unsubscribe();\n      }\n\n      render() {\n        // We need to hang onto the previous result of\n        // mapStateToProps to use the next time\n        // shouldComponentUpdate runs\n        this.oldProps = mapStateToProps(this.state.storeState, this.props)\n        const newProps = Object.assign(\n          {},\n          this.oldProps,\n          mapDispatchToProps(store.dispatch.bind(this))\n        );\n        return <WrappedComponent {...newProps} />;\n      }\n    };\n  };\n}\n```\nI've created a demo [here](https://codesandbox.io/s/o43p70k66). Open the console and prove to yourself that `shouldComponentUpdate` is doing its job.\n\n_I should note that this is not exactly what react-redux does because of edge cases, but the concept is still the same._\n\nNow our wrapper and wrapped components will only re-render when the props returned from `mapStateToProps` change! This is a huge performance gain. This implementation of `connect` explains why adherence to [immutability is so important](http://redux.js.org/docs/faq/ReactRedux.html#react-not-rerendering) in redux's reducers. If you fail to respect immutability, the shallow comparison in the `shouldComponentUpdate` in `WrapperComponent` will likely return `false`, causing your connected component to not re-render when it should.\n\n# Wrapping up\n\nReact-redux's `connect` method is remarkably simple and only performs a handful of operations.\n\n1. It manages our component's subscription to the store so that our component can update when the store updates.\n2. It allows us to explicitly define the slice of state our component is dependent upon using `mapStateToProps`.\n3. It gives our component access to `store.dispatch` without requiring a direct dependency on the store.\n4. It defines `shouldComponentUpdate`, ensuring that our components only re-render when the store state they depend on changes.\n\nI hope you found this article helpful. Please feel free to email me and reach out if you have questions. I put a [gist](https://gist.github.com/nadrane/5221c64c421efe421bda9fdaab167dc2) online containing the same code as the demo.",
      "fileAbsolutePath": "/Users/nickdrane/projects/blog/content/_posts/write-your-own-redux-connect.md"
    },
    "SitePage /using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres/": {
      "jsonName": "using-jq-to-effortlessly-ingest-newline-delimited-json-into-postgres-7b7",
      "internalComponentName": "ComponentUsingJqToEffortlesslyIngestNewlineDelimitedJsonIntoPostgres",
      "path": "/using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "4b07906cc8ba3d516483bc3a1d443e87",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /hidden-costs-of-postgresql-jsonb/": {
      "jsonName": "hidden-costs-of-postgresql-jsonb-6a0",
      "internalComponentName": "ComponentHiddenCostsOfPostgresqlJsonb",
      "path": "/hidden-costs-of-postgresql-jsonb/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "hidden-costs-of-postgresql-jsonb"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /hidden-costs-of-postgresql-jsonb/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "4a3a4cc15f0a98da53441bad337028af",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /ethical-engineering-for-the-average-engineer/": {
      "jsonName": "ethical-engineering-for-the-average-engineer-762",
      "internalComponentName": "ComponentEthicalEngineeringForTheAverageEngineer",
      "path": "/ethical-engineering-for-the-average-engineer/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "ethical-engineering-for-the-average-engineer"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /ethical-engineering-for-the-average-engineer/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "111d965e1c8b76e02560d8a3c2990497",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /build-your-own-nested-query-string-encoder/": {
      "jsonName": "build-your-own-nested-query-string-encoder-ac5",
      "internalComponentName": "ComponentBuildYourOwnNestedQueryStringEncoder",
      "path": "/build-your-own-nested-query-string-encoder/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "build-your-own-nested-query-string-encoder"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /build-your-own-nested-query-string-encoder/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "a7e9c58292553b6e4c002edbd8e54fb1",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /you're-hiring-programmers-wrong-a-case-for-interview-standardization/": {
      "jsonName": "youre-hiring-programmers-wrong-a-case-for-interview-standardization-3ae",
      "internalComponentName": "ComponentYoureHiringProgrammersWrongACaseForInterviewStandardization",
      "path": "/you're-hiring-programmers-wrong-a-case-for-interview-standardization/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "you're-hiring-programmers-wrong-a-case-for-interview-standardization"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /you're-hiring-programmers-wrong-a-case-for-interview-standardization/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "f35aa3887d5a907bebc7eb04f454323c",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /scraping-the-web-with-puppeteer-lessons-learned/": {
      "jsonName": "scraping-the-web-with-puppeteer-lessons-learned-6a6",
      "internalComponentName": "ComponentScrapingTheWebWithPuppeteerLessonsLearned",
      "path": "/scraping-the-web-with-puppeteer-lessons-learned/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "scraping-the-web-with-puppeteer-lessons-learned"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /scraping-the-web-with-puppeteer-lessons-learned/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "d4a144ba3b38c68c7caacfb248583c58",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /regex-and-automated-test-fuzzing/": {
      "jsonName": "regex-and-automated-test-fuzzing-d2c",
      "internalComponentName": "ComponentRegexAndAutomatedTestFuzzing",
      "path": "/regex-and-automated-test-fuzzing/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "regex-and-automated-test-fuzzing"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /regex-and-automated-test-fuzzing/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "c862732f4475eb197a88a901a393f444",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /build-your-own-regex/": {
      "jsonName": "build-your-own-regex-82a",
      "internalComponentName": "ComponentBuildYourOwnRegex",
      "path": "/build-your-own-regex/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "build-your-own-regex"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /build-your-own-regex/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "fa893b3e515123dcef6b1e57cb1a65dc",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /optimizing-elasticsearch-score/": {
      "jsonName": "optimizing-elasticsearch-score-ba9",
      "internalComponentName": "ComponentOptimizingElasticsearchScore",
      "path": "/optimizing-elasticsearch-score/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "optimizing-elasticsearch-score"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /optimizing-elasticsearch-score/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "7c0fe7c397ae69cfd7647b7a3b32d82f",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /write-your-own-redux-connect/": {
      "jsonName": "write-your-own-redux-connect-c78",
      "internalComponentName": "ComponentWriteYourOwnReduxConnect",
      "path": "/write-your-own-redux-connect/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "write-your-own-redux-connect"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /write-your-own-redux-connect/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "e24536b0778b996283c82e748c8a67d5",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /using-reduce/": {
      "jsonName": "using-reduce-627",
      "internalComponentName": "ComponentUsingReduce",
      "path": "/using-reduce/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "using-reduce"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /using-reduce/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "70991c092f1b5d670b81db819608cc0b",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /leveraging-immutability-in-react/": {
      "jsonName": "leveraging-immutability-in-react-278",
      "internalComponentName": "ComponentLeveragingImmutabilityInReact",
      "path": "/leveraging-immutability-in-react/",
      "component": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "componentChunkName": "component---src-templates-article-js",
      "context": {
        "slug": "leveraging-immutability-in-react"
      },
      "pluginCreator___NODE": "Plugin default-site-plugin",
      "pluginCreatorId": "Plugin default-site-plugin",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js",
      "id": "SitePage /leveraging-immutability-in-react/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "e700b71ad2921023ef717b0ab0c04345",
        "description": "Your site's \"gatsby-node.js\"",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /404/": {
      "jsonName": "404-22d",
      "internalComponentName": "Component404",
      "path": "/404/",
      "component": "/Users/nickdrane/projects/blog/src/pages/404.js",
      "componentChunkName": "component---src-pages-404-js",
      "context": {},
      "pluginCreator___NODE": "Plugin gatsby-plugin-page-creator",
      "pluginCreatorId": "Plugin gatsby-plugin-page-creator",
      "componentPath": "/Users/nickdrane/projects/blog/src/pages/404.js",
      "id": "SitePage /404/",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "ad84473a4539d31fc9fa6babda3d8836",
        "description": "Plugin gatsby-plugin-page-creator",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /": {
      "jsonName": "index",
      "internalComponentName": "ComponentIndex",
      "path": "/",
      "component": "/Users/nickdrane/projects/blog/src/pages/index.js",
      "componentChunkName": "component---src-pages-index-js",
      "context": {},
      "pluginCreator___NODE": "Plugin gatsby-plugin-page-creator",
      "pluginCreatorId": "Plugin gatsby-plugin-page-creator",
      "componentPath": "/Users/nickdrane/projects/blog/src/pages/index.js",
      "id": "SitePage /",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "e9ebafddba08b81e050aa70a2de84a3a",
        "description": "Plugin gatsby-plugin-page-creator",
        "owner": "internal-data-bridge"
      }
    },
    "SitePage /404.html": {
      "jsonName": "404-html-516",
      "internalComponentName": "Component404Html",
      "path": "/404.html",
      "component": "/Users/nickdrane/projects/blog/src/pages/404.js",
      "componentChunkName": "component---src-pages-404-js",
      "context": {},
      "pluginCreator___NODE": "Plugin prod-404",
      "pluginCreatorId": "Plugin prod-404",
      "componentPath": "/Users/nickdrane/projects/blog/src/pages/404.js",
      "id": "SitePage /404.html",
      "parent": "SOURCE",
      "children": [],
      "internal": {
        "type": "SitePage",
        "contentDigest": "9f02e9b570b789aec14019e8b9cab23f",
        "description": "Plugin prod-404",
        "owner": "internal-data-bridge"
      }
    }
  },
  "status": {
    "plugins": {},
    "PLUGINS_HASH": "813bedd3b03b21c1b9c5c37735188978"
  },
  "componentDataDependencies": {
    "nodes": {
      "Site": [
        "sq--src-components-layout-js"
      ],
      "86e7f107-140a-5cdd-bbf5-d54112d285b9": [
        "/using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres/"
      ],
      "4a464680-e690-5613-804d-8fcf3746b0d6": [
        "/hidden-costs-of-postgresql-jsonb/"
      ],
      "44b69257-3ece-526d-a8d9-25a8319c3e69": [
        "/ethical-engineering-for-the-average-engineer/"
      ],
      "6e5a1b24-f308-5104-8f33-b43745551d5f": [
        "/build-your-own-nested-query-string-encoder/"
      ],
      "d0cf29a8-abdd-58fe-8c0b-770f6384a439": [
        "/you're-hiring-programmers-wrong-a-case-for-interview-standardization/"
      ],
      "017704d5-98aa-5eef-8086-8c15e94510d0": [
        "/scraping-the-web-with-puppeteer-lessons-learned/"
      ],
      "ed26cf70-9fc2-52f1-b8c2-ceb799c50288": [
        "/regex-and-automated-test-fuzzing/"
      ],
      "17642430-0738-5531-88a3-12996ff69b04": [
        "/build-your-own-regex/"
      ],
      "7d3c9af5-23a6-5b5e-a90a-cd9cf594f734": [
        "/optimizing-elasticsearch-score/"
      ],
      "a9b06f31-6aa6-5fc0-928d-666b4b718c09": [
        "/write-your-own-redux-connect/"
      ],
      "74ada755-89ba-515c-9224-f120cb838f38": [
        "/using-reduce/"
      ],
      "7dee6c2e-c05c-5f6e-a526-d7f5c9ef3265": [
        "/leveraging-immutability-in-react/"
      ]
    },
    "connections": {
      "MarkdownRemark": [
        "/"
      ]
    }
  },
  "jsonDataPaths": {
    "hidden-costs-of-postgresql-jsonb-6a0": "987/path---hidden-costs-of-postgresql-jsonb-6-a-0-a3e-ZRf7xGwD8UjfmQub10Q7GNibDo",
    "sq--src-components-layout-js": 755544856,
    "using-jq-to-effortlessly-ingest-newline-delimited-json-into-postgres-7b7": "971/path---using-jq-to-effortlessly-ingest-newline-delimited-json-into-postgres-7-b-7-e1e-uPYcJQ0i4KXUhmOWHCSEPnIdvg",
    "index": "587/path---index-6a9-gXEEJ57z0QnmFF7zZKx3rGNl4Y",
    "ethical-engineering-for-the-average-engineer-762": "974/path---ethical-engineering-for-the-average-engineer-762-dbc-QUvAeI7MUtUfJkgrlPjXD1w56xg",
    "build-your-own-nested-query-string-encoder-ac5": "79/path---build-your-own-nested-query-string-encoder-ac-5-a44-xDlvWxJSj0yUXM3KgmeJnfUT4Q",
    "youre-hiring-programmers-wrong-a-case-for-interview-standardization-3ae": "756/path---youre-hiring-programmers-wrong-a-case-for-interview-standardization-3-ae-d14-4PWzzTYPmm681hx91ashi2nFHyY",
    "scraping-the-web-with-puppeteer-lessons-learned-6a6": "42/path---scraping-the-web-with-puppeteer-lessons-learned-6-a-6-bdd-hiLkO5LtoPvW2PhjhBOSyBY9VYc",
    "regex-and-automated-test-fuzzing-d2c": "373/path---regex-and-automated-test-fuzzing-d-2-c-dc1-yLrXb2NzEYP4izH1NbAUs5EFUgQ",
    "build-your-own-regex-82a": "801/path---build-your-own-regex-82-a-5a9-cNpLc2y8s77N3dId8Ii0aAeE9wQ",
    "optimizing-elasticsearch-score-ba9": "430/path---optimizing-elasticsearch-score-ba-9-749-bQEk0aOVsUafnnbelzxMFh9vM1E",
    "write-your-own-redux-connect-c78": "806/path---write-your-own-redux-connect-c-78-975-CTs2d6lWAEWYCGaCcVprUKtM7c",
    "using-reduce-627": "376/path---using-reduce-627-2fd-9xC75aT7CWCWSnExY7JhLNBdA",
    "dev-404-page-5f9": "920/path---dev-404-page-5-f-9-fab-NZuapzHg3X9TaN1iIixfv1W23E",
    "leveraging-immutability-in-react-278": "103/path---leveraging-immutability-in-react-278-37d-XMo8AlRBsHRlbpVIUvKn7RyJ8Y",
    "404-22d": "44/path---404-22-d-bce-NZuapzHg3X9TaN1iIixfv1W23E",
    "404-html-516": "164/path---404-html-516-62a-NZuapzHg3X9TaN1iIixfv1W23E",
    "sq--src-components-article-js": 2957138115,
    "undefined-195": "106/path---undefined-195-f7e-U15MEjdpJzJYiJeWZsUwfYSh0"
  },
  "components": {
    "/Users/nickdrane/projects/blog/src/templates/article.js": {
      "query": "query usersNickdraneProjectsBlogSrcTemplatesArticleJs1899859386(\n  $slug: String!\n) {\n  markdownRemark(slug: {eq: $slug}) {\n    html\n    frontmatter {\n      title\n      date\n    }\n  }\n}\n",
      "componentPath": "/Users/nickdrane/projects/blog/src/templates/article.js"
    },
    "/Users/nickdrane/projects/blog/.cache/dev-404-page.js": {
      "query": "",
      "componentPath": "/Users/nickdrane/projects/blog/.cache/dev-404-page.js"
    },
    "/Users/nickdrane/projects/blog/src/pages/404.js": {
      "query": "",
      "componentPath": "/Users/nickdrane/projects/blog/src/pages/404.js"
    },
    "/Users/nickdrane/projects/blog/src/pages/index.js": {
      "query": "query usersNickdraneProjectsBlogSrcPagesIndexJs2228972296 {\n  allMarkdownRemark {\n    totalCount\n    edges {\n      node {\n        excerpt\n        slug\n        frontmatter {\n          title\n          date\n        }\n      }\n    }\n  }\n}\n",
      "componentPath": "/Users/nickdrane/projects/blog/src/pages/index.js"
    }
  },
  "staticQueryComponents": {
    "sq--src-components-layout-js": {
      "name": "SiteTitleQuery",
      "componentPath": "/Users/nickdrane/projects/blog/src/components/layout.js",
      "id": "sq--src-components-layout-js",
      "jsonName": "sq--src-components-layout-js",
      "query": "query SiteTitleQuery {\n  site {\n    siteMetadata {\n      title\n    }\n  }\n}\n",
      "hash": 755544856
    }
  }
}