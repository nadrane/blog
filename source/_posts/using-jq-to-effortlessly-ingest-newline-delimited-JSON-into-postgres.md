---
title: Using jq to Effortlessly Ingest Line-delimited JSON into PostgreSQL
date: 2018-10-18
categories:
  - [Command Line]
  - [Postgres]
---

I recently wanted to ingest a [line-delimited](https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON) JSON file into [Postgres](https://www.postgresql.org/) for some quick data exploration. I was surprised when I couldn't find a simple solution online. Here is my approach.

<!-- more -->

## Downloading 250000 Hacker News Comments

Let's say we want to download all of the [Hacker News](https://news.ycombinator.com/) comments from the month of May. A line-delimited JSON file is available from [pushshift](https://files.pushshift.io/hackernews/HNI_2018-05.bz2). Fetching and decompressing the file is simple:

```bash
curl https://files.pushshift.io/hackernews/HNI_2018-05.bz2 | bzip2 -d
```

Here is what the dataset looks like:

```JSON
{
  "by": "criddell",
  "id": 16966059,
  "kids": [
    16966312,
    16966776,
    16969455,
    16966323
  ],
  "parent": 16965363,
  "retrieved_on": 1528401399,
  "text": "Yeah - there's always a HATEOAS comment somewhere and...",
  "time": 1525173078,
  "type": "comment"
}
```

## Formatting the Data

You might think that Postgres has a simple utility for loading line-delimited JSON. Like me, you'd be wrong. It's all the more surprising given that it has a [COPY](https://www.postgresql.org/docs/current/static/sql-copy.html) utility that's designed to load data from files. Unfortunately, that utility only supports `text`, `csv`, and `binary` formats.

Transforming our data into a CSV is a breeze with [jq](https://stedolan.github.io/jq/). We can pipe the JSON stream into the following command to extract the `id`, `by`, `parent`, and `text` fields. You can customize the command to extract whatever fields you like.

```bash
jq -r '[.id, .by, .parent, .text] | @csv'
```

The `-r` option indicates that we would like a raw string output, as opposed to JSON formatted with quotes. The `[.id, .by, .parent, .text]` part produces an array containing the desired fields and the pipe into `@csv` specifies the format. All that's left is to load the data into Postgres.

## Ingesting the Data

After creating the database

`createdb comment_db`

and applying the schema

```SQL
CREATE TABLE comment (
    id INTEGER PRIMARY KEY,
    by VARCHAR,
    parent INTEGER,
    text TEXT
);
```

we can hydrate our comments into `comment_db` using [psql](https://www.postgresql.org/docs/current/static/app-psql.html)

```bash
psql comment_db -c "COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)"
```

Note that the fields specified above need to be in the same order as the fields in the CSV stream generated by `jq`.

Here is the final command

```bash
curl https://files.pushshift.io/hackernews/HNI_2018-05.bz2 \
  | bzip2 -d \
  | jq -r '[.id, .by, .parent, .text] | @csv' \
  | psql comment_db -c "COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)"
```

One final caveat. You might notice that even though the `parent` column references a comment id, I've neglected to specify the foreign key constraint. This is because our command does not control for the order that comments are loaded. We would get a constraint error if a comment references a parent comment that has not yet been inserted. It would probably be easiest to apply these constraints after ingesting the data.

My name is Nick Drane. I do [consulting](/hire-me) out of Chicago area and am looking for new opportunities.