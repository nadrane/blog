{"componentChunkName":"component---src-templates-category-js","path":"/categories/Postgres/","webpackCompilationHash":"c9042278f67077be9671","result":{"data":{"allMarkdownRemark":{"nodes":[{"html":"<p>I recently wanted to ingest a <a href=\"https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON\">line-delimited</a> JSON file into <a href=\"https://www.postgresql.org/\">Postgres</a> for some quick data exploration. I was surprised when I couldn't find a simple CLI solution that parsed the JSON and loaded each field into its own column. Every approach I found instead inserted the entire JSON object in a JSONB field. Here is my solution.</p>\n<!-- more -->\n<h2>Downloading 250000 Hacker News Comments</h2>\n<p>Let's say we want to download all of the <a href=\"https://news.ycombinator.com/\">Hacker News</a> comments from the month of May. A line-delimited JSON file is available from <a href=\"https://files.pushshift.io/hackernews/HNI_2018-05.bz2\">pushshift</a>. Fetching and decompressing the file is simple:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> https://files.pushshift.io/hackernews/HNI_2018-05.bz2 <span class=\"token operator\">|</span> <span class=\"token function\">bzip2</span> -d</code></pre></div>\n<p>Here is what the dataset looks like:</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"by\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"criddell\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">16966059</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"kids\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">16966312</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16966776</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16969455</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16966323</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"parent\"</span><span class=\"token operator\">:</span> <span class=\"token number\">16965363</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"retrieved_on\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1528401399</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"text\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Yeah - there's always a HATEOAS comment somewhere and...\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"time\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1525173078</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"comment\"</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<h2>Formatting the Data</h2>\n<p>You might think that Postgres has a simple utility for loading line-delimited JSON. Like me, you'd be wrong. It's all the more surprising given that it has a <a href=\"https://www.postgresql.org/docs/current/static/sql-copy.html\">COPY</a> utility that's designed to load data from files. Unfortunately, that utility only supports <code class=\"language-markup\">text</code>, <code class=\"language-markup\">csv</code>, and <code class=\"language-markup\">binary</code> formats.</p>\n<p>Transforming our data into a CSV is a breeze with <a href=\"https://stedolan.github.io/jq/\">jq</a>. We can pipe the JSON stream into the following command to extract the <code class=\"language-text\">id</code>, <code class=\"language-text\">by</code>, <code class=\"language-text\">parent</code>, and <code class=\"language-text\">text</code> fields. You can customize the command to extract whatever fields you like.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">jq -r <span class=\"token string\">'[.id, .by, .parent, .text] | @csv'</span></code></pre></div>\n<p>The <code class=\"language-text\">-r</code> option indicates that we would like a raw string output, as opposed to JSON formatted with quotes. The <code class=\"language-js\"><span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span>id<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span>by<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span>parent<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">]</span></code> part produces an array containing the desired fields and the pipe into <code class=\"language-text\">bash&gt;@csv</code> specifies the format. All that's left is to load the data into Postgres.</p>\n<h2>Ingesting the Data</h2>\n<p>After creating the database</p>\n<p><code class=\"language-text\">createdb comment_db</code></p>\n<p>and applying the schema</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> <span class=\"token keyword\">comment</span> <span class=\"token punctuation\">(</span>\n    id <span class=\"token keyword\">INTEGER</span> <span class=\"token keyword\">PRIMARY</span> <span class=\"token keyword\">KEY</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">by</span> <span class=\"token keyword\">VARCHAR</span><span class=\"token punctuation\">,</span>\n    parent <span class=\"token keyword\">INTEGER</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">text</span> <span class=\"token keyword\">TEXT</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>we can hydrate our comments into <code class=\"language-text\">comment_db</code> using <a href=\"https://www.postgresql.org/docs/current/static/app-psql.html\">psql</a></p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">psql comment_db -c <span class=\"token string\">\"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"</span></code></pre></div>\n<p>Note that the fields specified above need to be in the same order as the fields in the CSV stream generated by <code class=\"language-text\">jq</code>.</p>\n<p>Here is the final command</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> https://files.pushshift.io/hackernews/HNI_2018-05.bz2 <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> <span class=\"token function\">bzip2</span> -d <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> jq -r <span class=\"token string\">'[.id, .by, .parent, .text] | @csv'</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> psql comment_db -c <span class=\"token string\">\"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"</span></code></pre></div>\n<h2>Supporting Referential Integrity</h2>\n<p>You will notice that despite the fact that the <code class=\"language-text\">comment.parent</code> refers to a comment id, we have omitted a foreign key constraint from our schema. This omission is because our command does not control for the order in which comments are loaded. We would have received constraint errors if we specified the foreign key relationship.</p>\n<p>We can overcome this obstacle by sorting our incoming comments by id.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> https://files.pushshift.io/hackernews/HNI_2018-05.bz2 <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> <span class=\"token function\">bzip2</span> -d <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> jq -s -r <span class=\"token string\">'sort_by(.id) | .[] | [.id, .by, .parent, .text] | @csv'</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> psql comment_db -c <span class=\"token string\">\"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"</span></code></pre></div>\n<p>If you have a primary key that doesn't serially increase - perhaps you're using a <a href=\"https://en.wikipedia.org/wiki/Natural_key\">natural key</a> or a UUID as your primary key - then you could also sort on a <code class=\"language-text\">created_at</code> timestamp</p>\n<h2>Tradeoffs</h2>\n<p>Everything in software engineering has a tradeoff, and I would be remiss to to not mention them here. That <code class=\"language-text\">-s</code> option we specified above instructs <code class=\"language-text\">jq</code> to download the entire dataset into memory, a requirement for sorting. If you dataset is too large, then the command will fail (<code class=\"language-text\">jq</code> failed for me at 769MB).</p>\n<p>The first option does not suffer this limitation and will work for arbitrarily large datasets. This is because it leverages <a href=\"https://en.wikipedia.org/wiki/Stream_(computing)\">streams</a> to only work on small chunks of data at once. If your dataset is large and you want foreign key constraints, you could use this streaming approach and then apply the constraints after data ingestion completes.</p>\n<p><em>If you have a data engineering or PostgreSQL related problem, I do <a href=\"/hire-me\">consulting</a> work and am currently looking for new clients. Please <a href=\"mailto:nick@nickdrane.com\">contact me</a> for more details</em></p>","excerpt":"<p>I recently wanted to ingest a <a href=\"https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON\">line-delimited</a> JSON file into <a href=\"https://www.postgresql.org/\">Postgres</a> for some quick data exploration. I was surprised when I couldn't find a simple CLI solution that parsed the JSON and loaded each field into its own column. Every approach I found instead inserted the entire JSON object in a JSONB field. Here is my solution.</p>\n","frontmatter":{"title":"Using Shell Commands to Effortlessly Ingest Line-Delimited JSON into PostgreSQL","date":"2018-10-18T00:00:00.000Z","url":"using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres"}},{"html":"<p><a href=\"https://www.postgresql.org/\">Postgres</a> introduced the <a href=\"https://www.postgresql.org/docs/current/static/datatype-json.html\">JSONB</a> type in version 9.4 with considerable excitement. JSONB promised to marry a favorite relational database with the noSQL world, permitting data to be stored in the database as JSON without the need for re-parsing whenever a field is accessed. Moreover, the binary storage format permits indexing and complex queries against the stored JSON blobs. This data format embodies the flexible schema and was readily adopted at <a href=\"https://fraight.ai/\">Fraight</a>.</p>\n<!-- more -->\n<h2>Background</h2>\n<p>At Fraight, we've built a centralized communication platform that collates all inbound/outbound communications between our brokerage company and thousands of trucking partners. One of our main objectives is to build a system that parses and automatically responds to inbound text messages, emails, and faxes. We knew we would eventually need the nitty-gritty details of these messages, so we captured the data by dumping entire http response bodies into a JSONB column named <code class=\"language-text\">meta</code> in our database’s <code class=\"language-text\">message</code> table.</p>\n<p>In an ideal world, the third party API responses we collected would have been broken down into discrete chunks and stored in separate columns, but our approach was a pragmatic design decision. We knew it wasn't worth the engineering effort to try to understand the multitude of fields we receive from a half-dozen APIs, particularly when we had no idea at the time how we might use this information. But we did know it was valuable. And since we wanted it to be queryable, we chose JSONB over it's more inert and sometimes more size efficient cousin, the JSON datatype.</p>\n<h2>Discovery</h2>\n<p>Fraight’s CTO approached me the other day and explained that query performance over the <code class=\"language-text\">message</code> table had deteriorated. He elaborated that the queries were only slow when the <code class=\"language-text\">meta</code> column was included in the result set. We had previously experienced slowdown in the <code class=\"language-text\">message</code> table when entire email attachment bodies were getting serialized and stored in the <code class=\"language-text\">meta</code> column, and I suspected that the root cause of our current performance problem was in a similar vein. A quick <a href=\"#footnote1\">query</a> revealed that our <code class=\"language-text\">meta</code> column was often quite large.</p>\n<p>The average size of the <code class=\"language-text\">meta</code> column was 3.7 kb. That might not seem large, but for our 100,000 row table, that meant 400mb of (mostly unused) metadata. At the high end of the spectrum, some messages were up to 17mb in size. The precise details of why this dataset slows down queries are a bit esoteric<sup><a href=\"#footnote2\">2</a></sup>, but it was clear that we were storing too much information. You can read more about how Postgres stores large data values using <a href=\"https://www.postgresql.org/docs/current/static/storage-toast.html\">TOAST</a> and <a href=\"#footnote3\">how I validated this was a problem</a>.</p>\n<h2>Further Complications</h2>\n<p>The first queries against the <code class=\"language-text\">meta</code> column were for very simple tasks like showing the raw contents of an email to a user, perhaps in the case where message content extraction failed. Over time we began using specific fields to drive business logic, and our application code started to expect that <code class=\"language-text\">meta</code>'s JSON would adhere to a specific shape. Any proposed solution would likely necessitate changes to this application code.</p>\n<p>So the problem was twofold:</p>\n<ol>\n<li>We needed to extract the actual metadata (as opposed to fields that drove business logic) from the <code class=\"language-text\">meta</code> column and place it in a location where it would not affect query performance over the <code class=\"language-text\">message</code> table.</li>\n<li>We wanted to make the metadata less easily accessible. We recognized that making it accessible through the ORM made it ripe for misuse. We wanted to give it an alternative API that would lessen a developer's likeliness to rely on its structure.</li>\n</ol>\n<h2>Solutioning</h2>\n<p>We looked at several solutions. Below are the three we considered most seriously:</p>\n<ol>\n<li>Use the ORM to omit the <code class=\"language-text\">meta</code> column from all queries unless specifically included. Since we unnecessarily perform <code class=\"language-text\">select *</code> <sup><a href=\"#footnote4\">4</a></sup> queries over the <code class=\"language-text\">message</code> table, this strategy, despite its messiness, would increase the performance in most cases (with the occasional slow query when metadata was actually required) and would in theory be simple to implement. It wouldn't, however, resolve our initial design shortcut.</li>\n<li>Keep the <code class=\"language-text\">meta</code> column as-is but extract specific keys to <a href=\"https://aws.amazon.com/s3/\">S3</a>. In general, greater than 99% of the size of any given column's <code class=\"language-text\">meta</code> field was from a single key. For example, many of the emails we capture include large attachments. Other times message bodies retain long, historical email chains. By extracting these problematic fields and uploading them to S3, the database would only need to store a reference to the S3 content. Then, upon request, the server could generate a <a href=\"https://docs.aws.amazon.com/AmazonS3/latest/dev//ShareObjectPreSignedURL.html\">pre-signed URL</a>, allowing the client to download large files directly from S3.</li>\n<li>Create a separate <code class=\"language-text\">metadata</code> table in Postgres. It would have a foreign key back to the original table where the metadata belonged. This solution solves the <code class=\"language-text\">select *</code> problem described above and offers an additional advantage: since the <code class=\"language-text\">meta</code> column pattern exists on more than just the <code class=\"language-text\">message</code> table, it offers a unified strategy for storing metadata.</li>\n</ol>\n<p>Our instinct was to go with #1 — the most simple and straightforward solution — and omit the <code class=\"language-text\">meta</code> column from most queries. Unfortunately, bugs in our ORM made it impossible to omit columns across joins without the occasional crash. We needed an alternate approach.</p>\n<p>This hiccup left the choice between S3 and a separate Postgres table. We agreed that pre-signed S3 URLs offered an ideal long-term alternative but ultimately chose a separate Postgres table for the same reason that we wanted to choose option #1: minimal risk and complexity. Our team is exceptionally experienced with Postgres, and we knew we could hit the ground running. S3, in contrast, had more unknowns, and given how rarely we access most of this metadata today, the value it would add over Postgres was tenuous at best.</p>\n<p>In addition to migrating metadata to its own table, we took a couple additional steps:</p>\n<ol>\n<li>We extracted the handful of regularly used fields from the metadata and migrated them into their own columns in the <code class=\"language-text\">message</code> table. This meant that we didn't need to retrieve large, megabyte sized blobs whenever we wanted a single field<sup><a href=\"#footnote5\">5</a></sup>. As an added advantage, we regained simple access to database <a href=\"https://www.postgresql.org/docs/current/static/ddl-constraints.html\">constraints</a>.</li>\n<li>When we created the new <code class=\"language-text\">metadata</code> table in Postgres, we made sure not to define a relationship between it and its related tables at the ORM layer, only the database layer. This makes it far more difficult for a developer to hobble performance by absentmindedly joining large metadata into queries. We introduced an API for accessing the metadata instead. The added advantage of this API is that we can now change the underlying implementation to use S3 (or anything else) in the future, without modifying dependent application code.</li>\n</ol>\n<p><em>If you have ever had performance problems with PostgreSQL, I do <a href=\"/hire-me\">consulting</a> work and am currently looking for new clients. Please <a href=\"mailto:nick@nickdrane.com\">contact me</a> for more details.</em></p>\n<h4>Footnotes</h4>\n<p><a name=\"footnote1\">1</a>: I used the following query:</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> <span class=\"token function\">avg</span><span class=\"token punctuation\">(</span>octet_length<span class=\"token punctuation\">(</span>m<span class=\"token punctuation\">.</span><span class=\"token string\">\"meta\"</span>::<span class=\"token keyword\">text</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">FROM</span> message <span class=\"token keyword\">as</span> m</code></pre></div>\n<p><a name=\"footnote2\">2</a>: I should clarify that performance measurements were done with a local Postgres installation where network congestion/throughput is not a relevant factor.</p>\n<p><a name=\"footnote3\">3</a>:\nI verified that the <code class=\"language-text\">meta</code> column was toasting with a little help from the <a href=\"https://www.postgresql.org/docs/10/static/disk-usage.html\">Postgres docs</a>:</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">SELECT</span> relname<span class=\"token punctuation\">,</span> relpages<span class=\"token punctuation\">,</span> relpages <span class=\"token operator\">*</span> <span class=\"token number\">8191</span> <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1024</span> <span class=\"token operator\">*</span> <span class=\"token number\">1024</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> size\n<span class=\"token keyword\">FROM</span> pg_class<span class=\"token punctuation\">,</span>\n     <span class=\"token punctuation\">(</span><span class=\"token keyword\">SELECT</span> reltoastrelid\n      <span class=\"token keyword\">FROM</span> pg_class\n      <span class=\"token keyword\">WHERE</span> relname <span class=\"token operator\">=</span> <span class=\"token string\">'message'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">AS</span> ss\n<span class=\"token keyword\">WHERE</span> oid <span class=\"token operator\">=</span> ss<span class=\"token punctuation\">.</span>reltoastrelid <span class=\"token operator\">OR</span>\n      oid <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">SELECT</span> indexrelid\n             <span class=\"token keyword\">FROM</span> pg_index\n             <span class=\"token keyword\">WHERE</span> indrelid <span class=\"token operator\">=</span> ss<span class=\"token punctuation\">.</span>reltoastrelid<span class=\"token punctuation\">)</span></code></pre></div>\n<p>For specifically the <code class=\"language-text\">message</code> table, this query returns the number of disk pages in the toast table and their total size in megabytes.</p>\n<table>\n<thead>\n<tr>\n<th>table</th>\n<th>disk pages</th>\n<th>size (mb)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>toast table</td>\n<td>17731</td>\n<td>139</td>\n</tr>\n<tr>\n<td>toast index</td>\n<td>209</td>\n<td>1</td>\n</tr>\n</tbody>\n</table>\n<p><a name=\"footnote4\">4</a>: We were not literally doing any <code class=\"language-text\">select *</code> queries. Our ORM's message model did, however, specify all of the columns of the <code class=\"language-text\">message</code> table. In retrospect, this was probably our biggest mistake. It meant that a call to <code class=\"language-text\">Message.find</code>, which is used all over the place, retrieved all columns on the <code class=\"language-text\">message</code> table, unless specifically directed otherwise. Usually, for a RESTful API, this is an acceptable performance tradeoff, but it didn't hold true in this circumstance.</p>\n<p><a name=\"footnote5\">5</a>: As I write this, I wonder if it wouldn't have been possible to leverage indexes to only retrieve specific pieces of the <code class=\"language-text\">meta</code> field. I wonder if our ORM provides any support for firstclass fields that are subfields of another field.</p>","excerpt":"<p><a href=\"https://www.postgresql.org/\">Postgres</a> introduced the <a href=\"https://www.postgresql.org/docs/current/static/datatype-json.html\">JSONB</a> type in version 9.4 with considerable excitement. JSONB promised to marry a favorite relational database with the noSQL world, permitting data to be stored in the database as JSON without the need for re-parsing whenever a field is accessed. Moreover, the binary storage format permits indexing and complex queries against the stored JSON blobs. This data format embodies the flexible schema and was readily adopted at <a href=\"https://fraight.ai/\">Fraight</a>.</p>\n","frontmatter":{"title":"The Hidden Costs of PostgreSQL's JSONB Datatype","date":"2018-09-30T00:00:00.000Z","url":"hidden-costs-of-postgresql-jsonb"}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"category":"Postgres"}}}