{"componentChunkName":"component---src-templates-category-js","path":"/categories/Shell/","webpackCompilationHash":"be161c4017b1db4ff602","result":{"data":{"allMarkdownRemark":{"nodes":[{"html":"<p>I recently wanted to ingest a <a href=\"https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON\">line-delimited</a> JSON file into <a href=\"https://www.postgresql.org/\">Postgres</a> for some quick data exploration. I was surprised when I couldn't find a simple CLI solution that parsed the JSON and loaded each field into its own column. Every approach I found instead inserted the entire JSON object in a JSONB field. Here is my solution.</p>\n<!-- more -->\n<h2>Downloading 250000 Hacker News Comments</h2>\n<p>Let's say we want to download all of the <a href=\"https://news.ycombinator.com/\">Hacker News</a> comments from the month of May. A line-delimited JSON file is available from <a href=\"https://files.pushshift.io/hackernews/HNI_2018-05.bz2\">pushshift</a>. Fetching and decompressing the file is simple:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> https://files.pushshift.io/hackernews/HNI_2018-05.bz2 <span class=\"token operator\">|</span> <span class=\"token function\">bzip2</span> -d</code></pre></div>\n<p>Here is what the dataset looks like:</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"by\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"criddell\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">16966059</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"kids\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">16966312</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16966776</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16969455</span><span class=\"token punctuation\">,</span> <span class=\"token number\">16966323</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"parent\"</span><span class=\"token operator\">:</span> <span class=\"token number\">16965363</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"retrieved_on\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1528401399</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"text\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Yeah - there's always a HATEOAS comment somewhere and...\"</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"time\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1525173078</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"comment\"</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<h2>Formatting the Data</h2>\n<p>You might think that Postgres has a simple utility for loading line-delimited JSON. Like me, you'd be wrong. It's all the more surprising given that it has a <a href=\"https://www.postgresql.org/docs/current/static/sql-copy.html\">COPY</a> utility that's designed to load data from files. Unfortunately, that utility only supports <code class=\"language-markup\">text</code>, <code class=\"language-markup\">csv</code>, and <code class=\"language-markup\">binary</code> formats.</p>\n<p>Transforming our data into a CSV is a breeze with <a href=\"https://stedolan.github.io/jq/\">jq</a>. We can pipe the JSON stream into the following command to extract the <code class=\"language-text\">id</code>, <code class=\"language-text\">by</code>, <code class=\"language-text\">parent</code>, and <code class=\"language-text\">text</code> fields. You can customize the command to extract whatever fields you like.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">jq -r <span class=\"token string\">'[.id, .by, .parent, .text] | @csv'</span></code></pre></div>\n<p>The <code class=\"language-text\">-r</code> option indicates that we would like a raw string output, as opposed to JSON formatted with quotes. The <code class=\"language-js\"><span class=\"token punctuation\">[</span><span class=\"token punctuation\">.</span>id<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span>by<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span>parent<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">.</span>text<span class=\"token punctuation\">]</span></code> part produces an array containing the desired fields and the pipe into <code class=\"language-text\">bash&gt;@csv</code> specifies the format. All that's left is to load the data into Postgres.</p>\n<h2>Ingesting the Data</h2>\n<p>After creating the database</p>\n<p><code class=\"language-text\">createdb comment_db</code></p>\n<p>and applying the schema</p>\n<div class=\"gatsby-highlight\" data-language=\"sql\"><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token keyword\">CREATE</span> <span class=\"token keyword\">TABLE</span> <span class=\"token keyword\">comment</span> <span class=\"token punctuation\">(</span>\n    id <span class=\"token keyword\">INTEGER</span> <span class=\"token keyword\">PRIMARY</span> <span class=\"token keyword\">KEY</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">by</span> <span class=\"token keyword\">VARCHAR</span><span class=\"token punctuation\">,</span>\n    parent <span class=\"token keyword\">INTEGER</span><span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">text</span> <span class=\"token keyword\">TEXT</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>we can hydrate our comments into <code class=\"language-text\">comment_db</code> using <a href=\"https://www.postgresql.org/docs/current/static/app-psql.html\">psql</a></p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">psql comment_db -c <span class=\"token string\">\"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"</span></code></pre></div>\n<p>Note that the fields specified above need to be in the same order as the fields in the CSV stream generated by <code class=\"language-text\">jq</code>.</p>\n<p>Here is the final command</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> https://files.pushshift.io/hackernews/HNI_2018-05.bz2 <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> <span class=\"token function\">bzip2</span> -d <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> jq -r <span class=\"token string\">'[.id, .by, .parent, .text] | @csv'</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> psql comment_db -c <span class=\"token string\">\"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"</span></code></pre></div>\n<h2>Supporting Referential Integrity</h2>\n<p>You will notice that despite the fact that the <code class=\"language-text\">comment.parent</code> refers to a comment id, we have omitted a foreign key constraint from our schema. This omission is because our command does not control for the order in which comments are loaded. We would have received constraint errors if we specified the foreign key relationship.</p>\n<p>We can overcome this obstacle by sorting our incoming comments by id.</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> https://files.pushshift.io/hackernews/HNI_2018-05.bz2 <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> <span class=\"token function\">bzip2</span> -d <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> jq -s -r <span class=\"token string\">'sort_by(.id) | .[] | [.id, .by, .parent, .text] | @csv'</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token operator\">|</span> psql comment_db -c <span class=\"token string\">\"COPY comment (id, by, parent, text) FROM STDIN WITH (FORMAT CSV)\"</span></code></pre></div>\n<p>If you have a primary key that doesn't serially increase - perhaps you're using a <a href=\"https://en.wikipedia.org/wiki/Natural_key\">natural key</a> or a UUID as your primary key - then you could also sort on a <code class=\"language-text\">created_at</code> timestamp</p>\n<h2>Tradeoffs</h2>\n<p>Everything in software engineering has a tradeoff, and I would be remiss to to not mention them here. That <code class=\"language-text\">-s</code> option we specified above instructs <code class=\"language-text\">jq</code> to download the entire dataset into memory, a requirement for sorting. If you dataset is too large, then the command will fail (<code class=\"language-text\">jq</code> failed for me at 769MB).</p>\n<p>The first option does not suffer this limitation and will work for arbitrarily large datasets. This is because it leverages <a href=\"https://en.wikipedia.org/wiki/Stream_(computing)\">streams</a> to only work on small chunks of data at once. If your dataset is large and you want foreign key constraints, you could use this streaming approach and then apply the constraints after data ingestion completes.</p>\n<p><em>If you have a data engineering or PostgreSQL related problem, I do <a href=\"/hire-me\">consulting</a> work and am currently looking for new clients. Please <a href=\"mailto:nick@nickdrane.com\">contact me</a> for more details</em></p>","excerpt":"<p>I recently wanted to ingest a <a href=\"https://en.wikipedia.org/wiki/JSON_streaming#Line-delimited_JSON\">line-delimited</a> JSON file into <a href=\"https://www.postgresql.org/\">Postgres</a> for some quick data exploration. I was surprised when I couldn't find a simple CLI solution that parsed the JSON and loaded each field into its own column. Every approach I found instead inserted the entire JSON object in a JSONB field. Here is my solution.</p>\n","frontmatter":{"title":"Using Shell Commands to Effortlessly Ingest Line-Delimited JSON into PostgreSQL","date":"2018-10-18T00:00:00.000Z","url":"using-jq-to-effortlessly-ingest-newline-delimited-JSON-into-postgres"}}]}},"pageContext":{"isCreatedByStatefulCreatePages":false,"category":"Shell"}}}